{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functions import import_imagedata, label_oh_tf, ImageProcessor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.models import resnet101\n",
    "\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from fns4wandb import train_log, build_optimizer\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from fns4wandb import train_log, build_optimizer\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg16 = vgg16(weights=\"IMAGENET1K_V1\")\n",
    "model_vgg16.classifier.pop(6)\n",
    "model_vgg16.to(device)\n",
    "#print(model_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(label, prediction): #TypeError: Singleton array tensor(3) cannot be considered a valid collection.\n",
    "    \n",
    "    label= np.array(label.cpu())\n",
    "\n",
    "    predictions_np = prediction.cpu().detach().numpy()\n",
    "    #y_pred' parameter of f1_score must be an array-like or a sparse matrix. Got 7 instead.\n",
    "    predicted_classes = np.argmax(predictions_np, axis=0)\n",
    "    #print('metrics Label:   ', label)\n",
    "    #print('metrics prediction   ', predicted_classes)\n",
    "    #avg_f1_score = f1_score(label, predictions_np, average='macro')\n",
    "    acc = accuracy_score(label, predicted_classes)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_im(img_path):\n",
    "    IP = ImageProcessor(device='cpu')\n",
    "    img = cv2.imread(img_path) #\n",
    "    img = IP.blank_padding(img, (224,224))\n",
    "    img = IP.to_tensor(img).to(device)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get len of \n",
    "\n",
    "file_path = r'/its/home/nn268/antvis/optics/AugmentedDS_IDSW/'\n",
    "random_seed =1\n",
    "img_len = len(os.listdir(file_path))\n",
    "\n",
    "\n",
    "#print(ids[4])\n",
    "x, y = import_imagedata(file_path)\n",
    "\n",
    "x_train, x_test, y_train, y_tests = train_test_split(x,y, test_size=0.2, train_size=0.8,\n",
    "                                 random_state=random_seed, shuffle=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train, test_size=0.1, train_size=0.8,\n",
    "                                 random_state=random_seed, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaughticalnonsence\u001b[0m (\u001b[33mantvis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'IDSW_VGG16_fine_80_112023'\n",
    "save_dict = {'Run' : title,\n",
    "            'Current_Epoch': 0}\n",
    "                #r'/its/home/nn268/antvis/optics/\n",
    "save_location = r'pickles/'#pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IDSW_VGG16_fine_80_112023'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dict['Run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Squeeze, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Do your print / debug stuff here\n",
    "        x = x.squeeze(0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs= 80, #30, \n",
    "    learning_rate =1e-5,\n",
    "    architecture ='CNN',\n",
    "    optimizer= 'adam',\n",
    "    weight_decay= 4e-5,\n",
    "    ks = 3,\n",
    "    scheduler=0.2,\n",
    "    f_lin_lay = 7168 #1024*7 = 7168\n",
    ")\n",
    "\n",
    "def train_model(model, loss_fn, x_train, x_val, y_train, y_val, config, best_acc=0): #train_dl, val_dl, \n",
    "    wandb.watch(model, loss_fn, log='all', log_freq=10)\n",
    "    \n",
    "    lr = config['learning_rate'] #1e-5 #config.learning_rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)#build_optimizer(model, optimizer=torch.optim.Adam(model.parameters(), lr=lr))#config.optimizer, config.learning_rate, config.weight_decay)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config['scheduler'], last_epoch=-1) #gamma=config.scheduler, last_epoch=-1)\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    #losses= []\n",
    "    #predictions = []\n",
    "    t_loss_list = []\n",
    "    v_loss_list = []\n",
    "    t_predict_list = []\n",
    "    v_predict_list = []\n",
    "    t_accuracy_list = []\n",
    "    v_accuracy_list = []\n",
    "    t_label_list = []\n",
    "    v_label_list = []\n",
    "    #labels = []\n",
    "    \n",
    "    total_epochs = 0\n",
    "    for epoch in tqdm(range(config['epochs'])): #config.epochs)):\n",
    "        print('E   ', epoch)\n",
    "        t_correct = 0\n",
    "        v_correct = 0\n",
    "    \n",
    "        if epoch == 0:\n",
    "            best_model = deepcopy(model)\n",
    "        #train_ids = random.shuffle(train_ids)\n",
    "        #print(type(train_ids))\n",
    "        print('training...')\n",
    "        for idx, img in enumerate(x_train): \n",
    "            \n",
    "            #print('Img: ', img)\n",
    "\n",
    "            x = preprocess_im(img)\n",
    "            \n",
    "            #print(x)\n",
    "\n",
    "            train_prediction = model.forward(x)\n",
    "     \n",
    "            train_label = label_oh_tf(y_train[idx], device=device, num_classes=11) # use same index val to index y (labels) and turn into onehot encoded label\n",
    "            #print(train_prediction, train_label)\n",
    "        \n",
    "        \n",
    "            t_loss = loss_fn(train_prediction, train_label)\n",
    "\n",
    "            if train_prediction.argmax() == train_label.argmax():\n",
    "                t_correct+=1\n",
    "            t_loss_list.append(t_loss)\n",
    "            t_predict_list.append(train_prediction)\n",
    "            train_acc = (t_correct / len(x_train))\n",
    "            t_accuracy_list.append(train_acc)\n",
    "            \n",
    "            t_label_list.append(train_label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            t_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        print('validating...')\n",
    "        for idx, img in enumerate(x_val):\n",
    "            \n",
    "            x =preprocess_im(img)\n",
    "\n",
    "            val_prediction = model.forward(x)\n",
    "            y = label_oh_tf(y_val[idx], device=device, num_classes=11)\n",
    "\n",
    "            \n",
    "            val_prediction = model.forward(x)\n",
    "\n",
    "            v_loss = loss_fn(val_prediction, y)\n",
    "            \n",
    "            v_loss_list.append(v_loss)\n",
    "            if val_prediction.argmax() == y.argmax():\n",
    "                v_correct +=1\n",
    "            v_loss_list.append(v_loss)\n",
    "            v_predict_list.append(val_prediction)\n",
    "            \n",
    "            v_label_list.append(y)\n",
    "            val_acc = (v_correct / len(y_val))\n",
    "            v_accuracy_list.append(val_acc)\n",
    "            \n",
    "        total_epochs += epoch\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = deepcopy(model)\n",
    "            save_dict['Current_Epoch'] += config['epochs']\n",
    "            save_dict['model.state_dict'] = model.state_dict()\n",
    "            save_dict['training_samples'] = len(train_ids)\n",
    "            save_dict['validation_samples'] = len(val_ids)\n",
    "            save_dict['t_loss_list'] = t_loss_list\n",
    "            save_dict['t_predict_list'] = t_predict_list  \n",
    "            save_dict['t_accuracy_list'] = t_accuracy_list  #\n",
    "            save_dict['v_loss_list'] = v_loss_list\n",
    "            save_dict['v_predict_list'] = v_predict_list  #\n",
    "            save_dict['v_accuracy_list'] = v_accuracy_list  #\n",
    "            save_dict['t_labels'] = t_label_list\n",
    "            save_dict['v_labels'] = v_label_list\n",
    "            \n",
    "            title = save_dict['Run']\n",
    "            with open(f'{save_location}{title}.pkl', 'wb+') as f:\n",
    "                pickle.dump(save_dict, f)\n",
    "            print('improvment in metrics. model saved')\n",
    "        \n",
    "\n",
    "        if (epoch+1)%2==0:\n",
    "            train_log(t_loss, v_loss, epoch)\n",
    "            wandb.log({'train_accuracy_%': train_acc, 'epoch':epoch})\n",
    "            wandb.log({'val_accuracy_%': val_acc, 'epoch':epoch})\n",
    "    model= best_model\n",
    "    #labels = zip(t_label_list, v_label_list)\n",
    "    #losses = zip(t_loss_list, v_loss_list)\n",
    "    #predictions = zip(t_predict_list, v_predict_list)\n",
    "    return model,save_dict\n",
    "\n",
    "\n",
    "\n",
    "def pipeline(config): \n",
    "\n",
    "    title = f\"IDSW_on_Vgg16_fine\"\n",
    "    \n",
    "    loss_list=[]\n",
    "    #loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    with wandb.init(project=title, config=config):\n",
    "        config = wandb.config\n",
    "        model = nn.Sequential(\n",
    "            model_vgg16,\n",
    "            Squeeze(),\n",
    "            nn.Linear(4096,11),\n",
    "            nn.Softmax(dim=0),\n",
    "\n",
    "        ).to(device)\n",
    "\n",
    "        model, save_dict = train_model(model, loss_fn, x_train, x_val, y_train, y_val, config) #train_dl, val_dl\n",
    "\n",
    "    return model, save_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/its/home/nn268/antvis/optics/wandb/run-20231117_165829-5qr9xs93</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antvis/IDSW_on_Vgg16_fine/runs/5qr9xs93' target=\"_blank\">clean-frog-23</a></strong> to <a href='https://wandb.ai/antvis/IDSW_on_Vgg16_fine' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antvis/IDSW_on_Vgg16_fine' target=\"_blank\">https://wandb.ai/antvis/IDSW_on_Vgg16_fine</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antvis/IDSW_on_Vgg16_fine/runs/5qr9xs93' target=\"_blank\">https://wandb.ai/antvis/IDSW_on_Vgg16_fine/runs/5qr9xs93</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E    0\n",
      "training...\n"
     ]
    }
   ],
   "source": [
    "model,save_dict = pipeline(config) #7,168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "\n",
    "def plot_confusion(predictions:list, actual:list, title:str):\n",
    "    predict_list = [int(t.argmax()) for t in predictions]\n",
    "    actual = [int(l.argmax()) for l in actual]\n",
    "\n",
    "    actual = np.array(actual)\n",
    "    predict_list = np.array(predict_list)\n",
    "\n",
    "\n",
    "    #FixedLocator locations (3), usually from a call to set_ticks, does not match the number of labels (11).\n",
    "    print(f'\\n     {title}')\n",
    "    train_epoch_matrix = confusion_matrix(actual, predict_list, labels= [0,1,2,3,4,5,6,7,8,9,10])\n",
    "    disp= ConfusionMatrixDisplay(train_epoch_matrix, display_labels=[0,1,2,3,4,5,6,7,8,9,10])\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_predict = save_dict['t_predict_list']\n",
    "t_labels = save_dict['t_labels']\n",
    "\n",
    "v_predict = save_dict['v_predict_list'] # WHY IS THERE NOTHING IN V OREDICT LIST!\n",
    "v_labels = save_dict['v_labels']\n",
    "\n",
    "plot_confusion(t_predict, t_labels, 'Train Confusion Matrix')\n",
    "plot_confusion(v_predict, v_labels, 'Validation Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Do your print / debug stuff here\n",
    "        print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =nn.Sequential(\n",
    "    PrintLayer(),\n",
    "    model_vgg16,\n",
    "    PrintLayer(),\n",
    "    Squeeze(),\n",
    "    PrintLayer(),\n",
    "    nn.Linear(4096,11),\n",
    "    PrintLayer(),\n",
    "    nn.Softmax(dim=0),\n",
    "    PrintLayer()\n",
    "\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          ...,\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668]],\n",
      "\n",
      "         [[0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          ...,\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668]],\n",
      "\n",
      "         [[0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          ...,\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668],\n",
      "          [0.0668, 0.0668, 0.0668,  ..., 0.0668, 0.0668, 0.0668]]]],\n",
      "       device='cuda:1')\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1',\n",
      "       grad_fn=<NativeDropoutBackward0>)\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:1',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "tensor([ 0.1567, -0.1568, -0.1439,  0.2151,  0.0262,  0.0360, -0.0906, -0.1170,\n",
      "         0.0822, -0.3029, -0.0478], device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([0.1086, 0.0793, 0.0804, 0.1151, 0.0953, 0.0962, 0.0848, 0.0826, 0.1008,\n",
      "        0.0686, 0.0885], device='cuda:1', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = preprocess_im(x_train[0])\n",
    "\n",
    "\n",
    "train_prediction = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('2.weight',\n",
       "              tensor([[[[-5.5373e-01,  1.4270e-01,  5.2896e-01],\n",
       "                        [-5.8312e-01,  3.5655e-01,  7.6566e-01],\n",
       "                        [-6.9022e-01, -4.8019e-02,  4.8409e-01]],\n",
       "              \n",
       "                       [[ 1.7548e-01,  9.8630e-03, -8.1413e-02],\n",
       "                        [ 4.4089e-02, -7.0323e-02, -2.6035e-01],\n",
       "                        [ 1.3239e-01, -1.7279e-01, -1.3226e-01]],\n",
       "              \n",
       "                       [[ 3.1303e-01, -1.6591e-01, -4.2752e-01],\n",
       "                        [ 4.7519e-01, -8.2677e-02, -4.8700e-01],\n",
       "                        [ 6.3203e-01,  1.9308e-02, -2.7753e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.3254e-01,  1.2666e-01,  1.8605e-01],\n",
       "                        [-4.2805e-01, -2.4349e-01,  2.4628e-01],\n",
       "                        [-2.5066e-01,  1.4177e-01, -5.4864e-03]],\n",
       "              \n",
       "                       [[-1.4076e-01, -2.1903e-01,  1.5041e-01],\n",
       "                        [-8.4127e-01, -3.5176e-01,  5.6398e-01],\n",
       "                        [-2.4194e-01,  5.1928e-01,  5.3915e-01]],\n",
       "              \n",
       "                       [[-3.1432e-01, -3.7048e-01, -1.3094e-01],\n",
       "                        [-4.7144e-01, -1.5503e-01,  3.4589e-01],\n",
       "                        [ 5.4384e-02,  5.8683e-01,  4.9580e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7715e-01,  5.2149e-01,  9.8740e-03],\n",
       "                        [-2.7185e-01, -7.1709e-01,  3.1292e-01],\n",
       "                        [-7.5753e-02, -2.2079e-01,  3.3455e-01]],\n",
       "              \n",
       "                       [[ 3.0924e-01,  6.7071e-01,  2.0546e-02],\n",
       "                        [-4.6607e-01, -1.0697e+00,  3.3501e-01],\n",
       "                        [-8.0284e-02, -3.0522e-01,  5.4460e-01]],\n",
       "              \n",
       "                       [[ 3.1572e-01,  4.2335e-01, -3.4976e-01],\n",
       "                        [ 8.6354e-02, -4.6457e-01,  1.1803e-02],\n",
       "                        [ 1.0483e-01, -1.4584e-01, -1.5765e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 7.7599e-02,  1.2692e-01,  3.2305e-02],\n",
       "                        [ 2.2131e-01,  2.4681e-01, -4.6637e-02],\n",
       "                        [ 4.6407e-02,  2.8246e-02,  1.7528e-02]],\n",
       "              \n",
       "                       [[-1.8327e-01, -6.7425e-02, -7.2120e-03],\n",
       "                        [-4.8855e-02,  7.0427e-03, -1.2883e-01],\n",
       "                        [-6.4601e-02, -6.4566e-02,  4.4235e-02]],\n",
       "              \n",
       "                       [[-2.2547e-01, -1.1931e-01, -2.3425e-02],\n",
       "                        [-9.9171e-02, -1.5143e-02,  9.5385e-04],\n",
       "                        [-2.6137e-02,  1.3567e-03,  1.4282e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6520e-02, -3.2225e-02, -3.8450e-03],\n",
       "                        [-6.8206e-02, -1.9445e-01, -1.4166e-01],\n",
       "                        [-6.9528e-02, -1.8340e-01, -1.7422e-01]],\n",
       "              \n",
       "                       [[ 4.2781e-02, -6.7529e-02, -7.0309e-03],\n",
       "                        [ 1.1765e-02, -1.4958e-01, -1.2361e-01],\n",
       "                        [ 1.0205e-02, -1.0393e-01, -1.1742e-01]],\n",
       "              \n",
       "                       [[ 1.2661e-01,  8.5046e-02,  1.3066e-01],\n",
       "                        [ 1.7585e-01,  1.1288e-01,  1.1937e-01],\n",
       "                        [ 1.4656e-01,  9.8892e-02,  1.0348e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.2176e-02, -1.0766e-01, -2.6388e-01],\n",
       "                        [ 2.7957e-01, -3.7416e-02, -2.5471e-01],\n",
       "                        [ 3.4872e-01,  3.0041e-02, -5.5898e-02]],\n",
       "              \n",
       "                       [[ 2.5063e-01,  1.5543e-01, -1.7432e-01],\n",
       "                        [ 3.9255e-01,  3.2306e-02, -3.5191e-01],\n",
       "                        [ 1.9299e-01, -1.9898e-01, -2.9713e-01]],\n",
       "              \n",
       "                       [[ 4.6032e-01,  4.3399e-01,  2.8352e-01],\n",
       "                        [ 1.6341e-01, -5.8165e-02, -1.9196e-01],\n",
       "                        [-1.9521e-01, -4.5630e-01, -4.2732e-01]]]], device='cuda:1')),\n",
       "             ('2.bias',\n",
       "              tensor([ 0.4034,  0.3778,  0.4644, -0.3228,  0.3940, -0.3953,  0.3951, -0.5496,\n",
       "                       0.2693, -0.7602, -0.3508,  0.2334, -1.3239, -0.1694,  0.3938, -0.1026,\n",
       "                       0.0460, -0.6995,  0.1549,  0.5628,  0.3011,  0.3425,  0.1073,  0.4651,\n",
       "                       0.1295,  0.0788, -0.0492, -0.5638,  0.1465, -0.3890, -0.0715,  0.0649,\n",
       "                       0.2768,  0.3279,  0.5682, -1.2640, -0.8368, -0.9485,  0.1358,  0.2727,\n",
       "                       0.1841, -0.5325,  0.3507, -0.0827, -1.0248, -0.6912, -0.7711,  0.2612,\n",
       "                       0.4033, -0.4802, -0.3066,  0.5807, -1.3325,  0.4844, -0.8160,  0.2386,\n",
       "                       0.2300,  0.4979,  0.5553,  0.5230, -0.2182,  0.0117, -0.5516,  0.2108],\n",
       "                     device='cuda:1')),\n",
       "             ('6.0.weight',\n",
       "              tensor([[-0.0011, -0.0027,  0.0022,  ...,  0.0066, -0.0004, -0.0021],\n",
       "                      [ 0.0052,  0.0020,  0.0046,  ..., -0.0054, -0.0045, -0.0019],\n",
       "                      [-0.0005,  0.0052,  0.0018,  ...,  0.0068,  0.0005,  0.0091],\n",
       "                      ...,\n",
       "                      [-0.0075, -0.0096, -0.0025,  ..., -0.0079, -0.0106, -0.0036],\n",
       "                      [-0.0004,  0.0014, -0.0019,  ...,  0.0036,  0.0021,  0.0038],\n",
       "                      [ 0.0063,  0.0041, -0.0004,  ..., -0.0030,  0.0011,  0.0047]],\n",
       "                     device='cuda:1')),\n",
       "             ('6.0.bias',\n",
       "              tensor([ 0.0341,  0.0021,  0.0217,  ..., -0.0060,  0.0480,  0.0001],\n",
       "                     device='cuda:1')),\n",
       "             ('6.3.weight',\n",
       "              tensor([[-0.0113,  0.0104, -0.0017,  ..., -0.0161,  0.0121,  0.0065],\n",
       "                      [-0.0005, -0.0078,  0.0071,  ..., -0.0041,  0.0099, -0.0111],\n",
       "                      [-0.0109, -0.0052,  0.0168,  ..., -0.0036,  0.0035, -0.0224],\n",
       "                      ...,\n",
       "                      [-0.0107, -0.0073, -0.0038,  ..., -0.0025,  0.0083, -0.0054],\n",
       "                      [ 0.0054,  0.0081, -0.0136,  ...,  0.0041, -0.0004, -0.0023],\n",
       "                      [ 0.0288,  0.0066,  0.0049,  ...,  0.0244,  0.0066, -0.0073]],\n",
       "                     device='cuda:1')),\n",
       "             ('6.3.bias',\n",
       "              tensor([0.0332, 0.0616, 0.0307,  ..., 0.0456, 0.0442, 0.0588], device='cuda:1')),\n",
       "             ('6.6.weight',\n",
       "              tensor([[ 1.3450e-02,  4.2154e-02, -2.3040e-03,  ..., -4.9025e-04,\n",
       "                        1.8880e-02, -1.4209e-02],\n",
       "                      [ 7.6020e-03,  4.7305e-02, -5.0164e-03,  ..., -4.9127e-03,\n",
       "                       -5.6295e-03, -1.4197e-02],\n",
       "                      [ 2.1238e-03, -1.2520e-02, -1.8903e-02,  ..., -1.0263e-02,\n",
       "                        3.0020e-02, -2.8852e-02],\n",
       "                      ...,\n",
       "                      [-9.8278e-03,  3.2054e-02,  3.5979e-02,  ..., -5.6409e-03,\n",
       "                        8.3202e-03, -7.5155e-03],\n",
       "                      [ 1.6646e-02, -1.1247e-03,  1.5044e-03,  ..., -9.1578e-03,\n",
       "                       -8.6418e-03, -2.0923e-02],\n",
       "                      [-6.4900e-06, -2.2274e-02,  5.2750e-04,  ...,  4.4403e-02,\n",
       "                       -9.4047e-03, -1.2332e-02]], device='cuda:1')),\n",
       "             ('6.6.bias',\n",
       "              tensor([ 2.0239e-02, -2.9844e-02, -6.0355e-03,  5.3569e-03,  2.4703e-02,\n",
       "                       9.4541e-04,  7.6767e-03, -1.2997e-02, -1.7257e-02,  2.4917e-03,\n",
       "                      -2.9578e-03, -1.2410e-02,  1.9179e-02,  2.2878e-02,  1.6132e-02,\n",
       "                      -1.5368e-02,  1.3967e-03,  4.3597e-04,  1.4483e-02,  1.6664e-02,\n",
       "                       1.0173e-02,  2.3670e-02, -1.2096e-03,  1.6110e-02, -5.3769e-03,\n",
       "                      -1.1735e-02, -1.0727e-02, -2.9511e-02, -7.2404e-03,  3.9187e-02,\n",
       "                       1.1590e-02,  2.8063e-03, -5.4244e-03, -4.0255e-03,  1.3839e-04,\n",
       "                      -1.1612e-02, -1.8595e-03,  1.5770e-02, -1.7732e-02,  1.0556e-02,\n",
       "                       1.2518e-02,  6.8095e-03, -1.2896e-02, -4.4475e-03,  3.5410e-03,\n",
       "                      -2.7491e-03,  2.2288e-02,  2.3229e-02,  1.0078e-02,  1.2641e-02,\n",
       "                       7.9056e-03,  2.1109e-02, -1.6828e-02,  6.2936e-03, -2.8307e-02,\n",
       "                       7.4882e-03, -2.9438e-02,  4.9023e-02,  4.9702e-02,  3.8240e-02,\n",
       "                      -6.2107e-02,  1.7567e-02,  1.3668e-02,  5.5128e-02,  6.3641e-02,\n",
       "                      -1.5907e-02, -2.1543e-02,  2.4153e-02,  2.9221e-03,  3.6656e-02,\n",
       "                      -2.5589e-04,  1.0257e-02,  6.0446e-02, -2.0747e-02, -2.8418e-02,\n",
       "                       3.8297e-02,  2.2449e-02,  1.2476e-02, -1.6173e-02, -6.9245e-03,\n",
       "                      -1.2505e-02, -5.7967e-03, -1.4107e-02, -3.1422e-02, -1.4501e-02,\n",
       "                       2.6878e-02,  8.3460e-04,  6.1786e-02, -2.5483e-02,  4.5734e-02,\n",
       "                      -2.3228e-02, -4.4627e-04, -1.7937e-02, -2.0320e-02, -9.1343e-04,\n",
       "                      -6.4768e-03,  2.8838e-03,  1.1061e-02, -6.5366e-03, -2.7389e-02,\n",
       "                      -6.6679e-05,  1.2584e-02,  1.9082e-02,  3.7356e-02,  5.4718e-03,\n",
       "                       3.8678e-02,  6.9227e-03, -2.8615e-02, -6.7014e-03, -4.2353e-02,\n",
       "                      -1.1660e-01,  1.1963e-02,  1.0936e-03,  2.6548e-02,  1.8300e-02,\n",
       "                      -5.5016e-02, -1.6249e-03,  2.6446e-02, -2.7349e-03, -2.7034e-02,\n",
       "                       1.0505e-03, -3.8161e-02,  1.3213e-02, -4.9820e-03, -2.4466e-02,\n",
       "                       5.7482e-03, -6.1060e-02,  2.7150e-03,  5.9516e-03,  7.9525e-04,\n",
       "                      -6.7904e-02,  8.9988e-03,  4.4904e-02, -4.1445e-03, -4.8999e-02,\n",
       "                      -3.8526e-02, -3.0757e-02, -3.0013e-03,  4.4977e-03, -7.2840e-03,\n",
       "                       7.6950e-03,  5.5544e-03, -1.5949e-02, -3.3493e-02, -1.2481e-02,\n",
       "                      -1.9195e-02,  2.0324e-02,  5.3610e-03,  1.9483e-02,  1.8587e-02,\n",
       "                       5.8054e-03,  1.7311e-02, -8.2453e-02,  6.2503e-02, -1.2923e-02,\n",
       "                       2.0150e-02,  2.7138e-02, -1.7074e-03, -5.9132e-02,  2.4080e-02,\n",
       "                      -6.3872e-03,  3.1647e-02,  7.6580e-02, -3.4454e-04,  3.1730e-02,\n",
       "                      -2.8833e-02,  5.7331e-03, -3.9463e-02,  4.0252e-02,  8.9949e-03,\n",
       "                       3.5317e-02,  7.2347e-02, -5.3385e-04, -2.8689e-02,  6.2374e-02,\n",
       "                      -4.2100e-02,  1.2404e-02,  3.7113e-02,  4.2264e-02, -1.0894e-03,\n",
       "                       2.5798e-02,  1.3201e-02,  5.9872e-02,  2.6516e-02, -7.7218e-03,\n",
       "                       1.6828e-02,  1.9599e-02,  3.4458e-02, -8.9315e-03,  2.9612e-02,\n",
       "                      -1.1887e-02,  3.1144e-02,  3.7925e-02, -6.8350e-03,  2.9055e-02,\n",
       "                       7.0744e-02,  3.5352e-02,  4.5257e-03, -6.7377e-03,  4.8965e-02,\n",
       "                      -7.1239e-02,  6.5041e-03,  5.8401e-02,  5.3597e-02,  1.7927e-02,\n",
       "                       3.6933e-02, -1.9689e-02,  3.3865e-02, -1.3453e-02,  6.9940e-02,\n",
       "                       4.4340e-03, -1.8287e-02,  1.5174e-02, -2.2757e-02,  1.6309e-02,\n",
       "                       4.5446e-02, -2.8600e-02,  4.9436e-02,  7.5250e-03, -1.0546e-02,\n",
       "                      -2.2604e-02, -1.6502e-02,  4.5720e-02,  8.4474e-02, -1.2261e-02,\n",
       "                       7.2513e-02,  2.4530e-03,  6.4050e-03, -6.6105e-03,  7.0984e-02,\n",
       "                       3.5590e-02, -2.1918e-02,  6.7042e-02,  1.5241e-02,  6.0627e-02,\n",
       "                       1.0640e-02,  2.5195e-02,  4.0457e-02,  4.5375e-02,  6.4500e-02,\n",
       "                       1.4049e-02, -2.6033e-02,  6.9238e-03,  1.5410e-02,  1.0396e-02,\n",
       "                      -2.2500e-03, -2.1743e-02,  2.7361e-02, -4.0054e-03,  3.6920e-03,\n",
       "                       4.8795e-02,  2.2010e-02,  2.3484e-02, -3.5672e-02,  5.9931e-02,\n",
       "                       3.5723e-03,  1.0215e-02,  7.1785e-02,  1.5093e-02,  1.9409e-02,\n",
       "                      -1.0631e-02,  1.0041e-02, -3.5025e-02,  5.1083e-02,  6.7416e-03,\n",
       "                       1.5887e-02,  4.2009e-02, -5.0773e-02, -1.7900e-02,  2.7642e-02,\n",
       "                       2.0627e-02, -4.6130e-03,  2.1796e-02,  5.9296e-03, -3.2989e-02,\n",
       "                      -7.5952e-03,  1.9803e-02,  9.4290e-03, -8.7341e-03,  1.7098e-02,\n",
       "                      -2.2270e-02,  1.2361e-01, -3.8207e-02,  1.4395e-03,  5.4520e-02,\n",
       "                       4.8118e-02,  3.4782e-02, -4.7019e-03, -1.9831e-02,  3.8845e-02,\n",
       "                       6.0850e-03, -1.4381e-02,  4.9491e-02, -2.7058e-02, -2.4787e-02,\n",
       "                      -6.3279e-03,  1.4277e-02,  2.2483e-02, -4.9233e-02,  3.8480e-02,\n",
       "                      -2.9650e-02, -1.8599e-02, -1.9932e-03, -4.2109e-02, -6.0562e-02,\n",
       "                       1.7470e-02,  2.7788e-02, -3.4156e-02, -5.0233e-03, -2.0081e-02,\n",
       "                      -5.3831e-02, -7.0464e-03, -4.0125e-02,  5.2068e-02,  5.3731e-03,\n",
       "                       9.0153e-03, -3.2561e-05, -3.3795e-02, -2.7959e-02, -2.4056e-02,\n",
       "                      -1.0066e-02, -3.8689e-02, -2.3882e-02, -2.8881e-02,  1.9373e-03,\n",
       "                      -5.9984e-02, -1.3545e-02, -6.0373e-02, -3.1486e-02, -7.4332e-02,\n",
       "                       6.3616e-03, -3.0455e-02,  3.0601e-02,  5.2993e-02,  5.1747e-02,\n",
       "                       2.2551e-02,  9.0139e-03, -2.1590e-02, -4.8878e-02,  2.6130e-02,\n",
       "                      -1.2450e-03, -3.8225e-02,  6.8278e-03,  1.5274e-02,  2.5427e-02,\n",
       "                      -3.3353e-02,  3.3671e-02,  1.6623e-02, -1.6129e-02,  2.3300e-02,\n",
       "                       2.6170e-02, -8.3109e-03, -1.3620e-02, -1.0198e-02,  1.2645e-02,\n",
       "                      -2.7044e-03, -5.1699e-02, -1.2183e-02, -4.2912e-03,  7.0602e-02,\n",
       "                      -1.0591e-02,  3.3304e-02,  1.5505e-03,  2.7523e-02,  3.5229e-02,\n",
       "                       2.6252e-02,  4.7357e-02,  2.9449e-02, -1.5236e-02,  4.1031e-02,\n",
       "                      -1.1941e-02, -3.8416e-02,  3.4909e-02,  2.5368e-02,  5.3995e-04,\n",
       "                       3.2529e-03,  2.0679e-02,  1.6295e-02,  5.9731e-02,  1.5224e-02,\n",
       "                      -6.0552e-02, -1.7674e-02,  6.6975e-02,  1.4761e-02, -1.9037e-02,\n",
       "                      -3.4594e-03,  3.7793e-02, -5.1098e-03,  5.8897e-03,  1.9403e-02,\n",
       "                      -2.3564e-02,  1.9174e-02, -2.2748e-02, -2.0098e-02, -5.0562e-03,\n",
       "                       2.3758e-02,  6.6555e-03, -7.3947e-03, -5.6201e-02,  2.3188e-02,\n",
       "                       3.3316e-03,  2.4976e-02, -1.3101e-02,  2.4161e-02, -8.5088e-03,\n",
       "                       4.9697e-02,  1.0554e-02, -7.4188e-02, -7.5903e-02, -2.4110e-02,\n",
       "                       1.1068e-03, -3.4545e-03, -1.6059e-02,  3.6029e-02, -2.8019e-03,\n",
       "                      -2.6729e-02,  3.9026e-02, -4.2616e-02, -9.9599e-03, -8.3776e-03,\n",
       "                       2.8448e-02, -1.2167e-02, -5.3328e-02,  3.4896e-02,  5.4654e-02,\n",
       "                       8.9463e-03, -4.2294e-02, -6.1719e-02,  7.0205e-03, -3.2173e-02,\n",
       "                      -3.7807e-03,  5.2077e-02, -2.3974e-02,  1.7426e-02, -6.8874e-02,\n",
       "                      -2.6062e-03,  3.0595e-02, -1.5023e-02,  1.4022e-02, -5.0861e-02,\n",
       "                       3.7215e-03, -2.6806e-03,  3.5630e-02, -3.0362e-02,  2.2297e-02,\n",
       "                       4.9580e-02,  1.2369e-03, -3.4180e-02,  6.0701e-02,  3.3855e-02,\n",
       "                       6.1321e-03,  7.2363e-03, -1.6917e-03,  3.6149e-02,  2.1222e-02,\n",
       "                      -3.3272e-02,  1.9069e-02, -6.0149e-03, -2.7366e-02,  2.8984e-03,\n",
       "                      -3.4129e-03,  1.6887e-02, -3.5749e-03, -6.8191e-02,  2.3846e-02,\n",
       "                       1.6106e-02,  4.3023e-02,  1.3928e-02,  2.9887e-02,  1.4960e-02,\n",
       "                      -2.2989e-02,  2.0963e-02, -2.8435e-02,  2.0125e-02,  3.3749e-02,\n",
       "                       8.2363e-02, -2.5720e-02,  1.0292e-01,  1.2403e-03, -2.7880e-02,\n",
       "                       5.2865e-02, -2.4874e-02,  5.8873e-03, -4.3715e-03, -2.7919e-02,\n",
       "                       3.2519e-02, -1.7567e-02,  5.1975e-02, -8.3832e-02,  1.4786e-02,\n",
       "                      -1.2771e-02,  4.5308e-02,  2.4353e-02, -4.0642e-02, -5.5089e-02,\n",
       "                      -2.2368e-02,  3.3539e-02, -7.9810e-02,  2.2758e-02, -1.9840e-02,\n",
       "                       9.0427e-03, -2.4242e-02, -9.2313e-02,  1.1919e-02,  2.4050e-02,\n",
       "                       3.2105e-02, -8.4704e-02, -1.6657e-02, -3.0554e-03,  4.6309e-03,\n",
       "                      -2.3363e-02, -4.5799e-03,  1.5235e-02,  1.2100e-02,  1.2569e-02,\n",
       "                       1.6497e-02, -1.9934e-02,  3.3158e-02, -8.1766e-03, -2.0283e-02,\n",
       "                      -3.6283e-02, -7.5604e-03,  2.8872e-02,  2.8258e-03,  4.4513e-02,\n",
       "                       4.7551e-03,  7.7385e-02,  5.9271e-02, -3.0499e-02, -8.8495e-05,\n",
       "                      -1.0404e-02, -1.9523e-02, -8.4611e-04, -2.3710e-02,  1.6756e-02,\n",
       "                       3.8044e-02, -2.4882e-02,  2.7985e-02, -2.4580e-03, -2.7513e-03,\n",
       "                      -1.4628e-02, -4.4961e-02, -2.0490e-03, -4.6212e-02, -7.0188e-03,\n",
       "                      -5.3620e-03, -3.9620e-03, -1.1776e-02,  2.6604e-02,  2.2971e-02,\n",
       "                       2.5485e-02,  2.3256e-02, -3.0783e-02, -4.4985e-02, -2.1356e-02,\n",
       "                      -1.3835e-02, -3.7883e-02,  1.3674e-02, -3.5847e-02, -6.0238e-02,\n",
       "                      -1.6030e-02,  9.9658e-03, -3.9750e-02,  5.1179e-02,  3.4657e-02,\n",
       "                      -2.8058e-02,  8.1699e-04, -4.7458e-02, -5.6738e-03,  2.1224e-02,\n",
       "                       4.1393e-02, -1.0330e-02, -2.4760e-02, -2.8965e-02,  4.1361e-03,\n",
       "                       2.6227e-02,  1.1228e-02,  4.7628e-02, -2.4562e-02,  1.7258e-02,\n",
       "                      -4.8839e-02, -2.7126e-02, -7.2327e-02,  4.3933e-02, -3.7270e-02,\n",
       "                       2.8985e-03,  2.5128e-02,  2.5233e-02, -9.0966e-03,  3.1926e-02,\n",
       "                      -1.4033e-02, -5.3370e-03,  2.5925e-02, -8.3037e-03,  1.8597e-02,\n",
       "                      -3.4749e-02, -1.6617e-02,  8.2843e-02,  1.5086e-02, -5.0715e-02,\n",
       "                      -3.2014e-02, -3.3381e-03,  9.9305e-04,  3.2323e-02, -9.5939e-03,\n",
       "                       1.8850e-02,  3.1632e-02, -5.2701e-02,  8.1888e-03, -2.6958e-03,\n",
       "                       3.2904e-02, -2.0901e-02,  3.1157e-03,  2.2588e-02, -2.1898e-02,\n",
       "                      -4.5704e-02, -4.8732e-02,  4.9820e-02, -3.4660e-02, -5.2025e-02,\n",
       "                       5.6527e-02, -2.0000e-03,  4.7569e-03, -1.0292e-02, -1.2004e-02,\n",
       "                      -2.7112e-02, -4.4175e-03,  4.7015e-02,  9.4183e-03, -1.0293e-02,\n",
       "                       2.4827e-02, -2.1165e-02, -3.4728e-02,  2.6178e-02,  3.3729e-02,\n",
       "                      -1.0051e-02, -2.4959e-02, -4.0963e-02, -6.2759e-02,  7.6392e-03,\n",
       "                      -2.4506e-02, -3.5312e-02,  5.4566e-03, -5.4158e-02, -7.5320e-02,\n",
       "                      -1.8790e-02, -2.0657e-02, -1.2283e-02,  1.9106e-02,  1.3047e-02,\n",
       "                       1.1992e-02,  4.5593e-02, -1.0798e-02,  2.5652e-02,  2.9297e-02,\n",
       "                       7.1481e-02,  3.4396e-02,  1.7480e-02, -1.9279e-02, -3.8183e-02,\n",
       "                       4.4221e-02,  2.0793e-02,  1.7786e-02, -4.1785e-03,  1.4240e-02,\n",
       "                       4.6463e-02, -6.6139e-04,  1.8974e-03,  2.7180e-02,  2.9930e-02,\n",
       "                       3.3525e-02,  7.7600e-02,  3.6428e-02,  2.0724e-02,  4.4268e-02,\n",
       "                       6.6297e-03, -8.3840e-02,  1.1172e-02,  5.1371e-03, -1.3041e-02,\n",
       "                       1.1880e-02,  2.2298e-02,  1.2055e-02, -1.6229e-02, -2.8393e-02,\n",
       "                       2.8677e-02, -1.5870e-03,  5.2088e-05, -2.1988e-02, -1.6203e-02,\n",
       "                       8.4714e-03,  5.5201e-02, -1.0171e-02, -6.9343e-02, -1.8714e-02,\n",
       "                       4.0182e-02,  1.8755e-02,  2.8231e-03, -1.4027e-02, -1.4812e-02,\n",
       "                      -2.5366e-02, -2.1691e-02, -3.7325e-02,  1.9513e-02,  4.1396e-02,\n",
       "                      -6.7270e-02,  5.4385e-02,  3.6110e-02, -3.2586e-02, -1.5797e-02,\n",
       "                      -1.4585e-02, -4.5290e-03, -2.4140e-02,  4.1866e-02, -4.2935e-02,\n",
       "                       3.4227e-02,  1.4480e-02,  2.3407e-03, -1.0121e-02,  1.3590e-02,\n",
       "                       1.0312e-02, -3.6855e-02, -5.6961e-03, -4.5189e-02, -8.2578e-03,\n",
       "                      -2.2380e-05,  5.0056e-02,  6.8531e-03, -6.7496e-03, -3.3575e-02,\n",
       "                      -6.2393e-02,  7.9509e-03,  1.4348e-02, -8.3250e-02, -3.0460e-02,\n",
       "                      -1.0488e-02, -2.0691e-02, -2.8651e-02,  3.6568e-02,  1.0958e-01,\n",
       "                       6.6219e-03, -2.2891e-02,  9.8843e-03,  4.3048e-02, -3.0546e-02,\n",
       "                       4.7700e-02, -3.5618e-02,  1.5470e-02, -1.3976e-02,  2.7720e-02,\n",
       "                      -2.5930e-02,  1.3437e-03,  2.8364e-02, -1.3526e-02, -6.8687e-02,\n",
       "                       1.7460e-02,  5.0171e-02,  1.7448e-02,  3.8029e-02,  9.8575e-03,\n",
       "                       2.2886e-02,  1.2857e-02, -2.6916e-02,  4.8951e-02, -9.6445e-03,\n",
       "                      -3.5893e-02, -2.6359e-02, -2.0544e-02,  3.6961e-03, -9.1653e-03,\n",
       "                       2.1354e-02, -4.2778e-02, -4.1528e-02, -3.8514e-02, -3.1796e-02,\n",
       "                       3.4630e-02,  4.7508e-02,  2.4460e-02, -6.2255e-02, -5.2742e-02,\n",
       "                       8.7197e-03, -4.4129e-03, -1.8273e-02, -3.5542e-02, -8.3854e-03,\n",
       "                       7.8142e-02,  1.6183e-02, -2.9596e-02,  4.5440e-03, -1.6409e-02,\n",
       "                      -6.8854e-02,  5.5268e-03, -9.1066e-04,  7.5423e-03,  3.0180e-04,\n",
       "                      -4.5678e-02,  2.2746e-02,  1.7497e-03, -3.3456e-02, -9.0392e-03,\n",
       "                      -6.2202e-02,  5.3786e-02,  3.8939e-04,  4.0257e-02,  3.1605e-03,\n",
       "                      -6.9488e-02, -3.2256e-02,  2.3048e-02,  2.6307e-03, -3.9207e-02,\n",
       "                      -5.0876e-02, -4.5844e-02,  3.2221e-02, -2.7124e-02, -7.5084e-03,\n",
       "                      -4.4676e-03,  4.1103e-02, -5.6786e-03, -3.9598e-02,  1.4338e-02,\n",
       "                       2.3843e-03,  1.4621e-02,  4.6011e-02,  7.8583e-04,  1.3014e-02,\n",
       "                       1.4411e-02, -2.8963e-02, -4.4605e-02,  8.5262e-03, -2.8862e-02,\n",
       "                      -5.3216e-03,  2.0222e-02,  3.5987e-03,  1.5160e-02,  8.0866e-02,\n",
       "                      -1.5476e-02,  4.4872e-02,  6.0198e-04, -1.2368e-02,  8.1617e-03,\n",
       "                      -3.7857e-03, -4.2203e-02,  2.6076e-02,  2.1671e-02, -1.6940e-02,\n",
       "                       1.3443e-02, -2.6324e-02,  3.6311e-02,  4.8506e-02, -3.7339e-03,\n",
       "                       1.4108e-02,  7.7902e-04, -6.0017e-02,  3.7350e-03, -2.6110e-02,\n",
       "                       5.0281e-03, -2.4351e-02, -2.7377e-02, -3.7669e-02,  3.8070e-02,\n",
       "                       1.0932e-02,  2.8160e-02, -3.5762e-02,  2.3261e-02,  1.9776e-02,\n",
       "                      -3.8261e-03, -6.5859e-03, -1.8929e-02, -4.5027e-02, -1.1969e-02,\n",
       "                       8.7756e-03, -2.6216e-02, -2.1189e-02,  1.2913e-02, -1.8512e-02,\n",
       "                      -2.6655e-02,  4.0894e-03,  1.6387e-03, -1.6818e-02, -1.0804e-02,\n",
       "                       3.9582e-02,  1.8490e-02,  2.5662e-02, -4.7438e-02,  1.8842e-02,\n",
       "                      -3.9618e-02, -2.2390e-02, -4.7946e-03,  6.2678e-04,  1.4640e-03,\n",
       "                      -2.9613e-02, -2.3728e-02,  5.8741e-03,  3.9343e-02, -4.4972e-03,\n",
       "                       1.2785e-03,  1.9657e-02,  8.6271e-03,  9.4731e-03, -8.3145e-02,\n",
       "                      -9.5304e-03,  5.8406e-03, -2.7983e-02,  1.1320e-02,  5.2982e-03,\n",
       "                       2.1871e-02, -8.8594e-04, -3.8309e-03, -5.7810e-02,  3.0410e-03,\n",
       "                      -2.1917e-02, -7.4865e-02, -4.3740e-02,  2.0407e-02, -3.4062e-03,\n",
       "                       1.8807e-02, -1.7270e-02,  1.7385e-02,  4.7496e-02,  6.6395e-02,\n",
       "                      -1.7039e-02, -1.0944e-02,  7.3278e-03,  2.6257e-02, -2.1868e-02,\n",
       "                      -1.0651e-02,  2.5665e-02, -3.9925e-02,  9.3388e-03,  6.9712e-02,\n",
       "                      -1.0000e-02, -3.2376e-02, -7.8124e-02, -2.6672e-02, -1.0220e-04,\n",
       "                       2.9822e-02,  6.2931e-03, -1.5316e-02, -2.4690e-02, -2.3386e-02,\n",
       "                      -8.5112e-03, -7.5672e-03, -3.5472e-02,  8.3757e-03, -4.4933e-02,\n",
       "                      -2.5905e-02, -3.5467e-02, -7.1374e-02,  9.2666e-03, -7.3159e-02,\n",
       "                      -9.6658e-02, -3.1080e-02, -3.5131e-02, -1.6773e-02, -2.2672e-02,\n",
       "                       7.4318e-03, -9.8196e-03, -4.4741e-02, -2.3274e-02, -4.2749e-02,\n",
       "                      -1.0063e-02,  4.5505e-03, -1.6073e-02, -4.8172e-02, -4.4076e-02,\n",
       "                       6.1854e-02, -9.5113e-03,  2.6095e-02, -8.6355e-02,  1.6138e-02,\n",
       "                      -8.2407e-03,  5.0933e-02,  1.8536e-03, -5.1323e-02,  2.2101e-02,\n",
       "                      -3.0345e-03,  3.6723e-02,  2.7383e-02, -1.4681e-02,  1.0468e-02,\n",
       "                       1.5814e-02,  1.8042e-02,  5.9351e-02, -1.9994e-04, -5.2148e-02,\n",
       "                      -2.8923e-02, -3.6363e-02, -6.5360e-02, -6.3748e-04, -2.7562e-02,\n",
       "                      -4.4246e-02, -4.8769e-02, -4.5221e-02, -5.8486e-02, -3.1612e-02,\n",
       "                      -1.4006e-02, -5.0941e-02,  2.0858e-02, -3.2510e-02, -7.1847e-03],\n",
       "                     device='cuda:1')),\n",
       "             ('10.weight',\n",
       "              tensor([[ 0.0028,  0.0155,  0.0051,  ..., -0.0027,  0.0071, -0.0090],\n",
       "                      [-0.0152,  0.0131,  0.0095,  ..., -0.0073, -0.0043,  0.0033],\n",
       "                      [ 0.0104,  0.0150, -0.0103,  ..., -0.0033, -0.0037, -0.0139],\n",
       "                      ...,\n",
       "                      [ 0.0088, -0.0137,  0.0043,  ..., -0.0028,  0.0024, -0.0117],\n",
       "                      [ 0.0064, -0.0052, -0.0022,  ..., -0.0039, -0.0051,  0.0005],\n",
       "                      [ 0.0075,  0.0134,  0.0142,  ...,  0.0048,  0.0061, -0.0027]],\n",
       "                     device='cuda:1')),\n",
       "             ('10.bias',\n",
       "              tensor([-0.0037,  0.0003, -0.0111,  0.0071, -0.0037,  0.0070, -0.0148, -0.0059,\n",
       "                       0.0033,  0.0014,  0.0018], device='cuda:1'))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = resnet101(weights=\"IMAGENET1K_V1\")print(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functions import import_imagedata, label_oh_tf, ImageProcessor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.models import resnet101\n",
    "\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from fns4wandb import train_log, build_optimizer\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_vgg16 = vgg16(weights=\"IMAGENET1K_V1\")\n",
    "# print(model_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Initialize model with the best available weights\n",
    "model_vgg16_featlayers = models.vgg16(weights=\"IMAGENET1K_FEATURES\").features#.to(device)\n",
    "model_vgg16_featlayers.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the inference transforms\n",
    "# This does some preprocessing behind the scenes,\n",
    "\n",
    "# 1) Resized of the input to resize_size=[256];\n",
    "# 2) Followed by a central crop of crop_size=[224];\n",
    "\n",
    "# vgg16 and resnet accept input image size of 224Ã—224\n",
    "\n",
    "\n",
    "\n",
    "#print(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MLP, linear classification, function\n",
    "\n",
    "class Three_Lay_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Three_Lay_MLP, self).__init__()\n",
    "        \n",
    "        self.lins = nn.Sequential(\n",
    "            nn.Linear(3584, 100), #1024x7 and 1024x100. 7, 1024\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(100,11),\n",
    "            nn.Linear(11,11),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.lins(x)\n",
    "        return x\n",
    "        \n",
    "#lin    conv output -> 100\n",
    "#relu    ()\n",
    "#lin    100 -> 100\n",
    "#relu    ()\n",
    "#do     ~0.5\n",
    "#lin    100 -> 11\n",
    "#softmax ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(label, prediction): #TypeError: Singleton array tensor(3) cannot be considered a valid collection.\n",
    "    #print('l  ',label) #len(11)\n",
    "    label= np.array(label)\n",
    "    print(type(label)) #'y_pred' parameter of f1_score must be an array-like or a sparse matrix. Got 7 instead.\n",
    "    #label = label.argmax()\n",
    "    #print('l  ',label)\n",
    "    predictions_np = prediction.detach().numpy()\n",
    "    #print('p  ',predictions_np)\n",
    "    predicted_classes = np.argmax(predictions_np, axis=0)\n",
    "    #predicted_classes = prediction.argmax()\n",
    "    avg_f1_score = f1_score(label, predicted_classes, average='macro')\n",
    "    acc = accuracy_score(label, predicted_classes)\n",
    "    \n",
    "    return avg_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feats_28' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8e782b93b222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# normalize the feature vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mscaled_feats_28\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats_28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'feats_28' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# creating a function for normalizing the feature vector\n",
    "# using a standard scaling (results in mean=0 and standard deviation=1)\n",
    "def scale_feats(feat_vec):\n",
    "  # Scaling the features to the same range of values\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(feat_vec)\n",
    "  scaled_feat_vec = scaler.transform(feat_vec)\n",
    "  print(\"\\n A peek at the scaled dataset features: \\n\"+str(scaled_feat_vec))\n",
    "\n",
    "  return scaled_feat_vec\n",
    "\n",
    "# normalize the feature vector\n",
    "scaled_feats_28 = scale_feats(feats_28)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class AutoFeature(): # file_path, number\\n    def __init__(self, img_files, labels):\\n        self.img_files = img_files\\n        self.label = label_oh_tf(labels, device='cpu', num_classes=11)\\n        \\n        #self.labels=[label_oh_tf(label, device='cpu', num_classes=11) for label in labels]\\n        #self.labels= [int(label) for label in labels]\\n        #self.labels = torch.tensor(labels, dtype=torch.long) # tensofy the labels\\n        #self.labels = label_oh_tf(labels, device, 11)\\n    #def __len__(self):\\n    #    return len(self.labels)\\n    def __getitem__(self):\\n        #given an index, will return items within that index range\\n        # extracting features using pretrained model\\n        feats = get_img_feats(self.img_files) #[idx]\\n        \\n        feats_tensor = torch.tensor(feats, dtype=torch.float32)\\n        #print(type(self.labels))\\n        #print(self.labels[3])\\n        return feats_tensor, self.label #[idx]\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temi code. week 6. notebook used vgg16 pretrained features, and trained a MLP for classification of beans\n",
    "# this is the method I am following \n",
    "\n",
    "IP = ImageProcessor(device='cpu')\n",
    "\n",
    "# Temi func. get features from passing through images into vgg16 conv/ feature layers.\n",
    "# creating a function to get features learnt in the pretrained VGG16\n",
    "# extracting these features for our own list of images\n",
    "def get_img_feats(img_path): \n",
    "    #print(img_path)\n",
    "    img = cv2.imread(img_path) #\n",
    "    #print(img)\n",
    "    img = IP.blank_padding(img, (224,224))\n",
    "    img = IP.to_tensor(img)\n",
    "    #print('image shape post read and pad: \\n ',img.shape)\n",
    "    # Step 3: Apply inference preprocessing transforms\n",
    "    #img = preprocess(img).unsqueeze(0) # preprocess is the model with weights\n",
    "    img = preprocess(img)#.unsqueeze(0)\n",
    "\n",
    "    # Step 4: Use the model and print the predicted category\n",
    "    auto_feats = model_vgg16_featlayers(img).squeeze(0)\n",
    "    auto_feats = auto_feats.detach().numpy()\n",
    "    auto_feats = np.mean(auto_feats, axis=1,keepdims=False) #, keepdims=False\n",
    "    #print('get_img_feats autofeats', auto_feats)\n",
    "    #print('auto_feat: \\n ', auto_feats)\n",
    "    return auto_feats#.to(device)\n",
    "\n",
    "\n",
    "# class to manage data and turn imgs into vgg16 features\n",
    "\n",
    "\"\"\"class AutoFeature(): # file_path, number\n",
    "    def __init__(self, img_files, labels):\n",
    "        self.img_files = img_files\n",
    "        self.label = label_oh_tf(labels, device='cpu', num_classes=11)\n",
    "        \n",
    "        #self.labels=[label_oh_tf(label, device='cpu', num_classes=11) for label in labels]\n",
    "        #self.labels= [int(label) for label in labels]\n",
    "        #self.labels = torch.tensor(labels, dtype=torch.long) # tensofy the labels\n",
    "        #self.labels = label_oh_tf(labels, device, 11)\n",
    "    #def __len__(self):\n",
    "    #    return len(self.labels)\n",
    "    def __getitem__(self):\n",
    "        #given an index, will return items within that index range\n",
    "        # extracting features using pretrained model\n",
    "        feats = get_img_feats(self.img_files) #[idx]\n",
    "        \n",
    "        feats_tensor = torch.tensor(feats, dtype=torch.float32)\n",
    "        #print(type(self.labels))\n",
    "        #print(self.labels[3])\n",
    "        return feats_tensor, self.label #[idx]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3432\n"
     ]
    }
   ],
   "source": [
    "print(img_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get len of \n",
    "\n",
    "file_path = r'/its/home/nn268/optics/AugmentedDS_IDSW/'\n",
    "random_seed =1\n",
    "img_len = len(os.listdir(file_path))\n",
    "\n",
    "ids = np.arange(0, img_len)\n",
    "#print(ids[4])\n",
    "\n",
    "\n",
    "train_ids, test_ids = train_test_split(ids, test_size=0.2, train_size=0.8,\n",
    "                                 random_state=random_seed, shuffle=True)\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.1, train_size=0.8,\n",
    "                                 random_state=random_seed, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AutoFeature' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-05c8e50ce1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'AutoFeature' object is not callable"
     ]
    }
   ],
   "source": [
    "type(train(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaughticalnonsence\u001b[0m (\u001b[33mantvis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3374 2376  625 3354]\n",
      "x :  /its/home/nn268/optics/AugmentedDS_IDSW/IDSW008_170823_1109_SW_361.JPG_Augmented_right_1.JPG\n",
      "y: 7\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# split data intro train, val, test\n",
    "\"\"\"\n",
    "x, y = import_imagedata(file_path)\n",
    "train_set = AutoFeature(x[train_ids], y[train_ids])\n",
    "val_set = AutoFeature(x[val_ids], y[val_ids])\n",
    "test_set = AutoFeature(x[test_ids], y[test_ids])\n",
    "\n",
    "#print('train set 0: \\n',train_set[0])\n",
    "#print('what is train set?', type(train_set))\n",
    "#print('train set len', len(train_set))\n",
    "\n",
    "for i in train_set:\n",
    "    print('i 0 \\n',i[0])\n",
    "    print('i 1 \\n',i[1])\n",
    "    break\n",
    "\n",
    "print('testing forloop')\n",
    "print(type(train_set))\n",
    "x_train = [i[0] for i in train_set]\n",
    "y_train = [i[1] for i in train_set]\n",
    "print('testend')\n",
    "print(x_train[0])\n",
    "print(y_train[0])\"\"\"\n",
    "#x_train, y_train = train_set\n",
    "#x_val, y_val = val_set\n",
    "#x_test, y_test = test_set\n",
    "#train_dl = DataLoader(train_set, batch_size=1)\n",
    "#val_dl = DataLoader(val_set, batch_size=1)\n",
    "#test_dl = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "print(train_ids[:4])\n",
    "idx= 2\n",
    "x, y = import_imagedata(file_path)\n",
    "\n",
    "print('x : ',x[idx])\n",
    "print('y:', y[idx])\n",
    "\n",
    "train = AutoFeature(x[idx], y[idx])\n",
    "j,k = train.__getitem__()\n",
    "print(type(j))\n",
    "print(type(k))\n",
    "#print('train: ',train[:], '\\n')\n",
    "#print('train 1', train, '\\n')\n",
    "#for i in train:\n",
    "#    print(i)\n",
    "#print('train 0',train[:10])\n",
    "#x_train = train[0]\n",
    "#label = train[1]\n",
    "\n",
    "#print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from fns4wandb import train_log, build_optimizer\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# split data intro train, val, test\n",
    "\n",
    "# instancing the autofeature class\n",
    "# creating an object for train, val test\n",
    "\n",
    "#train_set = AutoFeature(x[train_ids], y[train_ids])\n",
    "#val_set = AutoFeature(x[val_ids], y[val_ids])\n",
    "#test_set = AutoFeature(x[test_ids], y[test_ids])\n",
    "\n",
    "#print('train set 0: \\n',train_set[0])\n",
    "#print('what is train set?', type(train_set))\n",
    "#print('train set len', len(train_set))\n",
    "\n",
    "\n",
    "\n",
    "#x_test, y_test = test_set\n",
    "#x_test = [i[0] for i in test_set]\n",
    "#y_test = [i[1] for i in test_set]\n",
    "\n",
    "#train_dl = DataLoader(train_set, batch_size=1)\n",
    "#val_dl = DataLoader(val_set, batch_size=1)\n",
    "#test_dl = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = dict(\n",
    "    epochs= 2, #30, \n",
    "    learning_rate =1e-5,\n",
    "    architecture ='CNN',\n",
    "    optimizer= 'adam',\n",
    "    weight_decay= 4e-5,\n",
    "    ks = 3,\n",
    "    scheduler=0.2,\n",
    "    f_lin_lay = 7168 #1024*7 = 7168\n",
    ")\n",
    "\n",
    "# pass in ids\n",
    "# create class instance for single ids\n",
    "# index that class to get img feature and label FOR THAT ids.\n",
    "# per epoch\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, loss_fn, train_ids, val_ids, config): #train_dl, val_dl, \n",
    "    #wandb.watch(model, loss_fn, log='all', log_freq=10)\n",
    "    \n",
    "    lr = config['learning_rate'] #1e-5 #config.learning_rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)#build_optimizer(model, optimizer=torch.optim.Adam(model.parameters(), lr=lr))#config.optimizer, config.learning_rate, config.weight_decay)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config['scheduler'], last_epoch=-1) #gamma=config.scheduler, last_epoch=-1)\n",
    "    \n",
    "    x, y = import_imagedata(file_path)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    best_acc = 0\n",
    "    losses= []\n",
    "    t_loss_list = []\n",
    "    v_loss_list = []\n",
    "    \n",
    "    for epoch in tqdm(range(config['epochs'])): #config.epochs)):\n",
    "        if epoch == 0:\n",
    "            best_model = deepcopy(model)\n",
    "        #train_ids = random.shuffle(train_ids)\n",
    "        #print(type(train_ids))\n",
    "        for idx in train_ids: #batch,(x_train, y_train) in enumerate(train_dl)\n",
    "\n",
    "            \n",
    "            x_train = get_img_feats(x[idx])\n",
    "            tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "            tensor = tensor.flatten()\n",
    "\n",
    "            train_prediction = model.forward(tensor)\n",
    "            #print(prediction.shape)\n",
    "            #print(train_prediction.argmax())\n",
    "            train_label = label_oh_tf(y[idx], device='cpu', num_classes=11)\n",
    "            #print(label.argmax())\n",
    "            t_loss = loss_fn(train_prediction, train_label)\n",
    "            \n",
    "            #train_avg_f1_score, train_acc = metrics(train_label, train_prediction)\n",
    "            t_loss_list.append(t_loss)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            t_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        for idx in val_ids:\n",
    "            \n",
    "            x_val = get_img_feats(x[idx])\n",
    "            #print(tens.shape)\n",
    "            tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "            y_val = label_oh_tf(y[idx], device='cpu', num_classes=11)\n",
    "            #print(label)\n",
    "            \n",
    "            val_prediction = model.forward(tensor)\n",
    "            #print(val_prediction.shape)\n",
    "            v_loss = loss_fn(val_prediction, y_val)\n",
    "            \n",
    "            v_loss_list.append(v_loss)\n",
    "            \n",
    "            #val_avg_f1_score, val_acc = metrics(y_val, val_prediction)\n",
    "        if val_acc > best_model_acc:\n",
    "            best_model_acc = val_acc\n",
    "            best_model = deep_copy(model)\n",
    "            print('improvment in metrics. model saved')\n",
    "        \n",
    "        losses.append([train_loss.item(), val_loss.item()])\n",
    "        #if (epoch+1)%2==0:\n",
    "        #    train_log(t_loss, v_loss, epoch)\n",
    "        #    #wandb.log({'train_accuracy_%': train_acc, 'epoch':epoch})\n",
    "        #    #wandb.log({'val_accuracy_%': val_acc, 'epoch':epoch})\n",
    "    model= best_model\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "\n",
    "\n",
    "def pipeline(config): \n",
    "\n",
    "    title = f\"IDSW_on_Vgg16_Convs_training_MLP_test1\"\n",
    "    \n",
    "    loss_list=[]\n",
    "    #loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    #with wandb.init(project=title, config=config):\n",
    "    #config = wandb.config\n",
    "    model = Three_Lay_MLP()\n",
    "\n",
    "    model, loss_list = train_model(model, loss_fn,  train_ids, val_ids, config) #train_dl, val_dl\n",
    "        \n",
    "    return model, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1024x7 and 7168x100)\n",
    "\n",
    "7168/16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model, loss_list = pipeline(config) #7,168"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

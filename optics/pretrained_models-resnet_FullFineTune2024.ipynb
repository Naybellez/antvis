{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functions import import_imagedata, get_data, label_oh_tf,  Unwrap, ImageProcessor, IDSWDataSetLoader\n",
    "from fns4wandb import build_optimizer, set_optimizer, hp_sweep, train_model, train_log, log_test_score, set_lossfn, pipeline, test_loop_batch, batch_loop\n",
    "from loop_fns import res_loop, test_loop,test_loop_res\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.models import resnet101\n",
    "\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from fns4wandb import train_log, build_optimizer\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from fns4wandb import set_lossfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_resnet = resnet101(weights=\"IMAGENET1K_V1\")#.eval\n",
    "\n",
    "newmodel = torch.nn.Sequential(*(list(model_resnet.children())[:-1]))\n",
    "newmodel=newmodel.to(device)\n",
    "print(newmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(label, prediction): #TypeError: Singleton array tensor(3) cannot be considered a valid collection.\n",
    "    \n",
    "    label= np.array(label.cpu())\n",
    "\n",
    "    predictions_np = prediction.cpu().detach().numpy()\n",
    "    #y_pred' parameter of f1_score must be an array-like or a sparse matrix. Got 7 instead.\n",
    "    predicted_classes = np.argmax(predictions_np, axis=0)\n",
    "    #print('metrics Label:   ', label)\n",
    "    #print('metrics prediction   ', predicted_classes)\n",
    "    #avg_f1_score = f1_score(label, predictions_np, average='macro')\n",
    "    acc = accuracy_score(label, predicted_classes)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_im(img_path):\n",
    "    IP = ImageProcessor(device='cpu')\n",
    "    img = cv2.imread(img_path) #\n",
    "    img = IP.blank_padding(img, (224,224))\n",
    "    img = IP.to_tensor(img).to(device)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Squeeze, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Do your print / debug stuff here\n",
    "        x = x.squeeze(0)\n",
    "        x = x.squeeze(1)\n",
    "        x = x.squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#run_title = \"IDSW_resnet_fine_hp_80_112023\"\n",
    "\n",
    "\"\"\"config = {\n",
    "    'method': 'random',\n",
    "    'metric':{\n",
    "        'goal': 'minimize',\n",
    "        'name': 'val_loss'},\n",
    "    'parameters': {\n",
    "        #'dropout':{\n",
    "        #    'values': [0.5, 0.4, 0.3]\n",
    "        #},\n",
    "\n",
    "        'epochs':{\n",
    "            'value': 70\n",
    "        },\n",
    "\n",
    "        'first_lin_lay':{\n",
    "            'values':[248832]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam']\n",
    "        },\n",
    "            'learning_rate': {\n",
    "                # a flat distribution between 0 and 0.1\n",
    "                'distribution': 'log_uniform_values',\n",
    "                'min': 5e-5,\n",
    "                'max': 1e-2\n",
    "            },\n",
    "        'loss_fn': {\n",
    "            'values': ['CrossEntropy', 'MSE'] #'MSE', \n",
    "        },\n",
    "        'data_set':{\n",
    "            'values':['Augmented']\n",
    "        },\n",
    "            'scheduler': {\n",
    "            'values': [0.1, 0.01, 0.001]\n",
    "        },\n",
    "        'ks': {\n",
    "            'values': [(3,5)]\n",
    "        },\n",
    "\n",
    "        'channels':{\n",
    "            'values': [3]\n",
    "        },\n",
    "        'num_classes': {\n",
    "            'values': [11]\n",
    "        },\n",
    "        'model_name' : {'values': ['resnet_fine']},\n",
    "        'channels' : {'values': [3]},\n",
    "        'image_path': {\n",
    "            'values': [r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/']\n",
    "        }\n",
    "        }\n",
    "    }\"\"\"\n",
    "\n",
    "config = dict(\n",
    "    epochs= 120, #30, \n",
    "    learning_rate =1e-4,\n",
    "    architecture ='CNN',\n",
    "    optimizer= 'adam',\n",
    "    weight_decay= 1.0e-03,\n",
    "    ks = (3,5),\n",
    "    scheduler=0.5,\n",
    "    f_lin_lay = 2048,#8192,#5958,#65536,#8192,#7168, #1024*7 = 7168\n",
    "    dataset = 'IDSW_Aug_22672',\n",
    "    loss_fn = 'CrossEntropy',\n",
    "    model = \"resnet100\"\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "col_dict = {\n",
    "    'colour': 'colour',\n",
    "    'size': [226,72],\n",
    "    'padding': 5,\n",
    "    'model_size': 'resnet'\n",
    "}\n",
    "\n",
    "title = f\"IDSWAug_resnet_e120_{col_dict['size']}_DL_140224\"\n",
    "save_dict = {'Run' : title,\n",
    "            'Current_Epoch': 0,\n",
    "            'save_location' : r'pickles/'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wandb login 2bf372d1273e4af99733e10529509d9b252efe88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaughticalnonsence\u001b[0m (\u001b[33mantvis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweep_id = wandb.sweep(config, project=f\"{run_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_s1(data):\n",
    "    for i in data:\n",
    "        i = i.unsqueeze(i, 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures import PrintLayer, Flattern\n",
    "def train(device,col_dict, save_dict, config=None):\n",
    "    # lists for save dict\n",
    "    t_loss_list = []\n",
    "    v_loss_list =[]\n",
    "    t_predict_list = []\n",
    "    t_label_list = []\n",
    "    v_predict_list = []\n",
    "    v_label_list = []\n",
    "    t_accuracy_list= []\n",
    "    v_accuracy_list= []\n",
    "    \n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test = get_data(file_path= r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/', seed= random.randint(0, 50))\n",
    "        \n",
    "        \n",
    "        #x_train_loader = IDSWDataSetLoader(x_train, y_train, col_dict=col_dict, device=device) #x_train, y_train, col_dict=col_dict, device=device\n",
    "        #train_loader = DataLoader(x_train_loader, shuffle=True, batch_size=4)\n",
    "        #x_test_loader = IDSWDataSetLoader(x_test, y_test, col_dict=col_dict, device=device)\n",
    "        #train_loader = DataLoader(x_test_loader, shuffle=True, batch_size=4)\n",
    "        #x_val_loader = IDSWDataSetLoader(x_val, y_val, col_dict=col_dict, device=device)\n",
    "        #train_loader = DataLoader(x_val_loader, shuffle=True, batch_size=4)\n",
    "        \n",
    "        #model =smallnet3(in_chan=3, f_lin_lay=67968, l_lin_lay=11, ks=(3,5)).to(device) #10368\n",
    "        resnet = nn.Sequential(\n",
    "            newmodel,\n",
    "            Flattern(),\n",
    "            Flattern(),\n",
    "            nn.Linear(config['f_lin_lay'],11), #2048\n",
    "            nn.Softmax(dim=0),\n",
    "        )\n",
    "    \n",
    "        resnet.to(device)\n",
    "        model=resnet\n",
    "        \n",
    "        if config.loss_fn == 'MSE':\n",
    "            loss_fn = nn.MSELoss()\n",
    "        elif config.loss_fn == 'CrossEntropy':\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        e_count = 0\n",
    "         # *\n",
    "\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\n",
    "\n",
    "        for epoch in tqdm(range(config.epochs)):\n",
    "            # current_loss, predict_list, num_correct, label_list, model, optimizer\n",
    "            #t_loss, t_predict_list_, t_num_correct, t_label_list_, model, optimizer = batch_loop(model, train_loader, epoch, loss_fn, device, col_dict, num_classes=11, optimizer=optimizer)\n",
    "            t_loss, t_predict_list_, t_num_correct, t_label_list_, model, optimizer = res_loop(model, x_train, y_train, epoch, loss_fn, device, col_dict, num_classes=11, optimizer=optimizer)\n",
    "\n",
    "            t_accuracy = (t_num_correct /len(x_train))*100\n",
    "            t_loss_list.append(t_loss)\n",
    "            t_predict_list.append(t_predict_list_)\n",
    "            t_label_list.append(t_label_list_)\n",
    "            t_accuracy_list.append(t_accuracy)\n",
    "\n",
    "            #v_loss, v_predict_list_, v_num_correct, v_label_list_= batch_loop(model, x_val_loader, epoch, loss_fn, device,col_dict,num_classes=11, train=False)\n",
    "            v_loss, v_predict_list_, v_num_correct, v_label_list_= res_loop(model, x_val, y_val, epoch, loss_fn, device,col_dict,num_classes=11, train=False)\n",
    "\n",
    "            v_accuracy= (v_num_correct / len(x_val))*100\n",
    "            v_loss_list.append(v_loss)\n",
    "            v_predict_list.append(v_predict_list_)\n",
    "            v_label_list.append(v_label_list_)\n",
    "            v_accuracy_list.append(v_accuracy)\n",
    "\n",
    "            t_avg_loss =t_loss/len(x_train)\n",
    "            v_avg_loss = v_loss /len(x_val)\n",
    "\n",
    "            e_count +=1\n",
    "            # logging\n",
    "            wandb.log({'avg_train_loss': t_avg_loss, 'epoch':epoch})\n",
    "            wandb.log({'avg_val_loss': v_avg_loss, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'train_loss': t_loss, 'epoch':epoch})\n",
    "            wandb.log({'val_loss': v_loss, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'train_correct': t_num_correct, 'epoch':epoch})\n",
    "            wandb.log({'val_correct': v_num_correct, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'train_accuracy_%': t_accuracy, 'epoch':epoch})\n",
    "            wandb.log({'val_accuracy_%': v_accuracy, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'t_labels': t_label_list, 'epoch':epoch})\n",
    "            wandb.log({'v_labels': v_label_list, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'t_predictions': t_predict_list, 'epoch':epoch})\n",
    "            wandb.log({'v_predictions': v_predict_list, 'epoch':epoch})\n",
    "\n",
    "            # add lists to save dict after all epochs run\n",
    "    save_dict['Current_Epoch'] = config['epochs']\n",
    "    save_dict['training_samples'] = len(x_train)# should this be the whole list for future graphs...?\n",
    "    save_dict['validation_samples'] = len(x_val)\n",
    "    save_dict['t_loss_list'] = t_loss_list #[c.to('cpu') for c in t_loss_list]\n",
    "    save_dict['t_predict_list'] = [[c.to('cpu') for c in k]for k in t_predict_list] #[[c.to('cpu') for c in k]for k in t_predict_list]  # [c.to('cpu') for c in t_predict_list] \n",
    "    save_dict['t_accuracy_list'] = t_accuracy_list #\n",
    "    save_dict['v_loss_list'] = v_loss_list #[c.to('cpu') for c in v_loss_list]\n",
    "    save_dict['v_predict_list'] = [[c.to('cpu') for c in k]for k in v_predict_list]#[[c.to('cpu') for c in k]for k in v_predict_list] # [c.to('cpu') for c in v_predict_list]\n",
    "    save_dict['v_accuracy_list'] = v_accuracy_list #\n",
    "    save_dict['t_labels'] = t_label_list #[[c.to('cpu') for c in k]for k in t_label_list]\n",
    "    save_dict['v_labels'] = v_label_list #[[c.to('cpu') for c in k] for k in v_label_list]\n",
    "    \n",
    "    title = save_dict['Run']\n",
    "    #test_predictions, test_y, test_accuracy = test_loop_batch(model, x_test_loader, loss_fn, device, col_dict, title, config.num_classes)\n",
    "    test_predictions, test_y, test_accuracy = test_loop_res(model, x_test, y_test, loss_fn, device, col_dict, title, num_classes=11)\n",
    "\n",
    "    save_dict['test_predictions']= [c.to('cpu') for c in test_predictions]\n",
    "    save_dict['test_labels'] = test_y\n",
    "    save_dict['test_acc'] = test_accuracy\n",
    "\n",
    "    \n",
    "    with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", 'wb+') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W= 128 #512 # input vol\\nK = 5 #1 # kernal size\\nP =0 # padding\\nS = 1# stride\\nconvOut =(W-K+(2*P))/S+1\\nprint(convOut)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "\n",
    "\"\"\"W= 128 #512 # input vol\n",
    "K = 5 #1 # kernal size\n",
    "P =0 # padding\n",
    "S = 1# stride\n",
    "convOut =(W-K+(2*P))/S+1\n",
    "print(convOut)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/its/home/nn268/antvis/antvis/optics/wandb/run-20240214_144530-vmtdfsth</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/vmtdfsth' target=\"_blank\">engaging-hug-278</a></strong> to <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics' target=\"_blank\">https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/vmtdfsth' target=\"_blank\">https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/vmtdfsth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–Š| 90/120 [1:06:31"
     ]
    }
   ],
   "source": [
    "\n",
    "#model, save_dict = pipeline(config['loss_fn'])\n",
    "model = train(device,col_dict, save_dict, config=config)\n",
    "\"\"\"def tr(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        model, save_dict = pipeline(config)\n",
    "        \n",
    "\n",
    "wandb.agent(sweep_id, tr, count=20)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5957.818181818182\n"
     ]
    }
   ],
   "source": [
    "#  (1x11648 and 46592x100)\n",
    "a = 16384*4\n",
    "print(a/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#model,save_dict = pipeline(config) #7,168\\n\\ndef tr(config=None):\\n    print(\\'1\\')\\n    with wandb.init(config=config):\\n        print(\\'2\\')\\n        config = wandb.config\\n        print(\\'3\\')\\n        model, save_dict = pipeline(config)\\n        \\nsweep_id = wandb.sweep(config, project=f\"{run_title}\"),\\n#wandb.agent(sweep_id, tr, count=20)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#model,save_dict = pipeline(config) #7,168\n",
    "\n",
    "def tr(config=None):\n",
    "    print('1')\n",
    "    with wandb.init(config=config):\n",
    "        print('2')\n",
    "        config = wandb.config\n",
    "        print('3')\n",
    "        model, save_dict = pipeline(config)\n",
    "        \n",
    "sweep_id = wandb.sweep(config, project=f\"{run_title}\"),\n",
    "#wandb.agent(sweep_id, tr, count=20)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 13 17:43:17 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:17:00.0 Off |                  Off |\n",
      "|  0%   47C    P8              27W / 450W |   5214MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:4E:00.0 Off |                  Off |\n",
      "| 30%   39C    P2              54W / 450W |   4715MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    503675      C   /usr/bin/python3                           5204MiB |\n",
      "|    1   N/A  N/A    506912      C   /usr/bin/python3                           1598MiB |\n",
      "|    1   N/A  N/A    507039      C   /usr/bin/python3                           3104MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "\n",
    "def plot_confusion(predictions:list, actual:list, title:str):\n",
    "    predict_list = [int(t.argmax()) for t in predictions]\n",
    "    actual = [int(l.argmax()) for l in actual]\n",
    "\n",
    "    actual = np.array(actual)\n",
    "    predict_list = np.array(predict_list)\n",
    "\n",
    "\n",
    "    #FixedLocator locations (3), usually from a call to set_ticks, does not match the number of labels (11).\n",
    "    print(f'\\n     {title}')\n",
    "    train_epoch_matrix = confusion_matrix(actual, predict_list, labels= [0,1,2,3,4,5,6,7,8,9,10])\n",
    "    disp= ConfusionMatrixDisplay(train_epoch_matrix, display_labels=[0,1,2,3,4,5,6,7,8,9,10])\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m v_predict \u001b[38;5;241m=\u001b[39m save_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_predict_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# WHY IS THERE NOTHING IN V OREDICT LIST!\u001b[39;00m\n\u001b[1;32m      5\u001b[0m v_labels \u001b[38;5;241m=\u001b[39m save_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mplot_confusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_predict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain Confusion Matrix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m plot_confusion(v_predict, v_labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Confusion Matrix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m, in \u001b[0;36mplot_confusion\u001b[0;34m(predictions, actual, title)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_confusion\u001b[39m(predictions:\u001b[38;5;28mlist\u001b[39m, actual:\u001b[38;5;28mlist\u001b[39m, title:\u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     predict_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(t\u001b[38;5;241m.\u001b[39margmax()) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[1;32m      6\u001b[0m     actual \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(l\u001b[38;5;241m.\u001b[39margmax()) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m actual]\n\u001b[1;32m      8\u001b[0m     actual \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(actual)\n",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_confusion\u001b[39m(predictions:\u001b[38;5;28mlist\u001b[39m, actual:\u001b[38;5;28mlist\u001b[39m, title:\u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     predict_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m()) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[1;32m      6\u001b[0m     actual \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(l\u001b[38;5;241m.\u001b[39margmax()) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m actual]\n\u001b[1;32m      8\u001b[0m     actual \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(actual)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "t_predict = save_dict['t_predict_list']\n",
    "t_labels = save_dict['t_labels']\n",
    "\n",
    "v_predict = save_dict['v_predict_list'] # WHY IS THERE NOTHING IN V OREDICT LIST!\n",
    "v_labels = save_dict['v_labels']\n",
    "\n",
    "plot_confusion(t_predict, t_labels, 'Train Confusion Matrix')\n",
    "plot_confusion(v_predict, v_labels, 'Validation Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Do your print / debug stuff here\n",
    "        print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Do your print / debug stuff here\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def pipeline(config): \n",
    "    \n",
    "    resnet = nn.Sequential(\n",
    "            newmodel,\n",
    "            Squeeze(),\n",
    "            nn.Linear(2048,11), #2048\n",
    "            nn.Softmax(dim=0),\n",
    "        )\n",
    "    \n",
    "    resnet.to(device)\n",
    "    model=resnet\n",
    "    loss_list=[]\n",
    "    model, save_dict = train_model(model,x_train, x_val, y_train, y_val, config) #train_dl, val_dl\"\"\n",
    "\n",
    "    return model, save_dict\"\"\"\n",
    "\n",
    "\"\"\"with wandb.init(project=title, config=config):\n",
    "    config = wandb.config\n",
    "    model = resnet\n",
    "\n",
    "    model, save_dict = train_model(model,x_train, x_val, y_train, y_val, config) #train_dl, val_dl\"\"\n",
    "\n",
    "return model, save_dict\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "def train_model(model,  x_train, x_val, y_train, y_val, config, best_acc=0): #train_dl, val_dl, \n",
    "    #wandb.watch(model, log='all', log_freq=10)\n",
    "    #print(config)\n",
    "    loss_fn = set_lossfn(config['loss_fn']) # ****\n",
    "    \n",
    "    lr = config['learning_rate'] #1e-5 #config.learning_rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)#build_optimizer(model, optimizer=torch.optim.Adam(model.parameters(), lr=lr))#config.optimizer, config.learning_rate, config.weight_decay)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config['scheduler'], last_epoch=-1) #gamma=config.scheduler, last_epoch=-1)\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    #losses= []\n",
    "    #predictions = []\n",
    "    t_loss_list = []\n",
    "    v_loss_list = []\n",
    "    t_predict_list = []\n",
    "    v_predict_list = []\n",
    "    t_accuracy_list = []\n",
    "    v_accuracy_list = []\n",
    "    t_label_list = []\n",
    "    v_label_list = []\n",
    "    #labels = []\n",
    "    \n",
    "    total_epochs = 0\n",
    "    for epoch in tqdm(range(config['epochs'])): #config.epochs)):\n",
    "        print('E   ', epoch)\n",
    "        t_correct = 0\n",
    "        v_correct = 0\n",
    "    \n",
    "        if epoch == 0:\n",
    "            model = model.to('cpu')\n",
    "            best_model = deepcopy(model)\n",
    "            model = model.to(device)\n",
    "            \n",
    "            \n",
    "        #train_ids = random.shuffle(train_ids)\n",
    "        #print(type(train_ids))\n",
    "        print('training...')\n",
    "        for idx, img in enumerate(x_train): \n",
    "            model.train()\n",
    "\n",
    "            x = preprocess_im(img)\n",
    "\n",
    "            train_prediction = model.forward(x)\n",
    "            \n",
    "            y = label_oh_tf(y_train[idx], device=device, num_classes=11) # use same index val to index y (labels) and turn into onehot encoded label\n",
    "\n",
    "            \"\"\"\n",
    "            #if idx % 1000 == 0:\n",
    "            #    print(idx, ' / ', len(x_train))\n",
    "            #    !nvidia-smi\n",
    "            \"\"\"\n",
    "            #print(train_prediction, train_label)\n",
    "        \n",
    "        \n",
    "            t_loss = loss_fn(train_prediction, y)\n",
    "\n",
    "            if train_prediction.argmax() == y.argmax():\n",
    "                t_correct+=1\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            t_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            t_loss_list.append(t_loss.to('cpu'))\n",
    "            t_predict_list.append(train_prediction.to('cpu'))\n",
    "            t_label_list.append(y.to('cpu'))\n",
    "            \n",
    "            train_acc = (t_correct / len(x_train))\n",
    "        t_accuracy_list.append(train_acc)\n",
    "            \n",
    "            # accuracy at every step. every epoch / by x_train\n",
    "            \n",
    "        print('validating...')\n",
    "        \n",
    "        for idx, img in enumerate(x_val):\n",
    "            model.eval()\n",
    "            x = preprocess_im(img)\n",
    "\n",
    "            val_prediction = model.forward(x)\n",
    "            \n",
    "            y = label_oh_tf(y_val[idx], device=device, num_classes=11)\n",
    "            \"\"\"\n",
    "            #if idx % 100 == 0:\n",
    "            #    print(idx, ' / ', len(x_val))\n",
    "            #    !nvidia-smi\n",
    "            \"\"\"\n",
    "\n",
    "            v_loss = loss_fn(val_prediction, y)\n",
    "            \n",
    "            if val_prediction.argmax() == y.argmax():\n",
    "                v_correct +=1\n",
    "                \n",
    "\n",
    "            v_loss = v_loss.to('cpu')\n",
    "            v_loss_list.append(v_loss.item())\n",
    "            \n",
    "            \n",
    "            val_prediction = val_prediction.to('cpu')\n",
    "            v_predict_list.append(val_prediction.detach().numpy())\n",
    "            \n",
    "            v_label_list.append(y.to('cpu').detach().numpy())\n",
    "            \n",
    "        val_acc = (v_correct / len(y_val))\n",
    "        v_accuracy_list.append(val_acc)\n",
    "            \n",
    "        total_epochs += epoch\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "    \n",
    "            #print('Start Of SAve -----------------------------')\n",
    "            #!nvidia-smi\n",
    "\n",
    "            best_acc = val_acc\n",
    "            \n",
    "            model = model.to('cpu')\n",
    "            best_model = model#deepcopy(model)\n",
    "            model = model.to(device)\n",
    "            \n",
    "\n",
    "            \n",
    "            save_dict['Current_Epoch'] += config['epochs']\n",
    "            save_dict['training_samples'] = len(x_train)\n",
    "            save_dict['validation_samples'] = len(x_val)\n",
    "            save_dict['t_loss_list'] = t_loss_list\n",
    "            save_dict['t_predict_list'] = t_predict_list  \n",
    "            save_dict['t_accuracy_list'] = t_accuracy_list  #\n",
    "            save_dict['v_loss_list'] = v_loss_list\n",
    "            save_dict['v_predict_list'] = v_predict_list  #\n",
    "            save_dict['v_accuracy_list'] = v_accuracy_list  #\n",
    "            save_dict['t_labels'] = t_label_list\n",
    "            save_dict['v_labels'] = v_label_list\n",
    "            \n",
    "            model_architecture = [nn.Sequential(\n",
    "                            model_vgg16,\n",
    "                            Squeeze(),\n",
    "                            nn.Linear(4096,11),\n",
    "                            nn.Softmax(dim=0),\n",
    "                        )]\n",
    "\n",
    "            save_dict['model.state_dict'] = model.state_dict()# .to('cpu')\n",
    "            #save_dict['model_architecture_untrained'] = model_architecture\n",
    "\n",
    "            title = save_dict['Run']\n",
    "            with open(f'{save_location}{title}.pkl', 'wb+') as f:\n",
    "                pickle.dump(save_dict, f)\n",
    "            \n",
    "            print('improvment in metrics. model saved')\n",
    "\n",
    "            #print('END Of SAve -----------------------------')\n",
    "            #!nvidia-smi\n",
    "\n",
    "        \n",
    "\n",
    "        #if (epoch+1)%2==0:\n",
    "            #train_log(t_loss, v_loss, epoch)\n",
    "            #wandb.log({'train_accuracy_%': train_acc, 'epoch':epoch})\n",
    "            #wandb.log({'val_accuracy_%': val_acc, 'epoch':epoch})\n",
    "            \n",
    "    model = best_model\n",
    "    #labels = zip(t_label_list, v_label_list)\n",
    "    #losses = zip(t_loss_list, v_loss_list)\n",
    "    #predictions = zip(t_predict_list, v_predict_list)\n",
    "    return model,save_dict\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model =nn.Sequential(\n",
    "    PrintLayer(),\n",
    "    model_vgg16,\n",
    "    PrintLayer(),\n",
    "    Squeeze(),\n",
    "    PrintLayer(),\n",
    "    nn.Linear(2048,11),\n",
    "    PrintLayer(),\n",
    "    nn.Softmax(dim=0),\n",
    "    PrintLayer()\n",
    "\n",
    ").to(device)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = preprocess_im(x_train[0])\n",
    "\n",
    "\n",
    "train_prediction = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu').state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

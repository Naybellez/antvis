{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK\n",
    "# checking classic modal on MNIST and CIFAR10\n",
    "# checking different sized (smaller) models on these datasets too\n",
    "\n",
    "# start on wrapped images\n",
    "# then again on unwrapped images\n",
    "\n",
    "# need to be sure, my model architecture still is the best\n",
    "# this will allow me to know if my low success is because og HP or the dataset being bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
      " 'parameters': {'epochs': {'value': 40},\n",
      "                'learning_rate': {'distribution': 'log_uniform_values',\n",
      "                                  'max': 0.01,\n",
      "                                  'min': 1e-05},\n",
      "                'loss_fn': {'values': ['MSE', 'CrossEntropy']},\n",
      "                'optimizer': {'values': ['adam']},\n",
      "                'scheduler': {'values': [0.2, 0.3, 0.4, 0.6]},\n",
      "                'weight_decay': {'values': [4e-05]}}}\n",
      "Using cuda:0 device\n",
      "Create sweep with ID: jeei2ym8\n",
      "Sweep URL: https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour/sweeps/jeei2ym8\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as maths\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import wandb\n",
    "import pprint\n",
    "\n",
    "from functions import import_imagedata, get_data, label_oh_tf,  Unwrap, ImageProcessor\n",
    "from architectures import vgg16net, smallnet1, smallnet2\n",
    "from loop_fns import loop, test_loop\n",
    "from fns4wandb import build_optimizer, set_optimizer, train_model, train_log, log_test_score\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "config ={\n",
    "    'method': 'random'\n",
    "}\n",
    "\n",
    "metric = {'name': 'loss',\n",
    "         'goal': 'minimize'}\n",
    "\n",
    "config['metric'] = metric\n",
    "\n",
    "param_dict ={\n",
    "    'optimizer':{\n",
    "        'values': ['adam']\n",
    "    },\n",
    "}\n",
    "\n",
    "config['parameters'] = param_dict\n",
    "\n",
    "param_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 40\n",
    "    },\n",
    "    'scheduler': {\n",
    "        'values': [0.2, 0.3, 0.4, 0.6]\n",
    "    }\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "param_dict.update({\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'log_uniform_values',\n",
    "        'min': 0.00001,\n",
    "        'max': 0.01\n",
    "      },\n",
    "    'weight_decay':{\n",
    "            'values': [4e-5] #1e-5,2e-5, 3e-5,\n",
    "      },\n",
    "    'loss_fn': {\n",
    "            'values': ['MSE', 'CrossEntropy']\n",
    "        },\n",
    "    })\n",
    "\n",
    "\n",
    "pprint.pprint(config)\n",
    "\n",
    "\n",
    "col_dict = {\n",
    "    'colour': 'colour',\n",
    "    'size': [28,28],   #36,113\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(config, project=f\"SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_{col_dict['colour']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# title, config\\ndef pipeline(hp): \\n    \\n    title = f\"{col_dict[\\'colour\\']}_Wrapped_MNIST_7c3l_3232\" #print(f\"HPS_UNwrapped_{col_dict[\\'colour\\']}\")\\n    \\n    \\n    \\n    with wandb.init(project=title, config=hp):\\n        config = wandb.config\\n        model = build_net(lin_layer_size =config.lin_layer_size,dropout =config.dropout, first_lin_lay=config.first_linear, ks= config.kernal_size,in_chan= config.first_in_channel).to(device)\\n        loss_fn = set_loss_fn(config.loss_fn)\\n        \\n        train_model(model, x_train, y_train, x_val, y_val,loss_fn, config)\\n        test_loop(model, x_test, y_test, loss_fn, device, col_dict,title)\\n        \\n    return model'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "                            # Common functions\n",
    "\n",
    "def set_lossfn(lf):\n",
    "    if lf =='MSE':\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif lf == 'CrossEntropy':\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=True)\n",
    "        x_train =[np.expand_dims(i, axis=2) for i in x_train]\n",
    "        x_val =[np.expand_dims(i, axis=2) for i in x_val]\n",
    "        x_test =[np.expand_dims(i, axis=2) for i in x_test]\n",
    "\n",
    "        model =smallnet2(in_chan=1, f_lin_lay=14400, l_lin_lay=10).to(device) \n",
    "        loss_fn = set_lossfn(config.loss_fn)\n",
    "        \n",
    "        e_count = 0\n",
    "       \n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config.scheduler, last_epoch=-1)\n",
    "\n",
    "        \n",
    "        for epoch in range(config.epochs):\n",
    "\n",
    "            t_loss, predict_list, t_num_correct, model, optimizer = loop(model, x_train, y_train, epoch, loss_fn, device, col_dict, num_classes=10,optimizer=optimizer, scheduler=scheduler)\n",
    "            t_accuracy = (t_num_correct /len(x_train))*100\n",
    "            v_loss, __, v_num_correct= loop(model, x_val, y_val, epoch, loss_fn, device,col_dict,num_classes=10, train=False) \n",
    "            v_accuracy= (v_num_correct / len(x_val))*100\n",
    "            \n",
    "            t_avg_loss =t_loss/len(x_train)\n",
    "            v_avg_loss = v_loss /len(x_val)\n",
    "            \n",
    "            e_count +=1\n",
    "            # logging\n",
    "            wandb.log({'avg_train_loss': t_avg_loss, 'epoch':epoch})\n",
    "            wandb.log({'avg_val_loss': v_avg_loss, 'epoch':epoch})\n",
    "            wandb.log({'train_loss': t_loss, 'epoch':epoch})\n",
    "            wandb.log({'val_loss': v_loss, 'epoch':epoch})\n",
    "            wandb.log({'train_accuracy_%': t_accuracy, 'epoch':epoch})\n",
    "            wandb.log({'val_accuracy_%': v_accuracy, 'epoch':epoch})\n",
    "\n",
    "    \n",
    "\"\"\"# title, config\n",
    "def pipeline(hp): \n",
    "    \n",
    "    title = f\"{col_dict['colour']}_Wrapped_MNIST_7c3l_3232\" #print(f\"HPS_UNwrapped_{col_dict['colour']}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    with wandb.init(project=title, config=hp):\n",
    "        config = wandb.config\n",
    "        model = build_net(lin_layer_size =config.lin_layer_size,dropout =config.dropout, first_lin_lay=config.first_linear, ks= config.kernal_size,in_chan= config.first_in_channel).to(device)\n",
    "        loss_fn = set_loss_fn(config.loss_fn)\n",
    "        \n",
    "        train_model(model, x_train, y_train, x_val, y_val,loss_fn, config)\n",
    "        test_loop(model, x_test, y_test, loss_fn, device, col_dict,title)\n",
    "        \n",
    "    return model\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hgqj1ye1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.6251069216994515e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_fn: CrossEntropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 4e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaughticalnonsence\u001b[0m (\u001b[33mantvis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/its/home/nn268/optics/wandb/run-20231011_163644-hgqj1ye1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour/runs/hgqj1ye1' target=\"_blank\">happy-sweep-1</a></strong> to <a href='https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour/sweeps/jeei2ym8' target=\"_blank\">https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour/sweeps/jeei2ym8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour' target=\"_blank\">https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour/sweeps/jeei2ym8' target=\"_blank\">https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour/sweeps/jeei2ym8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour/runs/hgqj1ye1' target=\"_blank\">https://wandb.ai/antvis/SanityCheck_modelSize_3conv2lin_Fasion_MNIST_lrscheduler_colour/runs/hgqj1ye1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/its/home/nn268/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

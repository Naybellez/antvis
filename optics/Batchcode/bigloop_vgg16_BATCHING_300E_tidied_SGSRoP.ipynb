{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c4fd19-5a09-4232-bdc4-42bbef0b6802",
   "metadata": {},
   "source": [
    "last updated 031224\n",
    "\n",
    "To investigate what is wrong with vgg16 learn - i.e. what i've changed.\n",
    "I am running the 4c model in this file adapted from vgg16.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6dbd95-2129-42b5-b70b-ee9f057b24c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch.Utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "import collections\n",
    "from IPython.display import clear_output\n",
    "#import time\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "sys.path.append('../.')\n",
    "from functions import ImageProcessor, IDSWDataSetLoader2, get_data\n",
    "from fns4wandb import set_lossfn\n",
    "from plotting import learning_curve, accuracy_curve, plot_confusion\n",
    "\n",
    "from modelManagment import get_lin_lay, choose_model2, choose_model1\n",
    "from loop_fns import train_val_batch, test_loop_batch\n",
    "from fileManagment import save2csv, save2json\n",
    "\n",
    "#from torchvision.models import vgg16\n",
    "\n",
    "\n",
    "#import torch.Utils.data.DataLoader as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c1e605-9a3b-48dc-a38e-8a955bd3c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "optimmy = 'SGD'\n",
    "model_type = 'vgg16'\n",
    "Var_WB_sched = \"Exp\"\n",
    "\n",
    "_save_location = f'/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/{model_type}/{optimmy}/{Var_WB_sched}/' \n",
    "checkpoint_saveloc = f\"/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/{model_type}/{optimmy}/modelCheckPoints/\"\n",
    "\n",
    "data_path = r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/'\n",
    "\n",
    "gitHASH = '682f9e66a8c166dc9c1b3de71e8e9f2c72d35ed2'\n",
    "\n",
    "\n",
    "start_epoch = 0\n",
    "epoch_val =  2# 150\n",
    "\n",
    "\n",
    "\n",
    "loadPreTrainedModel = False\n",
    "\n",
    "Pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sim_type = \"test\" #(Test or run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1481ccd-4d87-4c05-84d5-77e29ee981b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function wandb.wandb_agent.agent(sweep_id, function=None, entity=None, project=None, count=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb.login()\n",
    "#wandb login --relogin\n",
    "wandb.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ddbb49-828b-4de8-a31c-2a576cd736f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries                                                                                  * * * *   SETTINGS   * * * *\n",
    "\n",
    "date = date.today()\n",
    "#model_name = model_card['model']\n",
    "model_card_vgg = {'name': 'vgg', 'model': 'vgg16',\n",
    "                  'f_lin_lay':[200704,#200704,     #129024,#4096,  # (32x200704 and 3584x4096)\n",
    "                             200704,      #(16x64512 and 129024x4096)    (16x200704 and 64512x4096)\n",
    "                             200704,#14336#(16x200704 and 14336x4096)\n",
    "                             200704,\n",
    "                             200704, ##(32x200704 and 3584x4096)\n",
    "                             200704,\n",
    "                             200704,\n",
    "                            ],\n",
    "                 'idx': 0,\n",
    "                 'dropout':0.2}\n",
    "\n",
    "\n",
    "\n",
    "resolution_card_452144 = {'resolution':[452,144], 'padding':5, 'index':0}\n",
    "resolution_card_22672 = {'resolution':[226,72], 'padding':5, 'index':1}\n",
    "resolution_card_11336 = {'resolution':[113,36], 'padding':2, 'index':2}\n",
    "resolution_card_5715 = {'resolution':[57,18], 'padding':1, 'index':3}\n",
    "\n",
    "resolution_card_299 = {'resolution':[29,9], 'padding':0, 'index':4} # \n",
    "resolution_card_155 = {'resolution':[15,5], 'padding':0, 'index':5}\n",
    "resolution_card_83 = {'resolution':[8,3], 'padding':0, 'index':6}\n",
    "\n",
    "\n",
    "\n",
    "resolution_cards = [resolution_card_452144, resolution_card_11336, resolution_card_5715, resolution_card_155]#]#resolution_card_452144, resolution_card_22672, resolution_card_11336, \n",
    "#resolution_card_452144, resolution_card_22672, resolution_card_11336, resolution_card_5715,resolution_card_299, resolution_card_155,\n",
    "#resolution_cards = [resolution_card_11336] #resolution_card_452144, resolution_card_22672, resolution_card_11336, resolution_card_5715,resolution_card_299, resolution_card_155, resolution_card_83\n",
    "\n",
    "#learning_rate_cards = [5e-5, 6e-5, 8e-5]\n",
    "#learning_rate_cards = [8.21592E-05, 6.62E-05, 6.01E-05, 5.97E-05]\n",
    "learning_rate_cards=  [1e-3] #[0.1, 0.01, 1e-3,1e-4, 1e-5]#, 6e-5, 7e-5, 8e-5]\n",
    "#wd_cards = [4e-5, 5e-5, 3.00E-05, 2.00E-05]\n",
    "wd_cards =[0]\n",
    "#scheduler_cards = [\"RoP\"]# currently a value not a name/type. 041224 changing to string [\"RoP\", \"Exp\", \"NoSched\"]\n",
    "\n",
    "\n",
    "seeds = [8, 2,4,42]# 8\n",
    "\n",
    "#model_cards =[model_card_vgg, model_card_7c3l, model_card_4c3l, model_card_3c2l, model_card_2c2l]\n",
    "model_cards =[model_card_vgg]\n",
    "\n",
    "loss_fn_cards = ['CrossEntropy']# ,'MSE'] #\n",
    "                        \n",
    "config = dict({'parameters': 'parameters for big loop run'})\n",
    "config.update({'model_cards':model_cards})\n",
    "config.update({'resolution_cards':resolution_cards})\n",
    "config.update({'learning_rate_cards':learning_rate_cards})\n",
    "config.update({'wd_cards':wd_cards})\n",
    "config.update({'scheduler':Var_WB_sched})\n",
    "config.update({'seeds':seeds})\n",
    "config.update({'loss_fn_cards': loss_fn_cards})\n",
    "\n",
    "\n",
    "config.update({'batch_size': 32}) #64\n",
    "config.update({'epochs': epoch_val})\n",
    "config.update({'start_epoch': start_epoch})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87383b",
   "metadata": {},
   "source": [
    "# functions that are moved\n",
    "save2csv_nest_dict\n",
    "check_obj4np\n",
    "save2josn_nested_dict\n",
    "save2csv\n",
    "save2json\n",
    "read_in_json\n",
    "check_model_sizes_bits\n",
    "ptrblk_fin_mod_size\n",
    "train_val_batch\n",
    "test_loop_batch\n",
    "get_data\n",
    "get_lin_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77090bf1-52a9-41f6-9f69-5a72fe4286a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def _go(config=None):\n",
    "\n",
    "    if len(gitHASH) <1:\n",
    "        print(\"YOU FORGET THE GIT HASH\")\n",
    "        return\n",
    "    else:\n",
    "        #print('Git Hash registered')\n",
    "        pass\n",
    "        \n",
    "    with wandb.init(config=config, project=f\"{model_type} {epoch_val}E. {Var_WB_sched}. {optimmy}_{sim_type}\", notes=f\"{model_type} {epoch_val}E {Var_WB_sched} {optimmy}_{sim_type}\",):\n",
    "        config = wandb.config\n",
    "        #start = time.process_time()\n",
    "        model_card = model_card_vgg\n",
    "        #print(\"Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "                \n",
    "        model_name = model_card['model']\n",
    "        model_index = model_card['idx']\n",
    "        dropout = model_card['dropout'] \n",
    "        for res_idx, resolution_card in enumerate(config['resolution_cards']):\n",
    "            #print(\"Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "            resolution = resolution_card['resolution']\n",
    "            pad = resolution_card['padding']\n",
    "            lin_lay = get_lin_lay(model_card, resolution)\n",
    "            print('lin lay', lin_lay)\n",
    "            scheduler_value = 0\n",
    "        \n",
    "            lr = 1e-3#for lr_idx, lr in enumerate(config['learning_rate_cards']):\n",
    "                \n",
    "                   \n",
    "            for seed_idx, seed in enumerate(config['seeds']):\n",
    "                seed = seed\n",
    "                loss = 'CrossEntropy'\n",
    "                config['batch_size']\n",
    "\n",
    "                print('Model: ', str(model_name), f\" idx: {0} / {len(config.model_cards)}\")\n",
    "                print('resolution: ', str(resolution), f\" idx: {res_idx} / {len(config['resolution_cards'])}\")\n",
    "                print('learning rate: ', str(lr), f\" idx: {0} / {len(config['learning_rate_cards'])}\")\n",
    "                #print('weight decay: ', str(wd_card), f\" idx: {wd_idx} / {len(config['wd_cards'])}\")\n",
    "                print('scheduler: ', str(Var_WB_sched))#, f\" idx: {sched_idx} / {len(config['scheduler_cards'])}\")\n",
    "                print('seed: ', str(seed), f\" idx: {seed_idx} / {len(config['seeds'])}\")\n",
    "                print('loss function: ', str(loss))#, f\" idx: {0} / {len(config['loss_fn_cards'])}\")\n",
    "                print('Batch size: ', config['batch_size'])\n",
    "                print('Training epochs: ', config['epochs'])\n",
    "\n",
    "                epochs = config['epochs'] #40\n",
    "\n",
    "                IP = ImageProcessor(device)\n",
    "\n",
    "                wandb.log({'gitHash':gitHASH})\n",
    "                wandb.log({'Epochs': epochs})\n",
    "                \n",
    "                #print('3')\n",
    "                #!nvidia-smi\n",
    "                \n",
    "                # set save dictionary\n",
    "                save_dict = {'Run' : f\"{model_name}_{resolution}_{date}_{optimmy}_{config['epochs']}E_{Var_WB_sched}_{sim_type}\",\n",
    "                             'Current_Epoch': config['start_epoch'], # this is where i add the start epoch\n",
    "                             'start_epoch':config['start_epoch'],\n",
    "                             'save_location' : _save_location,\n",
    "                             'checkpoint_save_loc': checkpoint_saveloc,\n",
    "                             'res': resolution,\n",
    "                             'sched': Var_WB_sched,\n",
    "                             'model': model_name,\n",
    "                             'optimiser':optimmy,\n",
    "                             \"scheduler\": Var_WB_sched,\n",
    "                             'seed':seed}\n",
    "                \n",
    "                print(\"model name\", model_name, \" flinlay: \", lin_lay, \" dropout:\", dropout)\n",
    "                model = choose_model1(model_name, lin_lay, dropout).to(device)\n",
    "                \n",
    "                if loadPreTrainedModel:\n",
    "                    dir_pkl = f\"/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/{model_type}/{optimmy}/NoSched/\"\n",
    "                    pkl_name = f\"{model_type}_{resolution}_2024-11-26_{optimmy}_150E_{Var_WB_sched}_{resolution}_0.001_{seed}_CrossEntropy_{optimmy}.pkl\"#f\"{model_type}_{optimmy}_{Var_WB_sched}_150E_{resolution}_seed{seed}\"#_{resolution}_0.001_0_{seed}_CrossEntropy\n",
    "                    with open(dir_pkl+pkl_name, 'rb') as f:\n",
    "                        model_pkl = torch.load(f)\n",
    "                    model.load_state_dict(model_pkl['model.state_dict'])\n",
    "\n",
    "                print(\"After model init, Before data loading - Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "\n",
    "                x_train, y_train, x_val, y_val, x_test, y_test = get_data(random_seed=seed, file_path=data_path)\n",
    "                av_lum = IP.new_luminance(x_train)\n",
    "\n",
    "                train_ds = IDSWDataSetLoader2(x_train, y_train, resolution,pad,av_lum,model_name, device)# av_lum, res,pad,\n",
    "                train = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "                \n",
    "                test_ds = IDSWDataSetLoader2(x_test, y_test, resolution,pad,av_lum,model_name, device)\n",
    "                test = DataLoader(test_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "                val_ds = IDSWDataSetLoader2(x_val, y_val, resolution,pad,av_lum,model_name, device)\n",
    "                val = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "                \n",
    "                print(\"After data loading - Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "                loss_fn = set_lossfn(loss)\n",
    "                \n",
    "                # set optimizer\n",
    "                optimizer =torch.optim.SGD(model.parameters(), lr=lr)#torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "                wandb.watch(model, loss_fn, log='all', log_freq=2, idx = model_index)\n",
    "\n",
    "                loop_run_name = f\"{save_dict['Run']}_{resolution}_{lr}_{seed}_{loss}_{optimmy}\"\n",
    "\n",
    "                model, save_dict=  train_val_batch(model, train,val, loop_run_name,save_dict, lr, loss_fn,epochs, config['batch_size'], optimizer, scheduler_value, device)\n",
    "\n",
    "                test_acc,test_predict_list, y_test = test_loop_batch(model,test, loss_fn, config['batch_size'], device) #model, model_name, X, Y, res, pad, loss_fn, device, num_classes=11\n",
    "                \n",
    "                #print(test_predict_list)\n",
    "                print(' \\n train Acc: ', save_dict['t_accuracy_list'][-1])\n",
    "                print(' \\n val Acc: ', save_dict['v_accuracy_list'][-1])\n",
    "                print(' \\n test Acc: ', test_acc)\n",
    "                \n",
    "                save_dict.update({'test_acc': test_acc})\n",
    "                save_dict.update({'test_predict': test_predict_list})\n",
    "                save_dict.update({'test_labels': list(y_test)})\n",
    "\n",
    "                learning_curve(save_dict['t_loss_list'], save_dict['v_loss_list'], save_location=save_dict['save_location'],run_name=loop_run_name)\n",
    "                accuracy_curve(save_dict['t_accuracy_list'], save_dict['v_accuracy_list'],save_location=save_dict['save_location'],run_name=loop_run_name)\n",
    "                test_predict_list=[pred for pred in test_predict_list]\n",
    "                plot_confusion(predictions= test_predict_list, actual= y_test, title = \"Test Confusion matrix\", run_name = loop_run_name,save_location =save_dict['save_location'])\n",
    "                \n",
    "                wandb.log({'test_acc': test_acc})\n",
    "                wandb.log({'test_predict': test_predict_list})\n",
    "                wandb.log({'test_labels': list(y_test)})\n",
    "                #saving\n",
    "                diction = {}\n",
    "                d = date.today()\n",
    "                d=str(d)\n",
    "                diction.update({'Date':d})\n",
    "                diction.update({'gitHASH':str(gitHASH)})\n",
    "                diction.update({'model_name': str(model_name)})\n",
    "                diction.update({'loss_fn': str(loss)})\n",
    "                diction.update({'lr': str(lr)})\n",
    "                #diction.update({'wd': str(wd_card)})\n",
    "                #diction.update({'scheduler value': str(scheduler_value)})\n",
    "                diction.update({'seed': str(seed)})\n",
    "                diction.update({'resolution': str(resolution)})\n",
    "                diction.update({'pad': int(pad)})\n",
    "                diction.update({'lin_lay': int(lin_lay)})\n",
    "                #diction.update({'run time': (time.process_time() - run_start_time)})\n",
    "                diction.update(save_dict)\n",
    "                \n",
    "                save_location = save_dict['save_location']\n",
    "                title = save_dict['Run']\n",
    "                save2json(diction, loop_run_name, save_location)\n",
    "                save2csv(diction, title, save_location)\n",
    "\n",
    "                diction['model.state_dict'] = model.state_dict() #to('cpu').\n",
    "\n",
    "                with open(f\"{save_location}{loop_run_name}.pkl\", 'wb+') as f:\n",
    "                    #p\n",
    "                    torch.save(diction, f)\n",
    "                \n",
    "                clear_output()\n",
    "\n",
    "                \n",
    "                #print(f' \\n END {model_name} {resolution} Run Time: ',time.process_time() - run_start_time)\n",
    "                #!nvidia-smi\n",
    "                torch.cuda.empty_cache()\n",
    "            #print('Final Run time: ',time.process_time() - start)\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58d7d27-ef9d-43d9-98a8-1f11ff50090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaughticalnonsence\u001b[0m (\u001b[33mantvis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/its/home/nn268/antvis/antvis/optics/Batchcode/wandb/run-20241204_134524-tfc18u8x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antvis/vgg16%202E.%20Exp.%20SGD_Test/runs/tfc18u8x' target=\"_blank\">prime-pond-4</a></strong> to <a href='https://wandb.ai/antvis/vgg16%202E.%20Exp.%20SGD_Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antvis/vgg16%202E.%20Exp.%20SGD_Test' target=\"_blank\">https://wandb.ai/antvis/vgg16%202E.%20Exp.%20SGD_Test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antvis/vgg16%202E.%20Exp.%20SGD_Test/runs/tfc18u8x' target=\"_blank\">https://wandb.ai/antvis/vgg16%202E.%20Exp.%20SGD_Test/runs/tfc18u8x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin lay 200704\n",
      "Model:  vgg16  idx: 0 / 1\n",
      "resolution:  [452, 144]  idx: 0 / 4\n",
      "learning rate:  0.001  idx: 0 / 1\n",
      "scheduler:  Exp\n",
      "seed:  8  idx: 0 / 4\n",
      "loss function:  CrossEntropy\n",
      "Batch size:  32\n",
      "Training epochs:  2\n",
      "model name vgg16  flinlay:  200704  dropout: 0.2\n",
      "After model init, Before data loading - Current allocated memory (GB): 3.1316871643066406\n",
      "/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/\n",
      "After data loading - Current allocated memory (GB): 3.1316871643066406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Optimizer present:  SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_41299/4225299840.py\", line 101, in _go\n",
      "    model, save_dict=  train_val_batch(model, train,val, loop_run_name,save_dict, lr, loss_fn,epochs, config['batch_size'], optimizer, scheduler_value, device)\n",
      "  File \"/its/home/nn268/antvis/antvis/optics/Batchcode/.././loop_fns.py\", line 307, in train_val_batch\n",
      "    t_loss, train_prediction, t_label_list, t_correct, model, optimizer = loop_batch(model, train, loss_fn, batch_size,sample,random_value,epoch,loop_run_name, save_dict, device, optimizer =optimizer, scheduler= scheduler_value, train =True) #, scheduler =scheduler\n",
      "  File \"/its/home/nn268/antvis/antvis/optics/Batchcode/.././loop_fns.py\", line 222, in loop_batch\n",
      "    prediction = model.forward(x_batch)\n",
      "  File \"/its/home/nn268/antvis/antvis/optics/Batchcode/.././modelManagment.py\", line 122, in forward\n",
      "    out = self.layer1(x)\n",
      "  File \"/its/home/nn268/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/its/home/nn268/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/its/home/nn268/.local/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/its/home/nn268/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/its/home/nn268/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/its/home/nn268/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/its/home/nn268/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.55 GiB of which 260.50 MiB is free. Process 38962 has 19.74 GiB memory in use. Including non-PyTorch memory, this process has 3.54 GiB memory in use. Of the allocated memory 3.15 GiB is allocated by PyTorch, and 16.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41aed02ffe9438ca2985549bad44b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epochs</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epochs</td><td>2</td></tr><tr><td>gitHash</td><td>682f9e66a8c166dc9c1b...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-pond-4</strong> at: <a href='https://wandb.ai/antvis/vgg16%202E.%20Exp.%20SGD_Test/runs/tfc18u8x' target=\"_blank\">https://wandb.ai/antvis/vgg16%202E.%20Exp.%20SGD_Test/runs/tfc18u8x</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241204_134524-tfc18u8x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.55 GiB of which 260.50 MiB is free. Process 38962 has 19.74 GiB memory in use. Including non-PyTorch memory, this process has 3.54 GiB memory in use. Of the allocated memory 3.15 GiB is allocated by PyTorch, and 16.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m_go\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 101\u001b[0m, in \u001b[0;36m_go\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     97\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwatch(model, loss_fn, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, log_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, idx \u001b[38;5;241m=\u001b[39m model_index)\n\u001b[1;32m     99\u001b[0m loop_run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRun\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimmy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 101\u001b[0m model, save_dict\u001b[38;5;241m=\u001b[39m  \u001b[43mtrain_val_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_run_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m test_acc,test_predict_list, y_test \u001b[38;5;241m=\u001b[39m test_loop_batch(model,test, loss_fn, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], device) \u001b[38;5;66;03m#model, model_name, X, Y, res, pad, loss_fn, device, num_classes=11\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#print(test_predict_list)\u001b[39;00m\n",
      "File \u001b[0;32m~/antvis/antvis/optics/Batchcode/.././loop_fns.py:307\u001b[0m, in \u001b[0;36mtrain_val_batch\u001b[0;34m(model, train, val, loop_run_name, save_dict, lr, loss_fn, epochs, batch_size, optimizer, scheduler_value, device)\u001b[0m\n\u001b[1;32m    304\u001b[0m random_value \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandrange(\u001b[38;5;241m0\u001b[39m,batch_size)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 307\u001b[0m t_loss, train_prediction, t_label_list, t_correct, model, optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mloop_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrandom_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloop_run_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, scheduler =scheduler\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining..  2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m#!nvidia-smi\u001b[39;00m\n",
      "File \u001b[0;32m~/antvis/antvis/optics/Batchcode/.././loop_fns.py:222\u001b[0m, in \u001b[0;36mloop_batch\u001b[0;34m(model, data, loss_fn, batch_size, sample, random_value, epoch, loop_run_name, save_dict, device, optimizer, scheduler, train)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data,\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    221\u001b[0m     x_batch, y_batch \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 222\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(prediction, y_batch)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "File \u001b[0;32m~/antvis/antvis/optics/Batchcode/.././modelManagment.py:122\u001b[0m, in \u001b[0;36mchoose_model1.<locals>.VGG16Smaller.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 122\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out)\n\u001b[1;32m    124\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 23.55 GiB of which 260.50 MiB is free. Process 38962 has 19.74 GiB memory in use. Including non-PyTorch memory, this process has 3.54 GiB memory in use. Of the allocated memory 3.15 GiB is allocated by PyTorch, and 16.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "_go(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964b31e-4bae-4f65-bc46-ee1334a8f239",
   "metadata": {},
   "source": [
    "# 12.58 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.04 GiB is free\n",
    "23.65-8.04\n",
    "/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/4c3l/SGD/modelCheckPoints\n",
    "/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/4c/SGD/modelCheckPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59daceaa-fa47-4c76-90d6-f23550f85694",
   "metadata": {},
   "source": [
    "\n",
    "pred torch.Size([11])\n",
    "\n",
    "lab  torch.Size([5, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0ade6-cd6d-46db-ac95-6b368cbea443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only things that have been added are more dictionary items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6c0bb-8e01-4c7a-ac94-0f899faaff7a",
   "metadata": {},
   "source": [
    "cuda memory error\n",
    "\n",
    "currently trying to reduce number of vars and delete any large ones after use (del loss after loss has been added to current loss or loss list)\n",
    "\n",
    "14.39  261124 - reducing batch size to 32\n",
    "\n",
    "worked up to 160E.\n",
    "will train to 150, then when all done, read in the model files and continue to 300.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154693be-5f86-4945-95b3-dde95a4bc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=device, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023a4c1-bd26-406b-b140-5e9a4dd0add0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

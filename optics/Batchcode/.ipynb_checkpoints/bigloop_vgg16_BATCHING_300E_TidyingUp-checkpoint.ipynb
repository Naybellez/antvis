{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c4fd19-5a09-4232-bdc4-42bbef0b6802",
   "metadata": {},
   "source": [
    "last updated 13/12/24 - Trying Adam, vgg16, CE on Cifar-10 database\n",
    "\n",
    "- output layer of network needs to output to 10 channels  not the current 11.\n",
    "- download and seporate cifar 10 into train, val batch\n",
    "- wont need dataloader - too specific to idsw dataset\n",
    "\n",
    "To investigate what is wrong with vgg16 learn - i.e. what i've changed.\n",
    "I am running the 4c model in this file adapted from vgg16.\n",
    "\n",
    "\n",
    "_save_location, loop_run_name, wandb.init config & notes > for cifar10 sims, included var ds_kw in, or replacing Var_WB_sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6dbd95-2129-42b5-b70b-ee9f057b24c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 10:21:20.187932: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-19 10:21:20.355809: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-19 10:21:20.355854: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-19 10:21:20.382971: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-19 10:21:20.442271: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-19 10:21:21.028409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch.Utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "import collections\n",
    "from IPython.display import clear_output\n",
    "#import time\n",
    "import random\n",
    "import cv2\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#import wandb\n",
    "\n",
    "import sys\n",
    "sys.path.append('../.')\n",
    "\n",
    "# data\n",
    "from functions import ImageProcessor, label_oh_tf#, IDSWDataSetLoader2, get_data\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "#\n",
    "from fns4wandb import set_lossfn\n",
    "from plotting import learning_curve, accuracy_curve, plot_confusion\n",
    "\n",
    "from modelManagment import get_lin_lay, choose_model_out10 #, choose_model2, choose_model1\n",
    "from loop_fns import train_val_batch, test_loop_batch\n",
    "from fileManagment import save2csv, save2json\n",
    "\n",
    "from debuggingFuncs import check_model_sizes_bits\n",
    "#from torchvision.models import vgg16\n",
    "\n",
    "\n",
    "#import torch.Utils.data.DataLoader as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c1e605-9a3b-48dc-a38e-8a955bd3c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "optimmy = 'SGD' #'SGD'\n",
    "# note. optimizer is a single line in go- changed manually\n",
    "model_type = 'vgg16'\n",
    "Var_WB_sched = \"NoSched\"\n",
    "ds_kw =\"cifar10\"\n",
    "loss_type = 'MSE'\n",
    "\n",
    "#_save_location = f'/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/{model_type}/{optimmy}/{ds_kw}/' \n",
    "#checkpoint_saveloc = f\"/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/{model_type}/{optimmy}/modelCheckPoints/\"\n",
    "_save_location = f'/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/{model_type}/cifar10/'\n",
    "checkpoint_saveloc = _save_location\n",
    "\n",
    "#data_path = r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/'\n",
    "\n",
    "gitHASH = ' bdb92a70a6d28a07271550d4b602aaa284111019'\n",
    "\n",
    "\n",
    "start_epoch = 0\n",
    "epoch_val =  100 #300 #150 #300# 150\n",
    "\n",
    "\n",
    "\n",
    "loadPreTrainedModel = False\n",
    "\n",
    "Pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sim_type = \"run\" #(Test or run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1481ccd-4d87-4c05-84d5-77e29ee981b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.login()\n",
    "#wandb login --relogin\n",
    "#wandb.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3dc2e8b-52df-4a26-9ea9-f703f85f42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory._record_memory_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ddbb49-828b-4de8-a31c-2a576cd736f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries                                                                                  * * * *   SETTINGS   * * * *\n",
    "\n",
    "date = date.today()\n",
    "#model_name = model_card['model']\n",
    "model_card_vgg = {'name': 'vgg', 'model': 'vgg16',\n",
    "                  'f_lin_lay':[200704,#200704,     #129024,#4096,  # (32x200704 and 3584x4096)\n",
    "                             200704,      #(16x64512 and 129024x4096)    (16x200704 and 64512x4096)\n",
    "                             200704,#14336#(16x200704 and 14336x4096)\n",
    "                             200704,\n",
    "                             200704, ##(32x200704 and 3584x4096)\n",
    "                             200704,\n",
    "                             200704,\n",
    "                            ],\n",
    "                 'idx': 0,\n",
    "                 'dropout':0.2}\n",
    "\n",
    "\n",
    "\n",
    "resolution_card_452144 = {'resolution':[452,144], 'padding':5, 'index':0}\n",
    "resolution_card_22672 = {'resolution':[226,72], 'padding':5, 'index':1}\n",
    "resolution_card_11336 = {'resolution':[113,36], 'padding':2, 'index':2}\n",
    "resolution_card_5715 = {'resolution':[57,18], 'padding':1, 'index':3}\n",
    "\n",
    "resolution_card_299 = {'resolution':[29,9], 'padding':0, 'index':4} # \n",
    "resolution_card_155 = {'resolution':[15,5], 'padding':0, 'index':5}\n",
    "resolution_card_83 = {'resolution':[8,3], 'padding':0, 'index':6}\n",
    "\n",
    "\n",
    "\n",
    "resolution_cards = [resolution_card_452144, resolution_card_11336, resolution_card_5715, resolution_card_155]#]#resolution_card_452144, resolution_card_22672, resolution_card_11336, \n",
    "#resolution_card_452144, resolution_card_22672, resolution_card_11336, resolution_card_5715,resolution_card_299, resolution_card_155,\n",
    "#resolution_cards = [resolution_card_11336] #resolution_card_452144, resolution_card_22672, resolution_card_11336, resolution_card_5715,resolution_card_299, resolution_card_155, resolution_card_83\n",
    "\n",
    "#learning_rate_cards = [5e-5, 6e-5, 8e-5]\n",
    "#learning_rate_cards = [8.21592E-05, 6.62E-05, 6.01E-05, 5.97E-05]\n",
    "learning_rate_cards=  [1e-3] #[0.1, 0.01, 1e-3,1e-4, 1e-5]#, 6e-5, 7e-5, 8e-5]\n",
    "#wd_cards = [4e-5, 5e-5, 3.00E-05, 2.00E-05]\n",
    "wd_cards =[0]\n",
    "#scheduler_cards = [0]#, 0.1, 0.2]\n",
    "\n",
    "seeds = [8,2,4,42]# 8\n",
    "\n",
    "#model_cards =[model_card_vgg, model_card_7c3l, model_card_4c3l, model_card_3c2l, model_card_2c2l]\n",
    "model_cards =[model_card_vgg]\n",
    "\n",
    "loss_fn_cards = loss_type #['MSE']# ,''] #CrossEntropy\n",
    "                        \n",
    "config = dict({'parameters': 'parameters for big loop run'})\n",
    "config.update({'model_cards':model_cards})\n",
    "config.update({'resolution_cards':resolution_cards})\n",
    "config.update({'learning_rate_cards':learning_rate_cards})\n",
    "config.update({'wd_cards':wd_cards})\n",
    "config.update({'scheduler':Var_WB_sched})\n",
    "config.update({'seeds':seeds})\n",
    "config.update({'loss_fn_cards': loss_fn_cards})\n",
    "\n",
    "\n",
    "config.update({'batch_size': 32}) #64\n",
    "config.update({'epochs': epoch_val})\n",
    "config.update({'start_epoch': start_epoch})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87383b",
   "metadata": {},
   "source": [
    "# functions that are moved\n",
    "save2csv_nest_dict\n",
    "check_obj4np\n",
    "save2josn_nested_dict\n",
    "save2csv\n",
    "save2json\n",
    "read_in_json\n",
    "check_model_sizes_bits\n",
    "ptrblk_fin_mod_size\n",
    "train_val_batch\n",
    "test_loop_batch\n",
    "get_data\n",
    "get_lin_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261f7df-ccb3-48fa-b5de-313a96686550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77090bf1-52a9-41f6-9f69-5a72fe4286a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _go(config=None):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if len(gitHASH) <1:\n",
    "        print(\"YOU FORGET THE GIT HASH\")\n",
    "        return\n",
    "    else:\n",
    "        #print('Git Hash registered')\n",
    "        pass\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    assert x_train.shape == (50000, 32, 32, 3)\n",
    "    assert x_test.shape == (10000, 32, 32, 3)\n",
    "    assert y_train.shape == (50000, 1)\n",
    "    assert y_test.shape == (10000, 1)#\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3, shuffle=True)\n",
    "    #print(\"x train shape  \",x_train.shape)\n",
    "    #print(\"y train shape  \",y_train.shape)\n",
    "    #print(\"x test shape  \",x_test.shape)\n",
    "    #print(\"y test shape  \",y_test.shape)\n",
    "    #print(\"x val shape  \",x_val.shape)\n",
    "    #print(\"y val shape  \",y_val.shape)\n",
    "    \n",
    "    #print(x_train[0].shape)\n",
    "    #plt.imshow(x_train[4])\n",
    "    #plt.show()\n",
    "        \n",
    "    #with wandb.init(config=config, project=f\"{model_type} {epoch_val}E. {ds_kw}. {optimmy}_{sim_type}\", notes=f\"{model_type} {epoch_val}E {ds_kw} {optimmy}_{sim_type}\",):\n",
    "    #    config = wandb.config\n",
    "        #start = time.process_time()\n",
    "    model_card = model_card_vgg\n",
    "    #print(\"Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "            \n",
    "    model_name = model_card['model']\n",
    "    model_index = model_card['idx']\n",
    "    dropout = model_card['dropout'] \n",
    "    for res_idx, resolution_card in enumerate(config['resolution_cards']):\n",
    "        #print(\"Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "        resolution = resolution_card['resolution']\n",
    "        pad = resolution_card['padding']\n",
    "        lin_lay = get_lin_lay(model_card, resolution)\n",
    "        print('lin lay', lin_lay)\n",
    "        scheduler_value = 0\n",
    "    \n",
    "        lr = 1e-3#for lr_idx, lr in enumerate(config['learning_rate_cards']):\n",
    "            \n",
    "               \n",
    "        for seed_idx, seed in enumerate(config['seeds']):\n",
    "            seed = seed\n",
    "            loss = loss_type#'MSE'\n",
    "            config['batch_size']\n",
    "\n",
    "            print('Model: ', str(model_name), f\" idx: {0} / {len(config['model_cards'])}\")\n",
    "            print('resolution: ', str(resolution), f\" idx: {res_idx} / {len(config['resolution_cards'])}\")\n",
    "            print('learning rate: ', str(lr), f\" idx: {0} / {len(config['learning_rate_cards'])}\")\n",
    "            #print('weight decay: ', str(wd_card), f\" idx: {wd_idx} / {len(config['wd_cards'])}\")\n",
    "            print('scheduler: ', str(Var_WB_sched))# f\" idx: {sched_idx} / {len(config['scheduler_cards'])}\")\n",
    "            print('seed: ', str(seed), f\" idx: {seed_idx} / {len(config['seeds'])}\")\n",
    "            print('loss function: ', str(loss))#, f\" idx: {0} / {len(config['loss_fn_cards'])}\")\n",
    "            print('Batch size: ', config['batch_size'])\n",
    "            print('Training epochs: ', config['epochs'])\n",
    "\n",
    "            epochs = config['epochs'] #40\n",
    "\n",
    "            IP = ImageProcessor(device)\n",
    "\n",
    "            #wandb.log({'gitHash':gitHASH})\n",
    "            #wandb.log({'Epochs': epochs})\n",
    "            \n",
    "            #print('3')\n",
    "            #!nvidia-smi\n",
    "            \n",
    "            # set save dictionary\n",
    "            save_dict = {'Run' : f\"{model_name}_{resolution}_{date}_{optimmy}_{config['epochs']}E_{Var_WB_sched}_{sim_type}\",\n",
    "                         'Current_Epoch': config['start_epoch'], # this is where i add the start epoch\n",
    "                         'start_epoch':config['start_epoch'],\n",
    "                         'save_location' : _save_location,\n",
    "                         'checkpoint_save_loc': checkpoint_saveloc,\n",
    "                         'res': resolution,\n",
    "                         'sched': Var_WB_sched,\n",
    "                         'model': model_name,\n",
    "                         'optimiser':optimmy,\n",
    "                         'scheduler': Var_WB_sched,\n",
    "                         'seed':seed}\n",
    "            \n",
    "            print(\"model name\", model_name, \" flinlay: \", lin_lay, \" dropout:\", dropout)\n",
    "            model = choose_model_out10(model_name, lin_lay, dropout).to(device) #choose_model1(model_name, lin_lay, dropout).to(device)#.to(device)\n",
    "            #check_model_sizes_bits(model)#\n",
    "            \n",
    "            if loadPreTrainedModel:\n",
    "                dir_pkl = f\"/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/{model_type}/{optimmy}/NoSched/\"\n",
    "                pkl_name = f\"{model_type}_{resolution}_2024-11-26_{optimmy}_150E_{Var_WB_sched}_{resolution}_0.001_{seed}_CrossEntropy_{optimmy}.pkl\"#f\"{model_type}_{optimmy}_{Var_WB_sched}_150E_{resolution}_seed{seed}\"#_{resolution}_0.001_0_{seed}_CrossEntropy\n",
    "                with open(dir_pkl+pkl_name, 'rb') as f:\n",
    "                    model_pkl = torch.load(f)\n",
    "                model.load_state_dict(model_pkl['model.state_dict'])\n",
    "\n",
    "            #print(\"After model init, Before data loading - Current allocated memory (GB):\", torch.cuda.memory_allocated(device=device) / 1024 ** 3)\n",
    "            \n",
    "            # DATASET\n",
    "            #x_train, y_train, x_val, y_val, x_test, y_test = get_data(random_seed=seed, file_path=data_path)\n",
    "            #av_lum = IP.new_luminance(x_train)\n",
    "            av_lum = 0\n",
    "            #train_ds = IDSWDataSetLoader2(x_train, y_train, resolution,pad,av_lum,model_name, device)# av_lum, res,pad,\n",
    "            #train = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "\n",
    "            IP = ImageProcessor(device)\n",
    "            x_traint= [torch.tensor((cv2.resize(im, (224, 224))/255), dtype=torch.float32).permute(2, 0, 1) for im in x_train]#[IP.colour_size_tense(x, 'colour', (224, 224), av_lum,0, vg=True) for x in x_train]\n",
    "            x_valt= [torch.tensor((cv2.resize(im, (224, 224))/255), dtype=torch.float32).permute(2, 0, 1) for im in x_val]\n",
    "            x_testt= [torch.tensor((cv2.resize(im, (224, 224))/255), dtype=torch.float32).permute(2, 0, 1) for im in x_test]\n",
    "            \n",
    "            #x_valt= [IP.colour_size_tense(x, 'colour', (224, 224), av_lum,0, vg=True) for x in x_val]\n",
    "            #x_testt= [IP.colour_size_tense(x, 'colour', (224, 224), av_lum,0, vg=True) for x in x_test]\n",
    "            y_traint = [label_oh_tf(lab, 10) for lab in y_train]\n",
    "            y_valt = [label_oh_tf(lab, 10) for lab in y_val]\n",
    "            y_testt = [label_oh_tf(lab, 10) for lab in y_test]\n",
    "            #del x_train\n",
    "            #del y_train\n",
    "            #del x_val\n",
    "            #del y_val\n",
    "            #del x_test\n",
    "            #del y_test\n",
    "\n",
    "\n",
    "            #print(\"y test post processing \",type(y_testt[0]), len(y_testt), y_testt[0].shape)\n",
    "            #print(\"x test post processing\", type(x_testt[0]), len(x_testt), x_testt[0].shape)\n",
    "        \n",
    "            train = DataLoader(list(zip(x_traint, y_traint)), batch_size=config['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "            #test_ds = IDSWDataSetLoader2(x_test, y_test, resolution,pad,av_lum,model_name, device)\n",
    "            #test = DataLoader(test_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "            test = DataLoader(list(zip(x_testt, y_testt)), batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "\n",
    "            #val_ds = IDSWDataSetLoader2(x_val, y_val, resolution,pad,av_lum,model_name, device)\n",
    "            #val = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "            val = DataLoader(list(zip(x_valt, y_valt)), batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "            #print(\"train shape\", len(train))\n",
    "\n",
    "            \n",
    "            #print(\"After data loading - Current allocated memory (GB):\", torch.cuda.memory_allocated(device=device) / 1024 ** 3)\n",
    "            loss_fn = set_lossfn(loss)\n",
    "            \n",
    "            # set optimizer\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr)#torch.optim.Adam(model.parameters(),lr=lr)## #torch.optim.SGD(model.parameters(), lr=lr) # \n",
    "\n",
    "            #print(\"MODEL PARAMS : \",model.parameters())\n",
    "            #for p in model.parameters():\n",
    "            #    print(p)\n",
    "            \n",
    "            #print(\"After loading Optimizer - Current allocated memory (GB):\", torch.cuda.memory_allocated(device=device) / 1024 ** 3)\n",
    "            #wandb.watch(model, loss_fn, log='all', log_freq=10, idx = model_index)\n",
    "            #print(\"Pre Training - Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "\n",
    "            loop_run_name = f\"{save_dict['Run']}_{resolution}_{lr}_{seed}_{loss}_{optimmy}_{ds_kw}\"\n",
    "\n",
    "            model, save_dict=  train_val_batch(model, train,val, loop_run_name,save_dict, lr, loss_fn,epochs, config['batch_size'], optimizer, scheduler_value, device)\n",
    "            #model, save_dict=  train_val_batch(model, (x_train,y_train),(x_val, y_val), loop_run_name,save_dict, lr, loss_fn,epochs, config['batch_size'], optimizer, scheduler_value, device)\n",
    "\n",
    "            #print(\"Post Training - Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "            \n",
    "            test_acc,test_predict_list, y_test = test_loop_batch(model,test, loss_fn, config['batch_size'], device) #model, model_name, X, Y, res, pad, loss_fn, device, num_classes=11\n",
    "            \n",
    "            #print(test_predict_list)\n",
    "            print(' \\n train Acc: ', save_dict['t_accuracy_list'][-1])\n",
    "            print(' \\n val Acc: ', save_dict['v_accuracy_list'][-1])\n",
    "            print(' \\n test Acc: ', test_acc)\n",
    "            \n",
    "            save_dict.update({'test_acc': test_acc})\n",
    "            save_dict.update({'test_predict': test_predict_list})\n",
    "            save_dict.update({'test_labels': list(y_test)})\n",
    "\n",
    "            learning_curve(save_dict['t_loss_list'], save_dict['v_loss_list'], save_location=save_dict['save_location'],run_name=loop_run_name)\n",
    "            accuracy_curve(save_dict['t_accuracy_list'], save_dict['v_accuracy_list'],save_location=save_dict['save_location'],run_name=loop_run_name)\n",
    "            test_predict_list=[pred for pred in test_predict_list]\n",
    "            plot_confusion(predictions= test_predict_list, actual= y_test, title = \"Test Confusion matrix\", run_name = loop_run_name,save_location =save_dict['save_location'])\n",
    "            \n",
    "            #wandb.log({'test_acc': test_acc})\n",
    "            #wandb.log({'test_predict': test_predict_list})\n",
    "            #wandb.log({'test_labels': list(y_test)})\n",
    "            #saving\n",
    "            diction = {}\n",
    "            d = date.today()\n",
    "            d=str(d)\n",
    "            diction.update({'Date':d})\n",
    "            diction.update({'gitHASH':str(gitHASH)})\n",
    "            diction.update({'model_name': str(model_name)})\n",
    "            diction.update({'loss_fn': str(loss)})\n",
    "            diction.update({'lr': str(lr)})\n",
    "            #diction.update({'wd': str(wd_card)})\n",
    "            #diction.update({'scheduler value': str(scheduler_value)})\n",
    "            diction.update({'seed': str(seed)})\n",
    "            diction.update({'resolution': str(resolution)})\n",
    "            diction.update({'pad': int(pad)})\n",
    "            diction.update({'lin_lay': int(lin_lay)})\n",
    "            #diction.update({'run time': (time.process_time() - run_start_time)})\n",
    "            diction.update(save_dict)\n",
    "            \n",
    "            save_location = save_dict['save_location']\n",
    "            title = save_dict['Run']\n",
    "            save2json(diction, loop_run_name, save_location)\n",
    "            save2csv(diction, title, save_location)\n",
    "\n",
    "            diction['model.state_dict'] = model.state_dict().to('cpu') #to('cpu').\n",
    "\n",
    "            with open(f\"{save_location}{loop_run_name}.pkl\", 'wb+') as f:\n",
    "                #p\n",
    "                torch.save(diction, f)\n",
    "            \n",
    "            #clear_output()\n",
    "\n",
    "            \n",
    "            #print(f' \\n END {model_name} {resolution} Run Time: ',time.process_time() - run_start_time)\n",
    "            #!nvidia-smi\n",
    "            torch.cuda.empty_cache()\n",
    "        #print('Final Run time: ',time.process_time() - start)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d7d27-ef9d-43d9-98a8-1f11ff50090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▏     | 28/100 [50:44<2:10:51, 109.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Optimizer present:  SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "_go(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964b31e-4bae-4f65-bc46-ee1334a8f239",
   "metadata": {},
   "source": [
    "# 12.58 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.04 GiB is free\n",
    "23.65-8.04\n",
    "/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/4c3l/SGD/modelCheckPoints\n",
    "/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/4c/SGD/modelCheckPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59daceaa-fa47-4c76-90d6-f23550f85694",
   "metadata": {},
   "source": [
    "\n",
    "pred torch.Size([11])\n",
    "\n",
    "lab  torch.Size([5, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0ade6-cd6d-46db-ac95-6b368cbea443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only things that have been added are more dictionary items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6c0bb-8e01-4c7a-ac94-0f899faaff7a",
   "metadata": {},
   "source": [
    "cuda memory error\n",
    "\n",
    "currently trying to reduce number of vars and delete any large ones after use (del loss after loss has been added to current loss or loss list)\n",
    "\n",
    "- removeing wanb also helps with reducing memory consumption 16/12/24\n",
    "\n",
    "14.39  261124 - reducing batch size to 32\n",
    "\n",
    "worked up to 160E.\n",
    "will train to 150, then when all done, read in the model files and continue to 300.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154693be-5f86-4945-95b3-dde95a4bc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=device, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023a4c1-bd26-406b-b140-5e9a4dd0add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf798dcf-4b05-4efd-820a-5d96411718ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/vgg16/adam/NoSched//"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

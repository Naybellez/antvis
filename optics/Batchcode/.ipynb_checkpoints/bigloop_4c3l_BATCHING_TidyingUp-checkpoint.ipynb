{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c4fd19-5a09-4232-bdc4-42bbef0b6802",
   "metadata": {},
   "source": [
    "last updated 03/12/24\n",
    "\n",
    "This notebook is to check on why current vgg16 runs aren't learning.\n",
    "this file is importing functions. file was originally a 4c file that did learn as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6dbd95-2129-42b5-b70b-ee9f057b24c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch.Utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "import collections\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "sys.path.append('../.')\n",
    "\n",
    "from functions import ImageProcessor,  IDSWDataSetLoader2\n",
    "from fns4wandb import set_lossfn\n",
    "from plotting import learning_curve, accuracy_curve, plot_confusion\n",
    "from modelManagment import get_lin_lay, choose_model2\n",
    "import copy\n",
    "\n",
    "#import torch.Utils.data.DataLoader as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c1e605-9a3b-48dc-a38e-8a955bd3c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "model_type = '4c'\n",
    "optimmy = 'Adam'\n",
    "Var_WB_sched = \"NoSched\"\n",
    "\n",
    "\n",
    "_save_location = f'/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/{model_type}/70E/' \n",
    "checkpoint_saveloc = f\"/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/{model_type}/{optimmy}/modelCheckPoints/\"\n",
    "\n",
    "data_path = r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/'\n",
    "\n",
    "gitHASH = 'ac9b2d2910b48bbc4d8cb511b231b637b7edff2a'\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "start_epoch = 0\n",
    "epoch_val = 70  # 150\n",
    "\n",
    "loadPreTrainedModel = False\n",
    "\n",
    "sim_type = \"run\" #(Test or run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1481ccd-4d87-4c05-84d5-77e29ee981b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaughticalnonsence\u001b[0m (\u001b[33mantvis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "d = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87383b",
   "metadata": {},
   "source": [
    "452 144 5/452 *100 = 1%\n",
    "226 72 5/226 *100 = 2%\n",
    "113 36 5/113 *100 = 4% -- 2/113 *100= 1.7% ~ 2%\n",
    "57 18 (56.5,) 5/57 *100 = 8% -- 2/57 *100 = 3.5% ~ 4%. 1/57 = 1.75%\n",
    "29 9 (28.5,) 5/29 *100 = 17% -- 2/29 *100 = 6.89 ~ 7% 1/28 = 3.57 ~ 4%\n",
    "15 5 (14.5, 4.5)\n",
    "8 3 (7.5,2.5)\n",
    "4, 2 (, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ddbb49-828b-4de8-a31c-2a576cd736f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries                                                                                  * * * *   SETTINGS   * * * *\n",
    "\n",
    "date = date.today()\n",
    "\n",
    "model_card_vgg = {'name': 'vgg', 'model': 'vgg16',\n",
    "                  'f_lin_lay':[200704,#200704,     #129024,#4096,  # (1x229376 and 25088x4096)  1x229376 and 25088x4096) 1x229376 and 25088x4096)\n",
    "                             200704,      #(16x64512 and 129024x4096)    (16x200704 and 64512x4096)\n",
    "                             14336,\n",
    "                             3584,\n",
    "                             768,\n",
    "                             4096,\n",
    "                             4096,\n",
    "                            ],\n",
    "                 'idx': 0,\n",
    "                 'dropout':0.2}\n",
    "\n",
    "\n",
    "model_card_7c3l = {'name': '7c3l', 'model': '7c3l', 'channels': 3, 'Ks': (3,5),\n",
    "                  'f_lin_lay':[248832,    # 452 144 # p5\n",
    "                            59904,      # 226 72 # p5\n",
    "                            11264,      # 113 36 # p2\n",
    "                            1536,       # 57 18 # p1\n",
    "                            172032,           # 29 9\n",
    "                            172032,          # 15 5\n",
    "                            172032,         # 8 3\n",
    "                              ], \n",
    "                   'idx': 1,\n",
    "                  'dropout':0.2}\n",
    "\n",
    "\n",
    "\n",
    "model_card_4c3l = {'name': '4c3l', 'model': '4c3l', 'channels': 3, 'Ks': (3,5),\n",
    "                  'f_lin_lay':[539904,# 1055232,#539904,    # 452 144 # p5  (64x539904 and 1055232x100)\n",
    "                             141056, #141056,    # 226 72 # p5   64x141056 and 267264x100)\n",
    "                             35840,     # 113 36 # p2   (64x35840 and 304640x100)\n",
    "                             9984,      # 57 18 # p1 \n",
    "                             2304,      # 29 9\n",
    "                             512,       # 15 5\n",
    "                             256],      # 8 3\n",
    "                  'idx': 2,\n",
    "                  'dropout':0.2}      \n",
    "\n",
    "model_card_3c2l = {'name': '3c2l', 'model': '3c2l', 'channels': 3, 'Ks': (3,5),\n",
    "                  'f_lin_lay':[1069888,    # 452 144 # p5\n",
    "                             274688,     #226 72 # p5\n",
    "                             68096,      # 113 36 # p2\n",
    "                             17280,      # 57 18 # p1\n",
    "                             3840,       # 29 9\n",
    "                             960,        # 15 5\n",
    "                             256],\n",
    "                  'idx': 3,\n",
    "                  'dropout':0.2}       # 8 3\n",
    "\n",
    "model_card_2c2l = {'name': '2c2l', 'model': '2c2l', 'channels': 3, 'Ks': (3,5),\n",
    "                  'f_lin_lay':[1055232 , #1032192,# 16883712,#33767424,    # 452 144 # p5 # (1x33767424 and 1055232x100) (1x5276160 and 15828480x100) 1x33767424 and 5276160x100)\n",
    "                             267264,     #226 72 # p5                   (1x1032192 and 64512x100)\n",
    "                             64512,#   1032192,#64512,      # 113 36 # p2    ### (16x1055232 and 1032192x100) ###  16x1055232 and 1032192x100)\n",
    "                             15552,      # 57 18 # p1\n",
    "                             3072,       # 29 9\n",
    "                             640,        # 15 5\n",
    "                             128],\n",
    "                  'idx': 4,\n",
    "                  'dropout':0.1}       # 8 3\n",
    "\n",
    "resolution_card_452144 = {'resolution':[452,144], 'padding':5, 'index':0}\n",
    "resolution_card_22672 = {'resolution':[226,72], 'padding':5, 'index':1}\n",
    "resolution_card_11336 = {'resolution':[113,36], 'padding':2, 'index':2}\n",
    "resolution_card_5715 = {'resolution':[57,18], 'padding':1, 'index':3}\n",
    "\n",
    "resolution_card_299 = {'resolution':[29,9], 'padding':0, 'index':4} # \n",
    "resolution_card_155 = {'resolution':[15,5], 'padding':0, 'index':5}\n",
    "resolution_card_83 = {'resolution':[8,3], 'padding':0, 'index':6}\n",
    "\n",
    "\n",
    "\n",
    "resolution_cards = [resolution_card_11336, resolution_card_5715]#[resolution_card_11336, resolution_card_5715,\n",
    "#resolution_card_299, resolution_card_155, resolution_card_83]#]#resolution_card_452144, \n",
    "#resolution_cards = [resolution_card_11336]\n",
    "\n",
    "#learning_rate_cards = [5e-5, 6e-5, 8e-5]\n",
    "#learning_rate_cards = [8.21592E-05, 6.62E-05, 6.01E-05, 5.97E-05]\n",
    "learning_rate_cards=  [1e-3] #[0.1,0.01, 1e-3,1e-4, 1e-5]#, 6e-5, 7e-5, 8e-5]\n",
    "#wd_cards = [4e-5, 5e-5, 3.00E-05, 2.00E-05]\n",
    "wd_cards =[0]\n",
    "scheduler_cards = [0]#, 0.1, 0.2]\n",
    "\n",
    "seeds = [8,2,4, 42]#,2,3] # 4, 5,6\n",
    "\n",
    "#model_cards =[model_card_vgg, model_card_7c3l, model_card_4c3l, model_card_3c2l, model_card_2c2l]\n",
    "model_cards =[model_card_vgg]\n",
    "\n",
    "loss_fn_cards = ['CrossEntropy'] #,'CrossEntropy' 'MSE'\n",
    "                        \n",
    "config = dict({'parameters': 'parameters for big loop run'})\n",
    "config.update({'model_cards':model_cards})\n",
    "config.update({'resolution_cards':resolution_cards})\n",
    "config.update({'learning_rate_cards':learning_rate_cards})\n",
    "config.update({'wd_cards':wd_cards})\n",
    "config.update({'scheduler_cards':scheduler_cards})\n",
    "config.update({'seeds':seeds})\n",
    "config.update({'loss_fn_cards': loss_fn_cards})\n",
    "\n",
    "\n",
    "config.update({'batch_size': 64})\n",
    "config.update({'start_epoch': start_epoch})\n",
    "config.update({'epochs': epoch_val}) #60\n",
    "\n",
    "#print(model_card_vgg)\n",
    "#print('')\n",
    "#Pp.pprint(Config) # dictionary of dictionaries of lists and lists of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf5ca7f-b59a-427b-8da3-2848fa6d3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelManagment import get_lin_lay, choose_model\n",
    "from functions import get_data\n",
    "from loop_fns import train_val_batch, test_loop_batch\n",
    "from fileManagment import save2csv, save2json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _go(config=None):\n",
    "\n",
    "\n",
    "    if len(gitHASH) <1:\n",
    "        print(\"YOU FORGET THE GIT HASH\")\n",
    "        return\n",
    "    else:\n",
    "        print('Git Hash registered')\n",
    "        pass\n",
    "        \n",
    "    with wandb.init(config=config, project=f\"{model_type}. {epoch_val}E. {Var_WB_sched}. {optimmy}_{sim_type}\", notes=f\"{model_type}. {epoch_val}E. {Var_WB_sched}. {optimmy}_{sim_type}\",):\n",
    "        config = wandb.config\n",
    "        start = time.process_time()\n",
    "            \n",
    "        for model_idx, model_card in enumerate(config['model_cards']):\n",
    "            #print(\"Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "                    \n",
    "            model_name = model_card['model']\n",
    "            model_index = model_card['idx']\n",
    "            dropout = model_card['dropout'] \n",
    "            for res_idx, resolution_card in enumerate(config['resolution_cards']):\n",
    "                #print(\"Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "            \n",
    "                resolution = resolution_card['resolution']\n",
    "                pad = resolution_card['padding']\n",
    "                lin_lay = get_lin_lay(model_card, resolution)\n",
    "                print('lin lay', lin_lay)\n",
    "            \n",
    "                for lr_idx, lr in enumerate(config['learning_rate_cards']):\n",
    "                    for wd_idx, wd_card in enumerate(wd_cards):\n",
    "                        for sched_idx, scheduler_value in enumerate(config['scheduler_cards']):\n",
    "                            \n",
    "                            for seed_idx, seed in enumerate(config['seeds']):\n",
    "                                seed = seed\n",
    "                                for lossfn_idx, loss in enumerate(config['loss_fn_cards']):\n",
    "                                    \n",
    "                                    torch.cuda.empty_cache()\n",
    "                                    #print('2')\n",
    "                                    #!nvidia-smi\n",
    "  \n",
    "                                    config['batch_size']\n",
    "\n",
    "                                    print('Model: ', str(model_name), f\" idx: {model_idx} / {len(config.model_cards)}\")\n",
    "                                    print('resolution: ', str(resolution), f\" idx: {res_idx} / {len(config['resolution_cards'])}\")\n",
    "                                    print('learning rate: ', str(lr), f\" idx: {lr_idx} / {len(config['learning_rate_cards'])}\")\n",
    "                                    print('weight decay: ', str(wd_card), f\" idx: {wd_idx} / {len(config['wd_cards'])}\")\n",
    "                                    print('scheduler: ', str(scheduler_value), f\" idx: {sched_idx} / {len(config['scheduler_cards'])}\")\n",
    "                                    print('seed: ', str(seed), f\" idx: {seed_idx} / {len(config['seeds'])}\")\n",
    "                                    print('loss function: ', str(loss), f\" idx: {lossfn_idx} / {len(config['loss_fn_cards'])}\")\n",
    "                                    print('Batch size: ', config['batch_size'])\n",
    "                                    print('Training epochs: ', config['epochs'])\n",
    "                                    run_start_time = time.process_time()\n",
    "                                    print('start time: ',run_start_time)\n",
    "   \n",
    "                                    print(time.process_time() - start)\n",
    "\n",
    "                                    epochs = config['epochs'] #40\n",
    "\n",
    "                                    IP = ImageProcessor(device)\n",
    "\n",
    "                                    wandb.log({'gitHash':gitHASH})\n",
    "                                    wandb.log({'Epochs': epochs})\n",
    "                                    \n",
    "                                    save_dict = {'Run' : f\"{model_name}_{resolution}_{date}_{optimmy}_{config['epochs']}E_{Var_WB_sched}_{sim_type}\",\n",
    "                                                 'Current_Epoch': config['start_epoch'], # this is where i add the start epoch\n",
    "                                                 'start_epoch':config['start_epoch'],\n",
    "                                                 'save_location' : _save_location,\n",
    "                                                 'checkpoint_save_loc': checkpoint_saveloc,\n",
    "                                                 'res': resolution,\n",
    "                                                 'scheduler': Var_WB_sched,\n",
    "                                                 'model': model_name,\n",
    "                                                 'optimiser': optimmy,\n",
    "                                                 'seed':seed}\n",
    "                                    \n",
    "                                    print(\"model name\", model_name, \" flinlay: \", lin_lay, \" dropout:\", dropout)\n",
    "                                    model = choose_model2(model_name, lin_lay, dropout).to(device)\n",
    "                                    \n",
    "                                    # This is where I want to add the new code\n",
    "                                    if loadPreTrainedModel:\n",
    "                                        dir_pkl = f\"/its/home/nn268/antvis/antvis/optics/res_big_loop_saves/models/batch/schedulerRuns/4c/SGD/{Var_WB_sched}/\"\n",
    "                                        pkl_name = f\"{model_type}_{resolution}_2024-11-26_SGD_150E_{Var_WB_sched}_{resolution}_0.001_0_{seed}_CrossEntropy.pkl\"\n",
    "                                        #model_pkl = torch.load(dir_pkl+pkl_name)\n",
    "                                        with open(dir_pkl+pkl_name, 'rb') as f:\n",
    "                                            model_pkl = pickle.load(f)\n",
    "                                        #model_pkl = pickle.load(dir_pkl+pkl_name)\n",
    "                                        model.load_state_dict(model_pkl['model.state_dict'])\n",
    "\n",
    "                                    \n",
    "\n",
    "                                    #print(\"After model init, Before data loading - Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "\n",
    "                                    x_train, y_train, x_val, y_val, x_test, y_test = get_data(random_seed=seed, file_path=data_path) #, file_path\n",
    "                                    av_lum = IP.new_luminance(x_train)\n",
    "                                    #print(\"Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "                                    \n",
    "                                    train_ds = IDSWDataSetLoader2(x_train, y_train, resolution,pad,av_lum,model_name, device)# av_lum, res,pad,\n",
    "                                    train = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "\n",
    "                                    \n",
    "                                    test_ds = IDSWDataSetLoader2(x_test, y_test, resolution,pad,av_lum,model_name, device)\n",
    "                                    test = DataLoader(test_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "                                    #print(\"Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "                                    val_ds = IDSWDataSetLoader2(x_val, y_val, resolution,pad,av_lum,model_name, device)\n",
    "                                    val = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=True, drop_last=True) #, num_workers=2\n",
    "                                    \n",
    "                                    print(\"After data loading - Current allocated memory (GB):\", torch.cuda.memory_allocated() / 1024 ** 3)\n",
    "\n",
    "\n",
    "                                    #print('5')\n",
    "                                    #!nvidia-smi\n",
    "\n",
    "                                    loss_fn = set_lossfn(loss)\n",
    "                                    \n",
    "                                    # set optimizer\n",
    "                                    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)#torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "                                    wandb.watch(model, loss_fn, log='all', log_freq=2, idx = model_index)\n",
    "\n",
    "                                    loop_run_name = f\"{save_dict['Run']}_{resolution}_{lr}_{scheduler_value}_{seed}_{loss}\"\n",
    "         \n",
    "                                    model, save_dict=  train_val_batch(model, train,val, loop_run_name,save_dict, lr, loss_fn,epochs, config['batch_size'], optimizer, scheduler_value, device)\n",
    "\n",
    "                                    test_acc,test_predict_list, y_test = test_loop_batch(model,test, loss_fn, config['batch_size'], device) #model, model_name, X, Y, res, pad, loss_fn, device, num_classes=11\n",
    "                                    \n",
    "                                    #print(test_predict_list)\n",
    "                                    print(' \\n train Acc: ', save_dict['t_accuracy_list'][-1])\n",
    "                                    print(' \\n val Acc: ', save_dict['v_accuracy_list'][-1])\n",
    "                                    print(' \\n test Acc: ', test_acc)\n",
    "                                    \n",
    "                                    save_dict.update({'test_acc': test_acc})\n",
    "                                    save_dict.update({'test_predict': test_predict_list})\n",
    "                                    save_dict.update({'test_labels': list(y_test)})\n",
    "                                    #save_dict.update({'test_loss':test_loss})\n",
    "\n",
    "                                    learning_curve(save_dict['t_loss_list'], save_dict['v_loss_list'], save_location=save_dict['save_location'],run_name=loop_run_name)\n",
    "                                    accuracy_curve(save_dict['t_accuracy_list'], save_dict['v_accuracy_list'],save_location=save_dict['save_location'],run_name=loop_run_name)\n",
    "                                    test_predict_list=[pred.cpu() for pred in test_predict_list]\n",
    "                                    plot_confusion(predictions= test_predict_list, actual= y_test, title = \"Test Confusion matrix\", run_name = loop_run_name,save_location =save_dict['save_location'])\n",
    "                                    \n",
    "                                    wandb.log({'test_acc': test_acc})\n",
    "                                    wandb.log({'test_predict': test_predict_list})\n",
    "                                    wandb.log({'test_labels': list(y_test)})\n",
    "                                    #saving\n",
    "                                    diction = {}\n",
    "                                    d = date.today()\n",
    "                                    d=str(d)\n",
    "                                    diction.update({'Date':d})\n",
    "                                    diction.update({'gitHASH':str(gitHASH)})\n",
    "                                    diction.update({'model_name': str(model_name)})\n",
    "                                    diction.update({'loss_fn': str(loss)})\n",
    "                                    diction.update({'lr': str(lr)})\n",
    "                                    diction.update({'wd': str(wd_card)})\n",
    "                                    diction.update({'scheduler value': str(scheduler_value)})\n",
    "                                    diction.update({'seed': str(seed)})\n",
    "                                    diction.update({'resolution': str(resolution)})\n",
    "                                    diction.update({'pad': int(pad)})\n",
    "                                    diction.update({'lin_lay': int(lin_lay)})\n",
    "                                    diction.update({'run time': (time.process_time() - run_start_time)})\n",
    "                                    diction.update(save_dict)\n",
    "                                    \n",
    "                                    save_location = save_dict['save_location']\n",
    "                                    title = save_dict['Run']\n",
    "                                    save2json(diction, loop_run_name, save_location)\n",
    "                                    save2csv(diction, title, save_location)\n",
    "        \n",
    "                                    diction['model.state_dict'] = model.state_dict() #to('cpu').\n",
    "        \n",
    "                                    with open(f\"{save_location}{loop_run_name}.pkl\", 'wb+') as f:\n",
    "                                        #pickle.dump(diction, f)\n",
    "                                        torch.save(diction, f)\n",
    "                                    \n",
    "                                    clear_output()\n",
    "                                    \n",
    "                                    print(f' \\n END {model_name} {resolution} Run Time: ',time.process_time() - run_start_time)\n",
    "                                    #!nvidia-smi\n",
    "                                    torch.cuda.empty_cache()\n",
    "        print('Final Run time: ',time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58d7d27-ef9d-43d9-98a8-1f11ff50090d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git Hash registered\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/its/home/nn268/antvis/antvis/optics/Batchcode/wandb/run-20250107_134537-8m5q57nd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antvis/4c.%2070E.%20NoSched.%20Adam_run/runs/8m5q57nd' target=\"_blank\">honest-lake-3</a></strong> to <a href='https://wandb.ai/antvis/4c.%2070E.%20NoSched.%20Adam_run' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antvis/4c.%2070E.%20NoSched.%20Adam_run' target=\"_blank\">https://wandb.ai/antvis/4c.%2070E.%20NoSched.%20Adam_run</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antvis/4c.%2070E.%20NoSched.%20Adam_run/runs/8m5q57nd' target=\"_blank\">https://wandb.ai/antvis/4c.%2070E.%20NoSched.%20Adam_run/runs/8m5q57nd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin lay 14336\n",
      "Model:  vgg16  idx: 0 / 1\n",
      "resolution:  [113, 36]  idx: 0 / 2\n",
      "learning rate:  0.001  idx: 0 / 1\n",
      "weight decay:  0  idx: 0 / 1\n",
      "scheduler:  0  idx: 0 / 1\n",
      "seed:  8  idx: 0 / 4\n",
      "loss function:  CrossEntropy  idx: 0 / 1\n",
      "Batch size:  64\n",
      "Training epochs:  70\n",
      "start time:  12.578875967\n",
      "0.001237924999999862\n",
      "model name vgg16  flinlay:  14336  dropout: 0.2\n",
      "/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/\n",
      "After data loading - Current allocated memory (GB): 0.501183032989502\n",
      "Before Epochs of training - Current allocated memory (GB): 0.501183032989502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                               | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Optimizer present:  SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "training..  2\n",
      "train accuracy:  9.314903846153847\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                               | 0/70 [00:06<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_108934/1815666770.py\", line 129, in _go\n",
      "    model, save_dict=  train_val_batch(model, train,val, loop_run_name,save_dict, lr, loss_fn,epochs, config['batch_size'], optimizer, scheduler_value, device)\n",
      "  File \"/its/home/nn268/antvis/antvis/optics/Batchcode/.././loop_fns.py\", line 335, in train_val_batch\n",
      "    v_loss, val_prediction, v_label_list, val_correct= loop_batch(model, val, loss_fn, batch_size,sample,random_value,epoch,loop_run_name, save_dict, device, optimizer =None, scheduler= None, train =False)\n",
      "  File \"/its/home/nn268/antvis/antvis/optics/Batchcode/.././loop_fns.py\", line 241, in loop_batch\n",
      "    if y_batch[i].argmax() == prediction[i].argmax():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 5.1%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epochs</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epochs</td><td>70</td></tr><tr><td>gitHash</td><td>3e1975b1ac9d8c07dae4...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">honest-lake-3</strong> at: <a href='https://wandb.ai/antvis/4c.%2070E.%20NoSched.%20Adam_run/runs/8m5q57nd' target=\"_blank\">https://wandb.ai/antvis/4c.%2070E.%20NoSched.%20Adam_run/runs/8m5q57nd</a><br/> View job at <a href='https://wandb.ai/antvis/4c.%2070E.%20NoSched.%20Adam_run/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjUzMTIwNDE2MA==/version_details/v1' target=\"_blank\">https://wandb.ai/antvis/4c.%2070E.%20NoSched.%20Adam_run/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjUzMTIwNDE2MA==/version_details/v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250107_134537-8m5q57nd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m_go\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 129\u001b[0m, in \u001b[0;36m_go\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    125\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwatch(model, loss_fn, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, log_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, idx \u001b[38;5;241m=\u001b[39m model_index)\n\u001b[1;32m    127\u001b[0m loop_run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRun\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 129\u001b[0m model, save_dict\u001b[38;5;241m=\u001b[39m  \u001b[43mtrain_val_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_run_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m test_acc,test_predict_list, y_test \u001b[38;5;241m=\u001b[39m test_loop_batch(model,test, loss_fn, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], device) \u001b[38;5;66;03m#model, model_name, X, Y, res, pad, loss_fn, device, num_classes=11\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m#print(test_predict_list)\u001b[39;00m\n",
      "File \u001b[0;32m~/antvis/antvis/optics/Batchcode/.././loop_fns.py:335\u001b[0m, in \u001b[0;36mtrain_val_batch\u001b[0;34m(model, train, val, loop_run_name, save_dict, lr, loss_fn, epochs, batch_size, optimizer, scheduler_value, device)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidating...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m#!nvidia-smi\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m v_loss, val_prediction, v_label_list, val_correct\u001b[38;5;241m=\u001b[39m \u001b[43mloop_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrandom_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloop_run_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m v_loss_list\u001b[38;5;241m.\u001b[39mappend(v_loss)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m#[v_predict_list.append(pred) for pred in val_prediction]\u001b[39;00m\n",
      "File \u001b[0;32m~/antvis/antvis/optics/Batchcode/.././loop_fns.py:241\u001b[0m, in \u001b[0;36mloop_batch\u001b[0;34m(model, data, loss_fn, batch_size, sample, random_value, epoch, loop_run_name, save_dict, device, optimizer, scheduler, train)\u001b[0m\n\u001b[1;32m    238\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_batch)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_batch[i]\u001b[38;5;241m.\u001b[39margmax() \u001b[38;5;241m==\u001b[39m prediction[i]\u001b[38;5;241m.\u001b[39margmax():\n\u001b[1;32m    242\u001b[0m         num_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    245\u001b[0m [predict_list\u001b[38;5;241m.\u001b[39mappend(pred\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m prediction]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_go(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964b31e-4bae-4f65-bc46-ee1334a8f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.58 GiB. GPU 0 has a total capacty of 23.65 GiB of which 8.04 GiB is free\n",
    "23.65-8.04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59daceaa-fa47-4c76-90d6-f23550f85694",
   "metadata": {},
   "source": [
    "\n",
    "pred torch.Size([11])\n",
    "\n",
    "lab  torch.Size([5, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67bda7-4c6a-451c-9183-440a8fc39f03",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "18/11/24\n",
    "Added in the checkpoints every 25 epochs\n",
    "Checked that optimizer = SGD and not optimizer =optimizer = SGD\n",
    "set total epochs to 150.\n",
    "added in the creation and saving of LC and AC at each checkpoint.\n",
    "so checkpoints have a save of the model state dicts and lc and ac so far.\n",
    "\n",
    "HOWEVER\n",
    "there still does not look to be any learning... lc are flat and do not change over epochs.\n",
    "in the pytroch sgd page, the examples use momentum (0.9)\n",
    "considering running again with  momentum.\n",
    "\n",
    "hmm. no. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

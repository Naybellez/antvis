{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 050224\n",
    "# testing dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaughticalnonsence\u001b[0m (\u001b[33mantvis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as maths\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "#from torchsummary import summary\n",
    "#import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import wandb\n",
    "import pprint\n",
    "\n",
    "\n",
    "from functions import import_imagedata, get_data, label_oh_tf,  Unwrap, ImageProcessor,IDSWDataSetLoader\n",
    "from architectures import vgg16net, smallnet1, smallnet2, smallnet3\n",
    "from loop_fns import loop, test_loop\n",
    "from fns4wandb import build_optimizer, set_optimizer, hp_sweep, train_model, train_log, log_test_score, set_lossfn, pipeline, train_DL\n",
    "\n",
    "from architectures import build_net, smallnet3\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from loop_fns import loop\\n\\ndef pipeline(config, col_dict,save_dict, title, device, seed):\\n    x_train, y_train, x_val, y_val, x_test, y_test = get_data(r\\'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/\\', seed)\\n\\n    with wandb.init(project=title, config=config):\\n        config = wandb.config\\n        model = choose_model(config).to(device) ###\\n        loss_fn = set_lossfn(config.loss_fn)\\n        \\n        #t_save_dict = train_model(model, x_train, y_train, x_val, y_val, loss_fn, config, col_dict, save_dict, device)\\n        # train model begins\\n        wandb.watch(model, loss_fn, log=\\'all\\', log_freq=10)\\n        sample_count =0\\n        batch_count = 0\\n        e_count = 0\\n        t_loss_list = []\\n        v_loss_list =[]\\n        t_predict_list = []\\n        t_label_list = []\\n        v_predict_list = []\\n        v_label_list = []\\n        t_accuracy_list= []\\n        v_accuracy_list= []\\n\\n        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config.scheduler, last_epoch=-1)\\n        \\n        for epoch in tqdm(range(config.epochs)):            \\n            t_loss_, predict_list_, t_num_correct, t_label_list_, model, optimizer = loop(model, x_train, y_train, epoch, loss_fn, device, col_dict, config.num_classes, optimizer=optimizer, scheduler=scheduler) #model, x_train, y_train, epoch, loss_fn, device, col_dict, config.num_classes, optimizer=optimizer, scheduler=scheduler\\n            sample_count += len(x_train)\\n            t_loss_list.append(t_loss_)\\n            t_predict_list.append(predict_list_)\\n            t_label_list.append(t_label_list_)\\n            t_accuracy_list.append(t_num_correct/len(x_train))\\n            v_loss_, v_predict_list_, v_num_correct, v_label_list_= loop(model, x_val,y_val, epoch, loss_fn, device,col_dict, config.num_classes, train=False) \\n            v_loss_list.append(v_loss_)\\n            v_predict_list.append(v_predict_list_)\\n            v_label_list.append(v_label_list_)\\n            v_accuracy_list.append(v_num_correct/len(x_val))\\n\\n        save_dict[\\'Current_Epoch\\'] = config[\\'epochs\\']\\n        save_dict[\\'training_samples\\'] = len(x_train)\\n        save_dict[\\'validation_samples\\'] = len(x_val)\\n        save_dict[\\'t_loss_list\\'] = t_loss_list #\\n        save_dict[\\'t_predict_list\\'] = [[c.to(\\'cpu\\') for c in k]for k in t_predict_list] #\\n        save_dict[\\'t_accuracy_list\\'] = t_accuracy_list #\\n        save_dict[\\'v_loss_list\\'] = v_loss_list #\\n        save_dict[\\'v_predict_list\\'] = [[c.to(\\'cpu\\') for c in k]for k in v_predict_list]#\\n        save_dict[\\'v_accuracy_list\\'] = v_accuracy_list #\\n        save_dict[\\'t_labels\\'] = [[c.to(\\'cpu\\') for c in k]for k in t_label_list]\\n        save_dict[\\'v_labels\\'] = [[c.to(\\'cpu\\') for c in k] for k in v_label_list]\\n\\n\\n        test_loop(model, x_test, y_test, loss_fn, device, col_dict, title, config.num_classes) \\n\\n        title = save_dict[\\'Run\\']\\n        with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", \\'wb+\\') as f:\\n            pickle.dump(save_dict, f)\\n\\n    return model\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from loop_fns import loop\n",
    "\n",
    "def pipeline(config, col_dict,save_dict, title, device, seed):\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = get_data(r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/', seed)\n",
    "\n",
    "    with wandb.init(project=title, config=config):\n",
    "        config = wandb.config\n",
    "        model = choose_model(config).to(device) ###\n",
    "        loss_fn = set_lossfn(config.loss_fn)\n",
    "        \n",
    "        #t_save_dict = train_model(model, x_train, y_train, x_val, y_val, loss_fn, config, col_dict, save_dict, device)\n",
    "        # train model begins\n",
    "        wandb.watch(model, loss_fn, log='all', log_freq=10)\n",
    "        sample_count =0\n",
    "        batch_count = 0\n",
    "        e_count = 0\n",
    "        t_loss_list = []\n",
    "        v_loss_list =[]\n",
    "        t_predict_list = []\n",
    "        t_label_list = []\n",
    "        v_predict_list = []\n",
    "        v_label_list = []\n",
    "        t_accuracy_list= []\n",
    "        v_accuracy_list= []\n",
    "\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config.scheduler, last_epoch=-1)\n",
    "        \n",
    "        for epoch in tqdm(range(config.epochs)):            \n",
    "            t_loss_, predict_list_, t_num_correct, t_label_list_, model, optimizer = loop(model, x_train, y_train, epoch, loss_fn, device, col_dict, config.num_classes, optimizer=optimizer, scheduler=scheduler) #model, x_train, y_train, epoch, loss_fn, device, col_dict, config.num_classes, optimizer=optimizer, scheduler=scheduler\n",
    "            sample_count += len(x_train)\n",
    "            t_loss_list.append(t_loss_)\n",
    "            t_predict_list.append(predict_list_)\n",
    "            t_label_list.append(t_label_list_)\n",
    "            t_accuracy_list.append(t_num_correct/len(x_train))\n",
    "            v_loss_, v_predict_list_, v_num_correct, v_label_list_= loop(model, x_val,y_val, epoch, loss_fn, device,col_dict, config.num_classes, train=False) \n",
    "            v_loss_list.append(v_loss_)\n",
    "            v_predict_list.append(v_predict_list_)\n",
    "            v_label_list.append(v_label_list_)\n",
    "            v_accuracy_list.append(v_num_correct/len(x_val))\n",
    "\n",
    "        save_dict['Current_Epoch'] = config['epochs']\n",
    "        save_dict['training_samples'] = len(x_train)\n",
    "        save_dict['validation_samples'] = len(x_val)\n",
    "        save_dict['t_loss_list'] = t_loss_list #\n",
    "        save_dict['t_predict_list'] = [[c.to('cpu') for c in k]for k in t_predict_list] #\n",
    "        save_dict['t_accuracy_list'] = t_accuracy_list #\n",
    "        save_dict['v_loss_list'] = v_loss_list #\n",
    "        save_dict['v_predict_list'] = [[c.to('cpu') for c in k]for k in v_predict_list]#\n",
    "        save_dict['v_accuracy_list'] = v_accuracy_list #\n",
    "        save_dict['t_labels'] = [[c.to('cpu') for c in k]for k in t_label_list]\n",
    "        save_dict['v_labels'] = [[c.to('cpu') for c in k] for k in v_label_list]\n",
    "\n",
    "\n",
    "        test_loop(model, x_test, y_test, loss_fn, device, col_dict, title, config.num_classes) \n",
    "\n",
    "        title = save_dict['Run']\n",
    "        with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", 'wb+') as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "\n",
    "    return model\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\ngrid search\\n\\'learning_rate\\': {\\n            \\'values\\': [6.1e-5, 6.2e-5, 6.3e-5, 6.4e-5]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "grid search\n",
    "'learning_rate': {\n",
    "            'values': [6.1e-5, 6.2e-5, 6.3e-5, 6.4e-5]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"config = {\\n    'method': 'random',\\n    'metric':{\\n        'goal': 'minimize',\\n        'name': 'val_loss'},\\n    'parameters': {\\n        'dropout':{\\n            'values': [0.3]\\n        },\\n        'weight_decay':{\\n            'values': [3e-5]\\n        },\\n        'epochs':{\\n            'value': 80\\n        },\\n        'lin_layer_size': {\\n            'values': [100] #, 150, 50\\n        },\\n        'first_lin_lay':{\\n            'values':[67968]#67968 67968\\n        },\\n        'optimizer': {\\n            'values': ['adam']\\n        },\\n            'learning_rate': {\\n                'values': [6.01E-03]\\n            },\\n        'scheduler': {\\n            'values': [0.2]\\n        },\\n        'loss_fn': {\\n            'values': ['MSE', 'CrossEntropy']\\n        },\\n        'data_set':{\\n            'values':['Augmented']\\n        },\\n        'num_classes' : {\\n            'values':[11]\\n        },\\n        'ks': {\\n            'values': [(3,5)]\\n        },\\n        'model_name' : {'values': ['smallnet3']},\\n        'channels' : {'values': [3]},\\n        'image_path': {\\n            'values': [r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/']\\n        }\\n    }\\n}\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"config = {\n",
    "    'method': 'random',\n",
    "    'metric':{\n",
    "        'goal': 'minimize',\n",
    "        'name': 'val_loss'},\n",
    "    'parameters': {\n",
    "        'dropout':{\n",
    "            'values': [0.3]\n",
    "        },\n",
    "        'weight_decay':{\n",
    "            'values': [3e-5]\n",
    "        },\n",
    "        'epochs':{\n",
    "            'value': 80\n",
    "        },\n",
    "        'lin_layer_size': {\n",
    "            'values': [100] #, 150, 50\n",
    "        },\n",
    "        'first_lin_lay':{\n",
    "            'values':[67968]#67968 67968\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam']\n",
    "        },\n",
    "            'learning_rate': {\n",
    "                'values': [6.01E-03]\n",
    "            },\n",
    "        'scheduler': {\n",
    "            'values': [0.2]\n",
    "        },\n",
    "        'loss_fn': {\n",
    "            'values': ['MSE', 'CrossEntropy']\n",
    "        },\n",
    "        'data_set':{\n",
    "            'values':['Augmented']\n",
    "        },\n",
    "        'num_classes' : {\n",
    "            'values':[11]\n",
    "        },\n",
    "        'ks': {\n",
    "            'values': [(3,5)]\n",
    "        },\n",
    "        'model_name' : {'values': ['smallnet3']},\n",
    "        'channels' : {'values': [3]},\n",
    "        'image_path': {\n",
    "            'values': [r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "config = dict(\n",
    "    model_name = 'smallnet3',\n",
    "    epochs= 120, #120, \n",
    "    learning_rate =6.01E-05, #8.22E-05,# 6.40E-05  #8e-5,\n",
    "    dataset= 'IDSW_Aug',\n",
    "    architecture ='CNN',\n",
    "    optimizer= 'adam',\n",
    "    loss_fn = 'CrossEntropy',\n",
    "    weight_decay= 3.00E-05, #4e-5, #1.00E-05\n",
    "    dropout = 0.3, #0.4, #0.5 \n",
    "    first_lin_lay =1064448, #70272, #67968,#1087488,#1124352,#67968,#1087488, #70272,#1124352, #67968, #1087488, #67968, #1087488,\n",
    "    lin_layer_size= 100,\n",
    "    ks =3,#(3,5),\n",
    "    in_chan = 3,\n",
    "    num_classes =11,\n",
    "    scheduler = 0.2,\n",
    "    channels = 3\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "config = {\n",
    "     \"model_name\" : 'smallnet3',\n",
    "    \"first_lin_lay\" :995328,#36864,#4091904,#995328,#186368,#46592,#11648,# 477568,#46592,#11648,#46592, #11648,#46592,#4128768, #4128768, #13888, #267264, #1032192, #4128768, #1032192,#4128768, #267264, #13888, #1055232, #67968,#1055232,\n",
    "    \"epochs\" :120,#120, \n",
    "    \"learning_rate\" : 8.21592E-05, #6.62E-05, #5.97E-05, #6.01E-05, #6.62E-05, #0.00, 00821591686076769, #8e-5,\n",
    "    \"dataset\" : 'IDSW_Aug',\n",
    "    \"architecture\" :'CNN',\n",
    "    \"optimizer\": 'adam',\n",
    "    \"loss_fn\" : 'CrossEntropy',\n",
    "    \"weight_decay\": 4e-5, #4e-5, #2e-5, #3.00E-05,\n",
    "    \"dropout\" : 0.4, #0.4,\n",
    "    \"lin_layer_size\": 100,\n",
    "    \"ks\" : [3,5],\n",
    "    \"in_chan\" : 3,\n",
    "    \"num_classes\" :11,\n",
    "    \"scheduler\" : 0.2,\n",
    "    \"channels\": 3\n",
    "     \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "col_dict = {\n",
    "    'colour': 'colour',\n",
    "    'size': [113,36],\n",
    "    'padding': 5,\n",
    "    'model_size': '2c2l'\n",
    "}\n",
    "\n",
    "title = f\"IDSWAug_2c2l_e120_{col_dict['size']}_DL2_140224\"\n",
    "save_dict = {'Run' : title,\n",
    "            'Current_Epoch': 0,\n",
    "            'save_location' : r'pickles/'}\n",
    "#r'/its/home/nn268/antvis/optics/\n",
    "#pickles\n",
    "\n",
    "\n",
    "#sweep_id = wandb.sweep(config, project=title+f\"_{col_dict['colour']}_{col_dict['size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train(config=None):\\n    # lists for save dict\\n    t_loss_list = []\\n    v_loss_list =[]\\n    t_predict_list = []\\n    t_label_list = []\\n    v_predict_list = []\\n    v_label_list = []\\n    t_accuracy_list= []\\n    v_accuracy_list= []\\n    \\n    with wandb.init(config=config):\\n        config = wandb.config\\n        \\n        x_train, y_train, x_val, y_val, x_test, y_test = get_data(file_path= r\\'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/\\', seed=seed)\\n        \\n        model =smallnet3(in_chan=3, f_lin_lay=67968, l_lin_lay=11, ks=(3,5)).to(device) #10368\\n        if config.loss_fn == \\'MSE\\':\\n            loss_fn = nn.MSELoss()\\n        elif config.loss_fn == \\'CrossEntropy\\':\\n            loss_fn = nn.CrossEntropyLoss()\\n\\n        e_count = 0\\n         # *\\n\\n        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\\n\\n        for epoch in range(config.epochs):\\n            # current_loss, predict_list, num_correct, label_list, model, optimizer\\n            t_loss, t_predict_list_, t_num_correct, t_label_list_, model, optimizer = loop(model, x_train, y_train, epoch, loss_fn, device, col_dict, num_classes=11, optimizer=optimizer)\\n            t_accuracy = (t_num_correct /len(x_train))*100\\n            t_loss_list.append(t_loss)\\n            t_predict_list.append(t_predict_list_)\\n            t_label_list.append(t_label_list_)\\n            t_accuracy_list.append(t_accuracy)\\n\\n            v_loss, v_predict_list_, v_num_correct, v_label_list_= loop(model, x_val, y_val, epoch, loss_fn, device,col_dict,num_classes=11, train=False)\\n            v_accuracy= (v_num_correct / len(x_val))*100\\n            v_loss_list.append(v_loss)\\n            v_predict_list.append(v_predict_list_)\\n            v_label_list.append(v_label_list_)\\n            v_accuracy_list.append(v_accuracy)\\n\\n            t_avg_loss =t_loss/len(x_train)\\n            v_avg_loss = v_loss /len(x_val)\\n\\n            e_count +=1\\n            # logging\\n            wandb.log({\\'avg_train_loss\\': t_avg_loss, \\'epoch\\':epoch})\\n            wandb.log({\\'avg_val_loss\\': v_avg_loss, \\'epoch\\':epoch})\\n\\n            wandb.log({\\'train_loss\\': t_loss, \\'epoch\\':epoch})\\n            wandb.log({\\'val_loss\\': v_loss, \\'epoch\\':epoch})\\n\\n            wandb.log({\\'train_correct\\': t_num_correct, \\'epoch\\':epoch})\\n            wandb.log({\\'val_correct\\': v_num_correct, \\'epoch\\':epoch})\\n\\n            wandb.log({\\'train_accuracy_%\\': t_accuracy, \\'epoch\\':epoch})\\n            wandb.log({\\'val_accuracy_%\\': v_accuracy, \\'epoch\\':epoch})\\n\\n            wandb.log({\\'t_labels\\': t_label_list, \\'epoch\\':epoch})\\n            wandb.log({\\'v_labels\\': v_label_list, \\'epoch\\':epoch})\\n\\n            wandb.log({\\'t_predictions\\': t_predict_list, \\'epoch\\':epoch})\\n            wandb.log({\\'v_predictions\\': v_predict_list, \\'epoch\\':epoch})\\n\\n            # add lists to save dict after all epochs run\\n    save_dict[\\'Current_Epoch\\'] = config[\\'epochs\\']\\n    save_dict[\\'training_samples\\'] = len(x_train)# should this be the whole list for future graphs...?\\n    save_dict[\\'validation_samples\\'] = len(x_val)\\n    save_dict[\\'t_loss_list\\'] = t_loss_list #[c.to(\\'cpu\\') for c in t_loss_list]\\n    save_dict[\\'t_predict_list\\'] = [[c.to(\\'cpu\\') for c in k]for k in t_predict_list] #[[c.to(\\'cpu\\') for c in k]for k in t_predict_list]  # [c.to(\\'cpu\\') for c in t_predict_list] \\n    save_dict[\\'t_accuracy_list\\'] = t_accuracy_list #\\n    save_dict[\\'v_loss_list\\'] = v_loss_list #[c.to(\\'cpu\\') for c in v_loss_list]\\n    save_dict[\\'v_predict_list\\'] = [[c.to(\\'cpu\\') for c in k]for k in v_predict_list]#[[c.to(\\'cpu\\') for c in k]for k in v_predict_list] # [c.to(\\'cpu\\') for c in v_predict_list]\\n    save_dict[\\'v_accuracy_list\\'] = v_accuracy_list #\\n    save_dict[\\'t_labels\\'] = [[c.to(\\'cpu\\') for c in k]for k in t_label_list]\\n    save_dict[\\'v_labels\\'] = [[c.to(\\'cpu\\') for c in k] for k in v_label_list]\\n        \\n    test_predictions, test_y, test_accuracy = test_loop(model, x_test, y_test, loss_fn, device, col_dict, title, config.num_classes)\\n    save_dict[\\'test_predictions\\']= [c.to(\\'cpu\\') for c in test_predictions]\\n    save_dict[\\'test_labels\\'] = [c.to(\\'cpu\\') for c in test_y]\\n    save_dict[\\'test_acc\\'] = test_accuracy\\n\\n    title = save_dict[\\'Run\\']\\n    with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", \\'wb+\\') as f:\\n        pickle.dump(save_dict, f)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from functions import get_data\n",
    "#from loop_fns import test_loop\n",
    "\"\"\"\n",
    "def train(config=None):\n",
    "    # lists for save dict\n",
    "    t_loss_list = []\n",
    "    v_loss_list =[]\n",
    "    t_predict_list = []\n",
    "    t_label_list = []\n",
    "    v_predict_list = []\n",
    "    v_label_list = []\n",
    "    t_accuracy_list= []\n",
    "    v_accuracy_list= []\n",
    "    \n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        x_train, y_train, x_val, y_val, x_test, y_test = get_data(file_path= r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/', seed=seed)\n",
    "        \n",
    "        model =smallnet3(in_chan=3, f_lin_lay=67968, l_lin_lay=11, ks=(3,5)).to(device) #10368\n",
    "        if config.loss_fn == 'MSE':\n",
    "            loss_fn = nn.MSELoss()\n",
    "        elif config.loss_fn == 'CrossEntropy':\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        e_count = 0\n",
    "         # *\n",
    "\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            # current_loss, predict_list, num_correct, label_list, model, optimizer\n",
    "            t_loss, t_predict_list_, t_num_correct, t_label_list_, model, optimizer = loop(model, x_train, y_train, epoch, loss_fn, device, col_dict, num_classes=11, optimizer=optimizer)\n",
    "            t_accuracy = (t_num_correct /len(x_train))*100\n",
    "            t_loss_list.append(t_loss)\n",
    "            t_predict_list.append(t_predict_list_)\n",
    "            t_label_list.append(t_label_list_)\n",
    "            t_accuracy_list.append(t_accuracy)\n",
    "\n",
    "            v_loss, v_predict_list_, v_num_correct, v_label_list_= loop(model, x_val, y_val, epoch, loss_fn, device,col_dict,num_classes=11, train=False)\n",
    "            v_accuracy= (v_num_correct / len(x_val))*100\n",
    "            v_loss_list.append(v_loss)\n",
    "            v_predict_list.append(v_predict_list_)\n",
    "            v_label_list.append(v_label_list_)\n",
    "            v_accuracy_list.append(v_accuracy)\n",
    "\n",
    "            t_avg_loss =t_loss/len(x_train)\n",
    "            v_avg_loss = v_loss /len(x_val)\n",
    "\n",
    "            e_count +=1\n",
    "            # logging\n",
    "            wandb.log({'avg_train_loss': t_avg_loss, 'epoch':epoch})\n",
    "            wandb.log({'avg_val_loss': v_avg_loss, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'train_loss': t_loss, 'epoch':epoch})\n",
    "            wandb.log({'val_loss': v_loss, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'train_correct': t_num_correct, 'epoch':epoch})\n",
    "            wandb.log({'val_correct': v_num_correct, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'train_accuracy_%': t_accuracy, 'epoch':epoch})\n",
    "            wandb.log({'val_accuracy_%': v_accuracy, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'t_labels': t_label_list, 'epoch':epoch})\n",
    "            wandb.log({'v_labels': v_label_list, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'t_predictions': t_predict_list, 'epoch':epoch})\n",
    "            wandb.log({'v_predictions': v_predict_list, 'epoch':epoch})\n",
    "\n",
    "            # add lists to save dict after all epochs run\n",
    "    save_dict['Current_Epoch'] = config['epochs']\n",
    "    save_dict['training_samples'] = len(x_train)# should this be the whole list for future graphs...?\n",
    "    save_dict['validation_samples'] = len(x_val)\n",
    "    save_dict['t_loss_list'] = t_loss_list #[c.to('cpu') for c in t_loss_list]\n",
    "    save_dict['t_predict_list'] = [[c.to('cpu') for c in k]for k in t_predict_list] #[[c.to('cpu') for c in k]for k in t_predict_list]  # [c.to('cpu') for c in t_predict_list] \n",
    "    save_dict['t_accuracy_list'] = t_accuracy_list #\n",
    "    save_dict['v_loss_list'] = v_loss_list #[c.to('cpu') for c in v_loss_list]\n",
    "    save_dict['v_predict_list'] = [[c.to('cpu') for c in k]for k in v_predict_list]#[[c.to('cpu') for c in k]for k in v_predict_list] # [c.to('cpu') for c in v_predict_list]\n",
    "    save_dict['v_accuracy_list'] = v_accuracy_list #\n",
    "    save_dict['t_labels'] = [[c.to('cpu') for c in k]for k in t_label_list]\n",
    "    save_dict['v_labels'] = [[c.to('cpu') for c in k] for k in v_label_list]\n",
    "        \n",
    "    test_predictions, test_y, test_accuracy = test_loop(model, x_test, y_test, loss_fn, device, col_dict, title, config.num_classes)\n",
    "    save_dict['test_predictions']= [c.to('cpu') for c in test_predictions]\n",
    "    save_dict['test_labels'] = [c.to('cpu') for c in test_y]\n",
    "    save_dict['test_acc'] = test_accuracy\n",
    "\n",
    "    title = save_dict['Run']\n",
    "    with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", 'wb+') as f:\n",
    "        pickle.dump(save_dict, f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): # kemals dl\n",
    "\tdef __init__(self, x, y, col_dict, device):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\tself.col_dict = col_dict\n",
    "\t\tself.device = device\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.y)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tsize = self.col_dict['size']\n",
    "\t\timg = cv2.imread(self.x[idx])\n",
    "\t\timg = cv2.resize(img, (size[0], size[1]))\n",
    "\t\timg =  img / 255.\n",
    "\t\timg = img.astype(np.float32)\n",
    "\t\t#img = np.expand_dims(img, 0)\n",
    "\n",
    "\t\t#print(img)\n",
    "\t\t#class_id = self.class_map[class_name]\n",
    "\t\timg_tensor = torch.from_numpy(img)\n",
    "\t\timg_tensor = img_tensor.permute(2, 0, 1)\n",
    "\t\timg_label = torch.tensor([int(self.y[idx])])\n",
    "\t\timg_label = img_label.to(torch.float32)\n",
    "\t\treturn img_tensor, img_label\n",
    "\t\t#return img_tensor, class_id\n",
    "\n",
    "\n",
    "class CustomDatasetKN(Dataset): # building through Kemal's DL - find where i am going wrong\n",
    "\tdef __init__(self, x, y, col_dict, device):\n",
    "\t\tself.col_dict = col_dict\n",
    "\t\tself.device= device\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\tself.i= 0\n",
    "\t\t#self.data.append([x, y])\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.y)\n",
    "\t\n",
    "\tdef tensoring(self, img):\n",
    "\t\ttense = torch.tensor(img, dtype=torch.float32)\n",
    "\t\ttense = F.normalize(tense)\n",
    "\t\ttense = tense.permute(2, 0, 1)\n",
    "\t\treturn tense\n",
    "\n",
    "\tdef to_tensor(self, img):\n",
    "\t\tim_chan = img.shape[2]\n",
    "\t\timgY, imgX = img.shape[0], img.shape[1]\n",
    "\t\ttensor = self.tensoring(img)\n",
    "\t\ttensor = tensor.reshape(im_chan, imgY, imgX)\n",
    "\t\ttensor = tensor.to(self.device)\n",
    "\t\treturn tensor\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_path = self.x[idx]\n",
    "\t\t\n",
    "\t\tsize = self.col_dict['size']\n",
    "\t\tself.i+=1\n",
    "\t\t#print(self.i)\n",
    "\n",
    "\t\timg = cv2.imread(img_path)\n",
    "\t\timg = cv2.resize(img, (size[0], size[1]))\n",
    "\n",
    "\n",
    "\t\ttense = self.to_tensor(img)\n",
    "\n",
    "\t\tlabel = label_oh_tf(self.y[idx], 11)\n",
    "\n",
    "\t\treturn tense, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IDSWDataSetLoader(Dataset):\n",
    "    def __init__(self, x, y, col_dict, device): # transform =True\n",
    "        super(Dataset, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.col_dict = col_dict\n",
    "        \n",
    "        self.img_path = x\n",
    "        self.labels = y\n",
    "    \n",
    "        self.class_map = {\"1\":0,\"2\": 1,\n",
    "                            \"3\":2, \"4\":3,\n",
    "                            \"5\":4, \"6\": 5,\n",
    "                            \"7\":6, \"8\":7,\n",
    "                            \"9\":8, \"10\": 9,\n",
    "                            \"11\":10}\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        # length of dataset\n",
    "        return len(self.img_path)\n",
    "    \n",
    "    def tensoring(self, img):\n",
    "        tense = torch.tensor(img, dtype=torch.float32)\n",
    "        tense = F.normalize(tense)\n",
    "        tense = tense.permute(2, 0, 1)\n",
    "        return tense\n",
    "    \n",
    "    def to_tensor(self, img):\n",
    "        im_chan = img.shape[2]\n",
    "        imgY, imgX = img.shape[0], img.shape[1]\n",
    "        tensor = self.tensoring(img)\n",
    "        tensor = tensor.reshape(im_chan, imgY, imgX)\n",
    "        tensor = tensor.to(self.device)\n",
    "        return tensor\n",
    "    \n",
    "    def __getitem__(self, idx,transform=False):\n",
    "        size= self.col_dict['size']\n",
    "        pad = self.col_dict['padding']\n",
    "        img = cv2.imread(self.img_path[idx])\n",
    "        #print(\"img in \",img.shape)\n",
    "\t\t\n",
    "\t\t\n",
    "        self.transform = transform\n",
    "        im_chan = img.shape[2]\n",
    "        if size:\n",
    "            img = cv2.resize(img, (size[0], size[1]))\n",
    "            h = size[1]\n",
    "            w = size[0]\n",
    "        else:\n",
    "            h = img[0]\n",
    "            w= img[1]\n",
    "\t\t\n",
    "        img = img/255 #norm/\n",
    "        #print(\"img sized\", img.shape)\n",
    "        #plt.imshow(img)\n",
    "        #plt.show()\n",
    "        tense = self.to_tensor(img)\n",
    "        #print(\"post tense\",tense.shape)\n",
    "        \n",
    "        label = label_oh_tf(self.labels[idx], 11)\n",
    "\t\t\n",
    "        return tense, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP = ImageProcessor(device)\n",
    "i=1\n",
    "def batch_loop(model, loader, epoch, loss_fn, device, col_dict, num_classes, pad_size =5, optimizer =None, scheduler= None,train =True):\t# Train and Val loops. Default is train\n",
    "    \n",
    "    if train:\n",
    "        model.train()\n",
    "        #lr_ls = []\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    predict_list = []\n",
    "    label_list = []\n",
    "    total_count = 0\n",
    "    num_correct = 0\n",
    "    current_loss = 0\n",
    "    colour = col_dict['colour']\n",
    "    size = col_dict['size']\n",
    "    pad = col_dict['padding']\n",
    "    \n",
    "    #x_batch= x_batch.to(device)\n",
    "    #for idx, img in enumerate(X):\n",
    "    for x_batch, y_batch in loader:\n",
    "        #if i==1:\n",
    "        #    print(\"d loop1 \",x_batch.shape)\n",
    "        #    IP.view(x_batch[0], 5)\n",
    "        #    i+=5\n",
    "        #print(\"loop2\",x_batch[0].shape)\n",
    "        #print('x size',x_batch.shape[0])\n",
    "        #print('y size',y_batch.shape[0])\n",
    "        x_batch= x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_batch =y_batch.argmax()\n",
    "\n",
    "\n",
    "        prediction = model.forward(x_batch) # tense\n",
    "        \n",
    "        loss = loss_fn(prediction, y_batch)\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "        predict_list.append(prediction.argmax().to('cpu'))\n",
    "        label_list.append(y_batch.to('cpu'))#(label.to('cpu'))\n",
    "\n",
    "    #print(current_loss)\n",
    "    if train:\n",
    "        return current_loss, predict_list, label_list, model, optimizer #, lr_ls\n",
    "    else:\n",
    "        return current_loss, predict_list, label_list\n",
    "\n",
    "def test_loop_batch(model, loader, loss_fn, device, col_dict,title, num_classes):\n",
    "    model = model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    colour = col_dict['colour']\n",
    "    size = col_dict['size']\n",
    "    c = 0\n",
    "    acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            prediction = model.forward(x_batch)\n",
    "            all_preds.append(prediction)\n",
    "            all_labels.append(y_batch)\n",
    "\n",
    "    \n",
    "    for i in range(len(all_labels)):\n",
    "        if all_preds[i].argmax() == all_labels[i].argmax():\n",
    "            c +=1\n",
    "    acc = c/len(all_labels)\n",
    "    print(\"Model accuracy: %.2f%%\" % (acc*100))\n",
    "    \n",
    "    print('TEST ACCURACY: ',acc)\n",
    "    return all_preds , all_labels, acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '10' '2' '3' '4' '5' '6' '7' '8' '9']\n",
      "len x train 1681\n",
      "len x val 721\n",
      "['0' '1' '10' '2' '3' '4' '5' '6' '7' '8' '9']\n",
      "len x test 1030\n",
      "['0' '1' '10' '2' '3' '4' '5' '6' '7' '8' '9']\n"
     ]
    }
   ],
   "source": [
    "file_path = r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/'\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_data(file_path, seed=8)\n",
    "print(np.sort(np.unique(y_train)))\n",
    "print('len x train', len(x_train))\n",
    "print('len x val', len(x_val))\n",
    "print(np.sort(np.unique(y_val)))\n",
    "print('len x test', len(x_test))\n",
    "print(np.sort(np.unique(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/its/home/nn268/antvis/antvis/optics/wandb/run-20240214_101210-hwayncmh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/hwayncmh' target=\"_blank\">evanescent-caress-270</a></strong> to <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics' target=\"_blank\">https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/hwayncmh' target=\"_blank\">https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/hwayncmh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%| | 0/120 [00:00<?,/its/home/nn268/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "100%|█| 120/120 [06:39<\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.011 MB uploaded\\r'), FloatProgress(value=0.558481592390347, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>▅▇▆▅▁▆▄▇▅▆▆▄█▆▆▄▃▅▅▆▆▃▅▃█▄▃▄▃▅▅▄▄▅▃▆▆▆▇▆</td></tr><tr><td>avg_val_loss</td><td>▃▄▃▃▆▅▄▇▆▂▄▅▅▅▅▃▅▇▄▂▃▆▃▃▆▁▂▄▅▆▆▄▅▃▃▄▄█▅▅</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▅▇▆▅▁▆▄▇▅▆▆▄█▆▆▄▃▅▅▆▆▃▅▃█▄▃▄▃▅▅▄▄▅▃▆▆▆▇▆</td></tr><tr><td>val_loss</td><td>▃▄▃▃▆▅▄▇▆▂▄▅▅▅▅▃▅▇▄▂▃▆▃▃▆▁▂▄▅▆▆▄▅▃▃▄▄█▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.1503</td></tr><tr><td>avg_val_loss</td><td>0.14968</td></tr><tr><td>epoch</td><td>119</td></tr><tr><td>train_loss</td><td>252.65197</td></tr><tr><td>val_loss</td><td>107.91661</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">evanescent-caress-270</strong> at: <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/hwayncmh' target=\"_blank\">https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/hwayncmh</a><br/> View job at <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzOTQ0Nzc0OQ==/version_details/v3' target=\"_blank\">https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzOTQ0Nzc0OQ==/version_details/v3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240214_101210-hwayncmh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 12.50%\n",
      "TEST ACCURACY:  0.125\n"
     ]
    }
   ],
   "source": [
    "from fns4wandb import pipeline, choose_model\n",
    "import pickle\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from fns4wandb import train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_DL(device,col_dict, save_dict, config=None):\n",
    "    # lists for save dict\n",
    "    t_loss_list = []\n",
    "    v_loss_list =[]\n",
    "    t_predict_list = []\n",
    "    t_label_list = []\n",
    "    v_predict_list = []\n",
    "    v_label_list = []\n",
    "    t_accuracy_list= []\n",
    "    v_accuracy_list= []\n",
    "    \n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # import data\n",
    "        file_path = r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/'\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test = get_data(file_path, seed=8)\n",
    "        #print(np.sort(np.unique(y_train)))\n",
    "        #print('len x train', len(x_train))\n",
    "        #print('len x val', len(x_val))\n",
    "        #print(np.sort(np.unique(y_val)))\n",
    "        #print('len x test', len(x_test))\n",
    "        #print(np.sort(np.unique(y_test)))\n",
    "        \n",
    "        #train_loader_ = IDSWDataSetLoader(x_train, y_train, col_dict=col_dict, device=device)\n",
    "        train_loader_ = IDSWDataSetLoader(x_train, y_train, col_dict=col_dict, device=device)\n",
    "        #train_loader_ = CustomDataset(x_train, y_train, col_dict=col_dict, device=device)\n",
    "        #print(len(train_loader_), train_loader_[0], len(train_loader_[0][0]))\n",
    "        #print(\"trainDL \",train_loader_[0][0].shape)\n",
    "        train_loader = DataLoader(train_loader_, shuffle=True, batch_size=16, drop_last =True)\n",
    "\n",
    "        #val_loader_ = IDSWDataSetLoader(x_val, y_val, col_dict=col_dict, device=device)\n",
    "        val_loader_=  IDSWDataSetLoader(x_val, y_val, col_dict=col_dict, device=device)\n",
    "        #val_loader_=  CustomDataset(x_val, y_val, col_dict=col_dict, device=device)\n",
    "        val_loader = DataLoader(val_loader_, shuffle=True, batch_size=16, drop_last =True)\n",
    "\n",
    "\n",
    "        #test_loader_ = IDSWDataSetLoader(x_test, y_test, col_dict=col_dict, device=device)\n",
    "        test_loader_ = IDSWDataSetLoader(x_test, y_test, col_dict=col_dict, device=device)\n",
    "        #test_loader_ = CustomDataset(x_test, y_test, col_dict=col_dict, device=device)\n",
    "        test_loader = DataLoader(test_loader_, shuffle=True, batch_size=16, drop_last =True)\n",
    "\n",
    "\n",
    "        #x_train, y_train, x_test, y_test, x_val, y_val = IDSWDataSetLoader(col_dict, device)\n",
    "        #train_data_loader = DataLoader([x_train, y_train], batch_size=4, shuffle=True)\n",
    "        #test_data_loader = DataLoader([x_test, y_test], batch_size=4, shuffle=True)\n",
    "        #val_data_loader = DataLoader([x_val, y_val], batch_size=4, shuffle=True)\n",
    "        #model =smallnet3(in_chan=3, f_lin_lay=67968, l_lin_lay=11, ks=(3,5)).to(device) #10368\n",
    "        model = smallnet3(in_chan=config.channels, f_lin_lay=config.first_lin_lay, l_lin_lay=config.num_classes, ks=config.ks).to(device)#choose_model(config).to(device)\n",
    "        \n",
    "        if config.loss_fn == 'MSE':\n",
    "            loss_fn = nn.MSELoss()\n",
    "        elif config.loss_fn == 'CrossEntropy':\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        e_count = 0\n",
    "         # *\n",
    "\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config.scheduler, last_epoch=-1)\n",
    "\n",
    "        for epoch in tqdm(range(config.epochs)):\n",
    "            # current_loss, predict_list, num_correct, label_list, model, optimizer\n",
    "            #print(f\"training...{epoch}\")\n",
    "            t_loss, t_predict_list_, t_label_list_, model, optimizer = batch_loop(model, train_loader, epoch, loss_fn, device, col_dict, num_classes=11, optimizer=optimizer, scheduler=scheduler)\n",
    "            t_loss_list.append(t_loss)\n",
    "            t_predict_list.append(t_predict_list_)\n",
    "            t_label_list.append(t_label_list_)\n",
    "            \n",
    "            #print(f\"validating...{epoch}\")\n",
    "            v_loss, v_predict_list_, v_label_list_= batch_loop(model, val_loader, epoch, loss_fn, device,col_dict,num_classes=11, train=False)\n",
    "            v_loss_list.append(v_loss)\n",
    "            v_predict_list.append(v_predict_list_)\n",
    "            v_label_list.append(v_label_list_)\n",
    "\n",
    "\n",
    "            t_avg_loss =t_loss/len(x_train)\n",
    "            v_avg_loss = v_loss /len(x_val)\n",
    "\n",
    "            e_count +=1\n",
    "            # logging\n",
    "            wandb.log({'avg_train_loss': t_avg_loss, 'epoch':epoch})\n",
    "            wandb.log({'avg_val_loss': v_avg_loss, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'train_loss': t_loss, 'epoch':epoch})\n",
    "            wandb.log({'val_loss': v_loss, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'t_labels': t_label_list, 'epoch':epoch})\n",
    "            wandb.log({'v_labels': v_label_list, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'t_predictions': t_predict_list, 'epoch':epoch})\n",
    "            wandb.log({'v_predictions': v_predict_list, 'epoch':epoch})\n",
    "\n",
    "            # add lists to save dict after all epochs run\n",
    "    save_dict['Current_Epoch'] = config['epochs']\n",
    "    save_dict['training_samples'] = len(x_train)# should this be the whole list for future graphs...?\n",
    "    save_dict['validation_samples'] = len(x_val)\n",
    "    save_dict['t_loss_list'] = t_loss_list #[c.to('cpu') for c in t_loss_list]\n",
    "    save_dict['t_predict_list'] = [[c.to('cpu') for c in k]for k in t_predict_list] #[[c.to('cpu') for c in k]for k in t_predict_list]  # [c.to('cpu') for c in t_predict_list] \n",
    "    save_dict['t_accuracy_list'] = t_accuracy_list #\n",
    "    save_dict['v_loss_list'] = v_loss_list #[c.to('cpu') for c in v_loss_list]\n",
    "    save_dict['v_predict_list'] = [[c.to('cpu') for c in k]for k in v_predict_list]#[[c.to('cpu') for c in k]for k in v_predict_list] # [c.to('cpu') for c in v_predict_list]\n",
    "    save_dict['v_accuracy_list'] = v_accuracy_list #\n",
    "    save_dict['t_labels'] = t_label_list #[[c.to('cpu') for c in k]for k in t_label_list]\n",
    "    save_dict['v_labels'] = v_label_list #[[c.to('cpu') for c in k] for k in v_label_list]\n",
    "    \n",
    "    title = save_dict['Run']\n",
    "    test_predictions, test_y, test_accuracy = test_loop_batch(model, test_loader, loss_fn, device, col_dict, title, config.num_classes)\n",
    "    save_dict['test_predictions']= [c.to('cpu') for c in test_predictions]\n",
    "    save_dict['test_labels'] = test_y\n",
    "    save_dict['test_acc'] = test_accuracy\n",
    "\n",
    "    \n",
    "    with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", 'wb+') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "        \n",
    "    return model\n",
    "\n",
    "#wandb.agent(sweep_id, tr, count=25)\n",
    "# config, col_dict,save_dict, title, device, seed)\n",
    "\n",
    "\n",
    "\n",
    "#model = pipeline(config, col_dict,save_dict, title=\"2c2l_training_113x36, 3chan\", device=device, seed=seed)\n",
    "#model = train(device,col_dict, save_dict, config)\n",
    "model = train_DL(device,col_dict, save_dict, config)\n",
    "#pipeline(config, col_dict, save_dict, title, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr\n",
      "1\n",
      "41\n",
      "va\n",
      "1\n",
      "7\n",
      "te\n",
      "1\n",
      "2\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#print(1681/4)\n",
    "\n",
    "\"\"\"len x train 1681\n",
    "len x val 721\n",
    "len x test 1030\"\"\"\n",
    "def find_num(num):\n",
    "    for i in range(1,100):\n",
    "        if num>0 and num%i==0:\n",
    "            print(i)\n",
    "print('tr')\n",
    "find_num(1681)\n",
    "print('va')\n",
    "find_num(721)\n",
    "print('te')\n",
    "find_num(1030)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "03\n"
     ]
    }
   ],
   "source": [
    "n = '/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/IDSW003_060423_1133_SW_0029.JPG_Augmented_left_1.JPG'\n",
    "print(len(n))\n",
    "print(n[59:61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9944547134935307"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2161/541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612912"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.640776699029127\n",
    "452*452*3\n",
    "# 195264\n",
    "# 612912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def tr(config=None):\\n    with wandb.init(config=config):\\n        config = wandb.config\\n        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\\n        model = hp_sweep(config, col_dict, save_dict, device)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def tr(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = hp_sweep(config, col_dict, save_dict, device)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.agent(sweep_id, tr, count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/its/home/nn268/antvis/antvis/optics\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model = pipeline(config, col_dict, title=\"2c2l_training_113x36, 3chan\", image_file_path= \"/its/home/nn268/optics/AugmentedDS_IDSW/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

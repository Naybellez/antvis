{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwrapped hyperperam sweep\n",
    "\n",
    "# dwnload images one at a time\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '060923'\n",
    "save_location = r'saves/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "#import wget\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from torch.nn import functional\n",
    "from zipfile import ZipFile\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "#import umap.umap_ as umap\n",
    "import seaborn as sns\n",
    "\n",
    "#from boxsdk import OAuth2, Client\n",
    "from boxsdk import OAuth2, Client\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# set up the authication \n",
    "def box_auth_setup(access_token, start=False):\n",
    "    if start:\n",
    "        oauth =OAuth2(\n",
    "            client_id= 'hw534w4beg3mscd8v265vpkk8ndzc5y1',\n",
    "            client_secret = 'nmL4fcHjb2drntVJxGSqQvjt19t0hIlu',\n",
    "\n",
    "            access_token= access_token # changes\n",
    "            )\n",
    "\n",
    "        client = Client(oauth)\n",
    "        user = client.user().get()\n",
    "        print('Current User: ', user.id)\n",
    "\n",
    "        return client\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_box_items(folderID,client):\n",
    "    folder = client.folder(folder_id= folderID).get()\n",
    "    print(f'Folder \"{folder.name}\" has {folder.item_collection[\"total_count\"]} items in it')\n",
    "    items = client.folder(folder_id=folderID).get_items()\n",
    "\n",
    "    return items\n",
    "\n",
    "client = box_auth_setup('q8HIDJ1X1IqROOerwlzkrj7ypZqhDlje', start=True)\n",
    "#print(type(client))\n",
    "items = get_box_items('205117553143',client)\n",
    "\n",
    "def download_files(dataFolder, client, save_location):\n",
    "      for idx, item in enumerate(dataFolder):\n",
    "          save_location = save_location\n",
    "          item_content = client.file(item.id).get()\n",
    "          if item.name not in save_location:\n",
    "            with open(os.path.join(save_location,item.name), 'wb') as open_file:\n",
    "              item_content.download_to(open_file)\n",
    "              open_file.close()\n",
    "\"\"\"\n",
    "\"\"\"items = get_box_items('205117553143',client)\n",
    "download_files(items,client, '/its/home/nn268/optics/images')\"\"\"\n",
    "\"\"\"# download one file #dl_box_item() missing 1 required positional argument: 'item_id'\n",
    "def dl_box_item(item_id):\n",
    "    url = 'https://sussex.app.box.com/file/'\n",
    "    response = requests.get(url+item_id)\n",
    "    file_bytes = response.content\n",
    "    #image = Image.open(io.BytesIO(file_bytes)).convert('RGB')\n",
    "    return image\"\"\"\n",
    "\n",
    "\"\"\"item_name_list = []\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for item in items:\n",
    "    item_name_list.append(item.name)\n",
    "    images.append(item.id)\n",
    "\n",
    "for name in item_name_list: #labels\n",
    "    if name[0:4] == 'IDSW':\n",
    "        i = int(name[5:7]) -1 # -1 is to make the predictions later readable\n",
    "        i = str(i)\n",
    "        labels.append(i)\"\"\"\n",
    "\n",
    "\"\"\"def image_label_split(items):\n",
    "    item_name_list = []\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in items:\n",
    "        item_name_list.append(item.name)\n",
    "        images.append(item.id)\n",
    "\n",
    "    for name in item_name_list: #labels\n",
    "        if name[0:4] == 'IDSW':\n",
    "            i = int(name[5:7]) -1 # -1 is to make the predictions later readable\n",
    "            i = str(i)\n",
    "            labels.append(i)\n",
    "            \n",
    "    return images, labels\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "file_path = r'/its/home/nn268/optics/images/'\n",
    "\n",
    "for file in os.listdir(file_path):\n",
    "    if file[0:4] == 'IDSW':\n",
    "        i=int(file[5:7]) -1\n",
    "        i = str(i)\n",
    "        labels.append(i)\n",
    "\n",
    "for i in os.listdir(file_path):\n",
    "    if i[0:4] == 'IDSW':\n",
    "        j=file_path+i\n",
    "        images.append(j)\n",
    "\n",
    "label_arr =np.array(labels)\n",
    "image_arr = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036\n",
      "1036\n"
     ]
    }
   ],
   "source": [
    "print(len(images))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images, labels = image_label_split(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036 1036\n",
      "['3', '3', '2', '8', '1', '3', '3', '2', '0', '8']\n"
     ]
    }
   ],
   "source": [
    "print(len(images), len(labels))\n",
    "print(labels[:10])\n",
    "#print(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.3,\n",
    "                                                    random_state=42) #, stratify=np.array(label_arr)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
    "                                                  test_size = 0.1, train_size=0.9,\n",
    "                                      random_state=random_seed, shuffle = True) #,stratify=np.array(label_arr)[y_train]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "def Unwrap(imgIn):\n",
    "\n",
    "    def buildMap(Wd, Hd, R, Cx, Cy):\n",
    "        ys=np.arange(0,int(Hd))\n",
    "        xs=np.arange(0,int(Wd))\n",
    "\n",
    "        rs=np.zeros((len(xs),len(ys)))\n",
    "        rs=R*ys/Hd\n",
    "\n",
    "        thetas=np.expand_dims(((xs-offset)/Wd)*2*np.pi,1)\n",
    "\n",
    "        map_x=np.transpose(Cx+(rs)*np.sin(thetas)).astype(np.float32)\n",
    "        map_y=np.transpose(Cy+(rs)*np.cos(thetas)).astype(np.float32)\n",
    "        return map_x, map_y\n",
    "\n",
    "    #UNWARP\n",
    "    def Unwrap_(_img, xmap, ymap):\n",
    "        output = cv.remap(_img, xmap, ymap, cv.INTER_LINEAR)\n",
    "        return output\n",
    "\n",
    "\n",
    "    img=cv.resize(imgIn,None,fx=0.1,fy=0.1,interpolation=cv.INTER_LINEAR)\n",
    "\n",
    "    if img.shape[1] != img.shape[0]:\n",
    "        cropBlock=int((int(img.shape[1])-int(img.shape[0]))/2)\n",
    "        img=img[:,cropBlock:-cropBlock]\n",
    "\n",
    "    #distance to the centre of the image\n",
    "    offset=int(img.shape[0]/2)\n",
    "\n",
    "    #IMAGE CENTER\n",
    "    Cx = img.shape[0]/2\n",
    "    Cy = img.shape[1]/2\n",
    "\n",
    "    #RADIUS OUTER\n",
    "    R =- Cx\n",
    "\n",
    "    #DESTINATION IMAGE SIZE\n",
    "    Wd = int(abs(2.0 * (R / 2) * np.pi))\n",
    "    Hd = int(abs(R))\n",
    "\n",
    "    #BUILD MAP\n",
    "    xmap, ymap = buildMap(Wd, Hd, R, Cx, Cy)\n",
    "\n",
    "    #UNWARP\n",
    "    result = Unwrap_(img, xmap, ymap)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function removes red channel\n",
    "def zero_red(file_path, data_split=x_train):\n",
    "  if isinstance(file_path, str):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
    "    img[:,:,2] = np.zeros([img.shape[0], img.shape[1]])\n",
    "  elif isinstance(file_path, int):\n",
    "    img = cv2.imread(data_split[file_path], cv2.IMREAD_UNCHANGED)\n",
    "    img[:,:,2] = np.zeros([img.shape[0], img.shape[1]])\n",
    "\n",
    "  return img\n",
    "\n",
    "def open_process_image(file_path):\n",
    "  img = zero_red(file_path) # remove red channel\n",
    "  img = Unwrap(img) # unwrap image. make rectangle\n",
    "  return img\n",
    "\n",
    "def tensoring(file_path):\n",
    "  img = open_process_image(file_path)\n",
    "  img_tensor = torch.tensor(img) # turn to tensor\n",
    "  img_tensor = img_tensor.to(torch.float32) # tensor of float32s\n",
    "  img_tensor = functional.normalize(img_tensor)\n",
    "  img_tensor = img_tensor.permute(2, 0, 1)    # ensure channels are correct\n",
    "  img_tensor = img_tensor.reshape(1, 3, 144, 452) # batch, colour, y, x\n",
    "  img_tensor = img_tensor[:,0:2,:,:]\n",
    "  img_tensor = img_tensor.to(device)\n",
    "\n",
    "  return img_tensor\n",
    "\n",
    "def label_oh_tf(lab):\n",
    "    num_classes = 11\n",
    "    one_hot = np.zeros(num_classes)\n",
    "    lab = int(lab)\n",
    "    one_hot[lab] = 1\n",
    "    label = torch.tensor(one_hot)\n",
    "    label = label.to(torch.float32)\n",
    "    label = label.to(device) #\n",
    "    return label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - copy of what worked on MNIST\n",
    "class vgg16TorchNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg16TorchNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(  # 1, 2, 144, 452\n",
    "              nn.Conv2d(in_channels=2, out_channels=32, kernel_size=3, padding=2),\n",
    "              nn.ReLU(), #inplace=True\n",
    "              nn.Dropout(p=0.5),\n",
    "              nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=2),\n",
    "              nn.ReLU(), #inplace=True\n",
    "              nn.Conv2d(in_channels =64, out_channels=64, kernel_size=3),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(2, 2),\n",
    "              nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=2),\n",
    "              nn.ReLU(), #inplace=True\n",
    "              nn.Conv2d(in_channels =128, out_channels=128, kernel_size=3),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(2,2),\n",
    "              nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=2),\n",
    "              nn.ReLU(), #inplace=True\n",
    "              nn.Conv2d(in_channels =256, out_channels=256, kernel_size=3),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d(2,2),\n",
    "              nn.Dropout(p=0.5), # (1x258048 and 16384x100)\n",
    "          )\n",
    "\n",
    "        self.linear_1 = nn.Sequential(    #1x16384 and 4096x100)\n",
    "            nn.Linear(258048, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(100,11),\n",
    "            nn.Softmax(),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "      #forward method. opposition to backward pass\n",
    "      #print(x.shape)\n",
    "      x= self.conv_layers(x)\n",
    "      x = x.flatten()\n",
    "      x = x.squeeze()\n",
    "      #print('conv x', x.shape)\n",
    "      x = self.linear_1(x)\n",
    "      #print('lin1 x', x)\n",
    "      return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "def train_loop(model, x_train, y_train, epoch, optimizer, loss_fn):\n",
    "  model = model\n",
    "  x_train = x_train\n",
    "  y_train = y_train\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  predict_list = []\n",
    "  total_count = 0\n",
    "  num_correct = 0\n",
    "  current_loss = 0\n",
    "\n",
    "  total_samples = len(x_train)\n",
    "\n",
    "  for idx, img in enumerate(x_train):\n",
    "      tense = tensoring(img)\n",
    "      #print(tense.shape)\n",
    "\n",
    "      \"\"\"\n",
    "      img = np.array(tense.to('cpu').squeeze()[i])\n",
    "      print(img.shape)\n",
    "      plt.subplot(330 + 1)\n",
    "      plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "      plt.show()\n",
    "      \"\"\"\n",
    "\n",
    "      prediction = model.forward(tense)\n",
    "      label = label_oh_tf(y_train[idx])\n",
    "\n",
    "      loss = loss_fn(prediction, label)\n",
    "      predict_list.append(prediction.argmax())\n",
    "\n",
    "      #print('\\n ---------------------------------------------------------------')\n",
    "      #print('             Epoch: ', epoch, '  Sample: ', idx)\n",
    "\n",
    "      if prediction.argmax() == label.argmax():\n",
    "          print(f'\\n ########################### HIT ###########################  -- {idx} / {total_samples} \\n')\n",
    "          num_correct +=1\n",
    "      else:\n",
    "        #print('\\n ########################### MISS ########################### \\n')\n",
    "        pass\n",
    "\n",
    "      total_count+=1\n",
    "\n",
    "      #print(prediction, '\\n Prediction:  ', prediction.argmax())\n",
    "      #print('Label: ',label.argmax())\n",
    "      #print('Loss: ', loss.item())\n",
    "      #print('---------------------------------------------------------------')\n",
    "      #print(\" |||| ||||| ||||| ||||| ||||| ||||| |||| |||| ||||| |||| |||| \")\n",
    "\n",
    "      current_loss += loss.item()\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "\n",
    "  return current_loss, predict_list, num_correct, model, optimizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation loop\n",
    "\n",
    "def validation_loop(model, x_val, y_val, epoch, loss_fn):\n",
    "  model = model\n",
    "  x_val = x_val\n",
    "  y_val = y_val\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  predict_list = []\n",
    "  total_count = 0\n",
    "  num_correct = 0\n",
    "  current_loss = 0\n",
    "\n",
    "  for idx, img in enumerate(x_val):\n",
    "      tense_img = tensoring(img)\n",
    "      prediction = model.forward(tense_img)\n",
    "      label = label_oh_tf(y_val[idx])\n",
    "\n",
    "      loss = loss_fn(prediction, label)\n",
    "      predict_list.append(prediction.argmax())\n",
    "\n",
    "      #print('\\n ---------------------------------------------------------------')\n",
    "      #print('             Epoch: ', epoch, '  Sample: ', idx)\n",
    "\n",
    "      if prediction.argmax() == label.argmax():\n",
    "          #print('\\n ########################### HIT ########################### \\n')\n",
    "          num_correct +=1\n",
    "      else:\n",
    "        #print('\\n ########################### MISS ########################### \\n')\n",
    "        pass\n",
    "      total_count+=1\n",
    "\n",
    "      #print('ArgPrediction: ', prediction.argmax()) #, prediction,'ARRRGGGG',\n",
    "      #print('Label: ',label.argmax())\n",
    "      #print('Loss: ', loss.item())\n",
    "      #print('---------------------------------------------------------------')\n",
    "      #print(\" |||| ||||| ||||| ||||| ||||| ||||| |||| |||| ||||| |||| |||| \")\n",
    "\n",
    "      current_loss += loss.item()\n",
    "\n",
    "  return current_loss, predict_list, num_correct\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_loss_list = []\n",
    "t_predict_list =[]\n",
    "t_accuracy_list = []\n",
    "\n",
    "v_loss_list = []\n",
    "v_predict_list =[]\n",
    "v_accuracy_list = []\n",
    "\n",
    "total_epochs = 0\n",
    "\n",
    "title = f'IDSW_GB_'\n",
    "save_dict = {'Run' : title,\n",
    "            'Current_Epoch': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:  0.001 optim:  Adam loss fn:  MSELoss()\n",
      "EPOCH:  10\n",
      "----------------------\n",
      " \n",
      "                  TRAINING... \n",
      "\n",
      "----------------------\n",
      "\n",
      " ########################### HIT ###########################  -- 37 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 46 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 48 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 56 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 69 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 70 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 88 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 91 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 96 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 101 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 107 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 118 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 122 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 127 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 138 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 145 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 152 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 161 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 186 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 187 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 198 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 199 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 202 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 221 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 223 / 652 \n",
      "\n",
      "\n",
      " ########################### HIT ###########################  -- 224 / 652 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter sweep\n",
    "\n",
    "##### remember to make the pickle folder before running!\n",
    "##### adapt train loopppp\n",
    "\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "full_label = y_train\n",
    "\n",
    "def hyperparameter_sweep():\n",
    "    # a function to loop through hyperparameters for finding the best ones for certain architecture\n",
    "\n",
    "    epochs = 50\n",
    "\n",
    "    lr_list = [1e-3,1e-2, 5e-6,5e-5, 5e-4, 5e-3] #1e-7, 1e-6, #1e-7,1e-6,1e-5,1e-4,1e-3,1e-2, 5e-4, 5e-3, 5e-2 #1e-6, 5e-3, 5e-4,\n",
    "    optimiser_list = ['Adam', 'SGD'] #, 'SGD'\n",
    "    lossfn_list = [torch.nn.MSELoss()] #, torch.nn.NLLLoss(), ,\n",
    "\n",
    "    best_optim = None\n",
    "    best_lossfn = None\n",
    "    best_lr = 0\n",
    "    best_valaccuracy = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for loss_fn in lossfn_list:\n",
    "        for optim in optimiser_list:\n",
    "            for learning_rate in lr_list:\n",
    "\n",
    "\n",
    "\n",
    "                model = vgg16TorchNet().to(device) #model architecture\n",
    "                optim_list=[]\n",
    "                if optim =='Adam':\n",
    "                    optimizer1 = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                    optimizer2 = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "                    optim_list.append(optimizer1)\n",
    "                    optim_list.append(optimizer2)\n",
    "                elif optim == 'SGD':\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                for optimizer in optim_list:\n",
    "                    t_loss_list = []\n",
    "                    t_predict_list =[]\n",
    "                    t_accuracy_list = []\n",
    "\n",
    "                    v_loss_list = []\n",
    "                    v_predict_list =[]\n",
    "                    v_accuracy_list = []\n",
    "\n",
    "                    total_epochs = 0\n",
    "                    title = f'forest_HyperParameterSweep_{str(learning_rate)}_{str(optimizer)}_{str(loss_fn)}lecun'\n",
    "                    save_dict = {'Run' : title,\n",
    "                                'Current_Epoch': 0}\n",
    "\n",
    "                    print('\\n')\n",
    "                    print('LR: ', learning_rate)\n",
    "                    print('optimiser: ', optim)\n",
    "                    print('loss fn: ', loss_fn)\n",
    "\n",
    "                    for epoch in range(epochs):\n",
    "                      print('lr: ',learning_rate, 'optim: ',optim, 'loss fn: ',loss_fn)\n",
    "                      print('EPOCH: ', epoch)\n",
    "                      print('----------------------')\n",
    "                      print(' \\n                  TRAINING... \\n')\n",
    "                      print('----------------------')\n",
    "                      train_loss, train_predict_loss, train_num_correct, model, optimizer = train_loop(model, x_train, y_train, epoch, optimizer, loss_fn)\n",
    "                      t_loss_list.append(train_loss)\n",
    "                      t_predict_list.append(train_predict_loss)\n",
    "                      t_accuracy_list.append(train_num_correct / len(y_train))\n",
    "\n",
    "\n",
    "                      print('----------------------')\n",
    "                      print(' \\n                  VALIDATION... \\n')\n",
    "                      print('----------------------')\n",
    "                      val_loss, val_predict_loss, val_num_correct = validation_loop(model, x_val, y_val, epoch, loss_fn)\n",
    "                      v_loss_list.append(val_loss)\n",
    "                      v_predict_list.append(val_predict_loss)\n",
    "                      v_accuracy_list.append(val_num_correct/ len(y_val))\n",
    "\n",
    "                      if v_accuracy_list[-1] > best_valaccuracy:\n",
    "                            best_valaccuracy = v_accuracy_list[-1]\n",
    "                            best_optim = optimizer\n",
    "                            best_lossfn = loss_fn\n",
    "                            best_lr = learning_rate\n",
    "                            best_epoch = epoch\n",
    "\n",
    "                      total_epochs += 1 ### Total epochs is from a previous save\n",
    "\n",
    "\n",
    "                      save_dict['Current Epoch'] = total_epochs\n",
    "                      save_dict['model.state_dict'] = model.state_dict()\n",
    "                      save_dict['training_samples'] = len(x_train)\n",
    "                      save_dict['validation_samples'] = len(x_val)\n",
    "                      save_dict['t_loss_list'] = t_loss_list\n",
    "                      save_dict['t_predict_list'] = t_predict_list\n",
    "                      save_dict['t_accuracy_list'] = t_accuracy_list\n",
    "                      save_dict['v_loss_list'] = v_loss_list\n",
    "                      save_dict['v_predict_list'] = v_predict_list\n",
    "                      save_dict['v_accuracy_list'] = v_accuracy_list\n",
    "                      #save_dict['epochCount']+=1 Now using current_epoch above\n",
    "\n",
    "                      final_train_acc = round(t_accuracy_list[-1],3)\n",
    "                      final_val_acc = round(v_accuracy_list[-1],3)\n",
    "                      version =f'E{total_epochs}_lr{str(learning_rate)}_{optim}_{str(loss_fn)}_Acc_{final_train_acc}_{final_val_acc}_schholcompREF'\n",
    "\n",
    "                      if epoch==epochs-1:\n",
    "                          with open(save_location+f'f_HPS_{date}_{version}.pkl', 'wb') as f:\n",
    "                            pickle.dump(save_dict, f)\n",
    "\n",
    "\n",
    "\n",
    "                      clear_output()\n",
    "\n",
    "\n",
    "    print('Top results from hyperparameter sweep:')\n",
    "    print()\n",
    "    print(best_optim, best_lossfn, best_lr, best_valaccuracy, best_epoch)\n",
    "    return best_optim, best_lossfn, best_lr, best_valaccuracy, best_epoch, v_loss_list, t_loss_list, v_accuracy_list, t_accuracy_list\n",
    "\n",
    "\n",
    "\n",
    "best_optim, best_lossfn, best_lr, best_valaccuracy, best_epoch,v_loss_list, t_loss_list, v_accuracy_list, t_accuracy_list = hyperparameter_sweep()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

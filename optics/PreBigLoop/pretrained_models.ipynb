{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functions import import_imagedata, label_oh_tf, ImageProcessor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.models import resnet101\n",
    "\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from fns4wandb import train_log, build_optimizer\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from fns4wandb import train_log, build_optimizer\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_vgg16 = vgg16(weights=\"IMAGENET1K_V1\")\n",
    "# print(model_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Initialize model with the best available weights\n",
    "model_vgg16_featlayers = models.vgg16(weights=\"IMAGENET1K_FEATURES\").features#.to(device)\n",
    "model_vgg16_featlayers.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the inference transforms\n",
    "# This does some preprocessing behind the scenes,\n",
    "\n",
    "# 1) Resized of the input to resize_size=[256];\n",
    "# 2) Followed by a central crop of crop_size=[224];\n",
    "\n",
    "# vgg16 and resnet accept input image size of 224Ã—224\n",
    "\n",
    "\n",
    "\n",
    "#print(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MLP, linear classification, function\n",
    "\n",
    "class Three_Lay_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Three_Lay_MLP, self).__init__()\n",
    "        \n",
    "        self.lins = nn.Sequential(\n",
    "            nn.Linear(3584, 100), #1024x7 and 1024x100. 7, 1024\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(100,11),\n",
    "            nn.Linear(11,11),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.lins(x)\n",
    "        return x\n",
    "        \n",
    "#lin    conv output -> 100\n",
    "#relu    ()\n",
    "#lin    100 -> 100\n",
    "#relu    ()\n",
    "#do     ~0.5\n",
    "#lin    100 -> 11\n",
    "#softmax ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(label, prediction): #TypeError: Singleton array tensor(3) cannot be considered a valid collection.\n",
    "    #print('l  ',label) #len(11)\n",
    "    label= np.array(label)\n",
    "    print(type(label)) #'y_pred' parameter of f1_score must be an array-like or a sparse matrix. Got 7 instead.\n",
    "    #label = label.argmax()\n",
    "    #print('l  ',label)\n",
    "    predictions_np = prediction.detach().numpy()\n",
    "    #print('p  ',predictions_np)\n",
    "    predicted_classes = np.argmax(predictions_np, axis=0)\n",
    "    #predicted_classes = prediction.argmax()\n",
    "    avg_f1_score = f1_score(label, predicted_classes, average='macro')\n",
    "    acc = accuracy_score(label, predicted_classes)\n",
    "    \n",
    "    return avg_f1_score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.preprocessing import StandardScaler\\n\\n# creating a function for normalizing the feature vector\\n# using a standard scaling (results in mean=0 and standard deviation=1)\\ndef scale_feats(feat_vec):\\n  # Scaling the features to the same range of values\\n  scaler = StandardScaler()\\n  scaler.fit(feat_vec)\\n  scaled_feat_vec = scaler.transform(feat_vec)\\n  print(\"\\n A peek at the scaled dataset features: \\n\"+str(scaled_feat_vec))\\n\\n  return scaled_feat_vec\\n\\n# normalize the feature vector\\nscaled_feats_28 = scale_feats(feats_28)'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# creating a function for normalizing the feature vector\n",
    "# using a standard scaling (results in mean=0 and standard deviation=1)\n",
    "def scale_feats(feat_vec):\n",
    "  # Scaling the features to the same range of values\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(feat_vec)\n",
    "  scaled_feat_vec = scaler.transform(feat_vec)\n",
    "  print(\"\\n A peek at the scaled dataset features: \\n\"+str(scaled_feat_vec))\n",
    "\n",
    "  return scaled_feat_vec\n",
    "\n",
    "# normalize the feature vector\n",
    "scaled_feats_28 = scale_feats(feats_28)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class AutoFeature(): # file_path, number\\n    def __init__(self, img_files, labels):\\n        self.img_files = img_files\\n        self.label = label_oh_tf(labels, device='cpu', num_classes=11)\\n        \\n        #self.labels=[label_oh_tf(label, device='cpu', num_classes=11) for label in labels]\\n        #self.labels= [int(label) for label in labels]\\n        #self.labels = torch.tensor(labels, dtype=torch.long) # tensofy the labels\\n        #self.labels = label_oh_tf(labels, device, 11)\\n    #def __len__(self):\\n    #    return len(self.labels)\\n    def __getitem__(self):\\n        #given an index, will return items within that index range\\n        # extracting features using pretrained model\\n        feats = get_img_feats(self.img_files) #[idx]\\n        \\n        feats_tensor = torch.tensor(feats, dtype=torch.float32)\\n        #print(type(self.labels))\\n        #print(self.labels[3])\\n        return feats_tensor, self.label #[idx]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temi code. week 6. notebook used vgg16 pretrained features, and trained a MLP for classification of beans\n",
    "# this is the method I am following \n",
    "\n",
    "IP = ImageProcessor(device='cpu')\n",
    "\n",
    "# Temi func. get features from passing through images into vgg16 conv/ feature layers.\n",
    "# creating a function to get features learnt in the pretrained VGG16\n",
    "# extracting these features for our own list of images\n",
    "def get_img_feats(img_path): \n",
    "    #print(img_path)\n",
    "    img = cv2.imread(img_path) #\n",
    "    #print(img)\n",
    "    img = IP.blank_padding(img, (224,224))\n",
    "    img = IP.to_tensor(img).to(device)\n",
    "    #print('image shape post read and pad: \\n ',img.shape)\n",
    "    # Step 3: Apply inference preprocessing transforms\n",
    "    #img = preprocess(img).unsqueeze(0) # preprocess is the model with weights\n",
    "    #img = preprocess(img)#.unsqueeze(0)\n",
    "\n",
    "    # Step 4: Use the model and print the predicted category\n",
    "    auto_feats = model_vgg16_featlayers(img).squeeze(0)\n",
    "    auto_feats = auto_feats.cpu().detach().numpy()\n",
    "    auto_feats = np.mean(auto_feats, axis=1,keepdims=False) #, keepdims=False\n",
    "    #print('get_img_feats autofeats', auto_feats)\n",
    "    #print('auto_feat: \\n ', auto_feats)\n",
    "    auto_feats = torch.tensor(auto_feats, dtype=torch.float32)\n",
    "    return auto_feats.to(device)\n",
    "\n",
    "\n",
    "# class to manage data and turn imgs into vgg16 features\n",
    "\n",
    "\"\"\"class AutoFeature(): # file_path, number\n",
    "    def __init__(self, img_files, labels):\n",
    "        self.img_files = img_files\n",
    "        self.label = label_oh_tf(labels, device='cpu', num_classes=11)\n",
    "        \n",
    "        #self.labels=[label_oh_tf(label, device='cpu', num_classes=11) for label in labels]\n",
    "        #self.labels= [int(label) for label in labels]\n",
    "        #self.labels = torch.tensor(labels, dtype=torch.long) # tensofy the labels\n",
    "        #self.labels = label_oh_tf(labels, device, 11)\n",
    "    #def __len__(self):\n",
    "    #    return len(self.labels)\n",
    "    def __getitem__(self):\n",
    "        #given an index, will return items within that index range\n",
    "        # extracting features using pretrained model\n",
    "        feats = get_img_feats(self.img_files) #[idx]\n",
    "        \n",
    "        feats_tensor = torch.tensor(feats, dtype=torch.float32)\n",
    "        #print(type(self.labels))\n",
    "        #print(self.labels[3])\n",
    "        return feats_tensor, self.label #[idx]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3432\n"
     ]
    }
   ],
   "source": [
    "print(img_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get len of \n",
    "\n",
    "file_path = r'/its/home/nn268/antvis/optics/AugmentedDS_IDSW/'\n",
    "random_seed =1\n",
    "img_len = len(os.listdir(file_path))\n",
    "\n",
    "ids = np.arange(0, img_len)\n",
    "#print(ids[4])\n",
    "\n",
    "\n",
    "train_ids, test_ids = train_test_split(ids, test_size=0.2, train_size=0.8,\n",
    "                                 random_state=random_seed, shuffle=True)\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.1, train_size=0.8,\n",
    "                                 random_state=random_seed, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaughticalnonsence\u001b[0m (\u001b[33mantvis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3374 2376  625 3354]\n",
      "x :  /its/home/nn268/antvis/optics/AugmentedDS_IDSW/IDSW008_170823_1109_SW_361.JPG_Augmented_right_1.JPG\n",
      "y: 7\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# split data intro train, val, test\n",
    "\"\"\"\n",
    "x, y = import_imagedata(file_path)\n",
    "train_set = AutoFeature(x[train_ids], y[train_ids])\n",
    "val_set = AutoFeature(x[val_ids], y[val_ids])\n",
    "test_set = AutoFeature(x[test_ids], y[test_ids])\n",
    "\n",
    "#print('train set 0: \\n',train_set[0])\n",
    "#print('what is train set?', type(train_set))\n",
    "#print('train set len', len(train_set))\n",
    "\n",
    "for i in train_set:\n",
    "    print('i 0 \\n',i[0])\n",
    "    print('i 1 \\n',i[1])\n",
    "    break\n",
    "\n",
    "print('testing forloop')\n",
    "print(type(train_set))\n",
    "x_train = [i[0] for i in train_set]\n",
    "y_train = [i[1] for i in train_set]\n",
    "print('testend')\n",
    "print(x_train[0])\n",
    "print(y_train[0])\"\"\"\n",
    "#x_train, y_train = train_set\n",
    "#x_val, y_val = val_set\n",
    "#x_test, y_test = test_set\n",
    "#train_dl = DataLoader(train_set, batch_size=1)\n",
    "#val_dl = DataLoader(val_set, batch_size=1)\n",
    "#test_dl = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "print(train_ids[:4])\n",
    "idx= 2\n",
    "x, y = import_imagedata(file_path)\n",
    "\n",
    "print('x : ',x[idx])\n",
    "print('y:', y[idx])\n",
    "\n",
    "train = AutoFeature(x[idx], y[idx])\n",
    "j,k = train.__getitem__()\n",
    "print(type(j))\n",
    "print(type(k))\n",
    "#print('train: ',train[:], '\\n')\n",
    "#print('train 1', train, '\\n')\n",
    "#for i in train:\n",
    "#    print(i)\n",
    "#print('train 0',train[:10])\n",
    "#x_train = train[0]\n",
    "#label = train[1]\n",
    "\n",
    "#print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'IDSW_VGG16_MLP_112023'\n",
    "save_dict = {'Run' : title,\n",
    "            'Current_Epoch': 0}\n",
    "                #r'/its/home/nn268/antvis/optics/\n",
    "save_location = r'pickles/'#pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IDSW_VGG16_MLP_112023'"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dict['Run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# split data intro train, val, test\n",
    "\n",
    "# instancing the autofeature class\n",
    "# creating an object for train, val test\n",
    "\n",
    "#train_set = AutoFeature(x[train_ids], y[train_ids])\n",
    "#val_set = AutoFeature(x[val_ids], y[val_ids])\n",
    "#test_set = AutoFeature(x[test_ids], y[test_ids])\n",
    "\n",
    "#print('train set 0: \\n',train_set[0])\n",
    "#print('what is train set?', type(train_set))\n",
    "#print('train set len', len(train_set))\n",
    "\n",
    "\n",
    "\n",
    "#x_test, y_test = test_set\n",
    "#x_test = [i[0] for i in test_set]\n",
    "#y_test = [i[1] for i in test_set]\n",
    "\n",
    "#train_dl = DataLoader(train_set, batch_size=1)\n",
    "#val_dl = DataLoader(val_set, batch_size=1)\n",
    "#test_dl = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = dict(\n",
    "    epochs= 30, #30, \n",
    "    learning_rate =1e-5,\n",
    "    architecture ='CNN',\n",
    "    optimizer= 'adam',\n",
    "    weight_decay= 4e-5,\n",
    "    ks = 3,\n",
    "    scheduler=0.2,\n",
    "    f_lin_lay = 7168 #1024*7 = 7168\n",
    ")\n",
    "\n",
    "# pass in ids\n",
    "# create class instance for single ids\n",
    "# index that class to get img feature and label FOR THAT ids.\n",
    "# per epoch\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, loss_fn, train_ids, val_ids, config, best_acc=0): #train_dl, val_dl, \n",
    "    wandb.watch(model, loss_fn, log='all', log_freq=10)\n",
    "    \n",
    "    lr = config['learning_rate'] #1e-5 #config.learning_rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)#build_optimizer(model, optimizer=torch.optim.Adam(model.parameters(), lr=lr))#config.optimizer, config.learning_rate, config.weight_decay)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config['scheduler'], last_epoch=-1) #gamma=config.scheduler, last_epoch=-1)\n",
    "    \n",
    "    x, y = import_imagedata(file_path)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    #losses= []\n",
    "    #predictions = []\n",
    "    t_loss_list = []\n",
    "    v_loss_list = []\n",
    "    t_predict_list = []\n",
    "    v_predict_list = []\n",
    "    t_accuracy_list = []\n",
    "    v_accuracy_list = []\n",
    "    t_label_list = []\n",
    "    v_label_list = []\n",
    "    #labels = []\n",
    "    t_correct = 0\n",
    "    v_correct = 0\n",
    "    total_epochs = 0\n",
    "    for epoch in tqdm(range(config['epochs'])): #config.epochs)):\n",
    "        print('E   ', epoch)\n",
    "        if epoch == 0:\n",
    "            best_model = deepcopy(model)\n",
    "        #train_ids = random.shuffle(train_ids)\n",
    "        #print(type(train_ids))\n",
    "        print('training...')\n",
    "        for idx in train_ids: # trian_ids is a list of indexs\n",
    "            #print(idx,x[idx])\n",
    "            x_train = get_img_feats(x[idx]) # using an index from train_ids to index x (file_paths). get_img_feats reads img, does preprocessing and sends img into conv layers.\n",
    "            tensor = torch.tensor(x_train, dtype=torch.float32) # turns feature map into tensor\n",
    "            tensor = tensor.flatten() # flattens tensor to 1D\n",
    "\n",
    "            train_prediction = model.forward(tensor)\n",
    "            #print(prediction.shape)\n",
    "            #print(train_prediction.argmax())\n",
    "            train_label = label_oh_tf(y[idx], device=device, num_classes=11) # use same index val to index y (labels) and turn into onehot encoded label\n",
    "            #print(train_label.argmax())\n",
    "            t_loss = loss_fn(train_prediction, train_label)\n",
    "            \n",
    "            #train_avg_f1_score, train_acc = metrics(train_label, train_prediction)\n",
    "            if train_prediction.argmax() == train_label.argmax():\n",
    "                t_correct+=1\n",
    "            t_loss_list.append(t_loss)\n",
    "            t_predict_list.append(train_prediction)\n",
    "            train_acc = (t_correct / len(train_ids))*100\n",
    "            t_accuracy_list.append(train_acc)\n",
    "            \n",
    "            t_label_list.append(train_label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            t_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        print('validating...')\n",
    "        for idx in val_ids:\n",
    "            \n",
    "            x_val = get_img_feats(x[idx])\n",
    "            #print(tens.shape)\n",
    "            tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "            tensor = tensor.flatten()\n",
    "            y_val = label_oh_tf(y[idx], device=device, num_classes=11)\n",
    "            #print(label)\n",
    "            \n",
    "            val_prediction = model.forward(tensor)\n",
    "            #print(val_prediction.shape)\n",
    "            v_loss = loss_fn(val_prediction, y_val)\n",
    "            \n",
    "            v_loss_list.append(v_loss) #v_loss_list.append(val_loss)\n",
    "            # v_predict_list.append(val_predict_loss)\n",
    "            if val_prediction.argmax() == y_val.argmax():\n",
    "                v_correct +=1\n",
    "            v_loss_list.append(v_loss)\n",
    "            v_predict_list.append(val_prediction)\n",
    "            \n",
    "            v_label_list.append(y_val)\n",
    "            val_acc = (v_correct / len(val_ids))*100\n",
    "            v_accuracy_list.append(val_acc)\n",
    "            \n",
    "        total_epochs += epoch\n",
    "            #val_avg_f1_score, val_acc = metrics(y_val, val_prediction)\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = deepcopy(model)\n",
    "            save_dict['Current_Epoch'] += config['epochs']\n",
    "            save_dict['model.state_dict'] = model.state_dict()\n",
    "            save_dict['training_samples'] = len(train_ids)\n",
    "            save_dict['validation_samples'] = len(val_ids)\n",
    "            save_dict['t_loss_list'] = t_loss_list\n",
    "            save_dict['t_predict_list'] = t_predict_list  \n",
    "            save_dict['t_accuracy_list'] = t_accuracy_list  #\n",
    "            save_dict['v_loss_list'] = v_loss_list\n",
    "            save_dict['v_predict_list'] = v_predict_list  #\n",
    "            save_dict['v_accuracy_list'] = v_accuracy_list  #\n",
    "            title = save_dict['Run']\n",
    "            with open(f'{save_location}{title}_acc{int(best_acc)}_epoch{total_epochs}.pkl', 'wb+') as f:\n",
    "                pickle.dump(save_dict, f)\n",
    "            print('improvment in metrics. model saved')\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        if (epoch+1)%2==0:\n",
    "            train_log(t_loss, v_loss, epoch)\n",
    "            wandb.log({'train_accuracy_%': train_acc, 'epoch':epoch})\n",
    "            wandb.log({'val_accuracy_%': val_acc, 'epoch':epoch})\n",
    "    model= best_model\n",
    "    labels = zip(t_label_list, v_label_list)\n",
    "    losses = zip(t_loss_list, v_loss_list)\n",
    "    predictions = zip(t_predict_list, v_predict_list)\n",
    "    return model, losses, predictions, labels\n",
    "\n",
    "\n",
    "\n",
    "def pipeline(config): \n",
    "\n",
    "    title = f\"IDSW_on_Vgg16_Convs_training_MLP_test1\"\n",
    "    \n",
    "    loss_list=[]\n",
    "    #loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    with wandb.init(project=title, config=config):\n",
    "        config = wandb.config\n",
    "        model = Three_Lay_MLP().to(device)\n",
    "\n",
    "        model, loss_list, predictions, labels = train_model(model, loss_fn,  train_ids, val_ids, config) #train_dl, val_dl\n",
    "\n",
    "    return model, loss_list, predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "j = 8.342553\n",
    "print(int(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448.0"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1024x7 and 7168x100)\n",
    "\n",
    "7168/16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/its/home/nn268/antvis/optics/wandb/run-20231115_164500-lk7iipzf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antvis/IDSW_on_Vgg16_Convs_training_MLP_test1/runs/lk7iipzf' target=\"_blank\">serene-feather-27</a></strong> to <a href='https://wandb.ai/antvis/IDSW_on_Vgg16_Convs_training_MLP_test1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antvis/IDSW_on_Vgg16_Convs_training_MLP_test1' target=\"_blank\">https://wandb.ai/antvis/IDSW_on_Vgg16_Convs_training_MLP_test1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antvis/IDSW_on_Vgg16_Convs_training_MLP_test1/runs/lk7iipzf' target=\"_blank\">https://wandb.ai/antvis/IDSW_on_Vgg16_Convs_training_MLP_test1/runs/lk7iipzf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]<ipython-input-19-586fca09c9c0>:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(x_train, dtype=torch.float32) # turns feature map into tensor\n",
      "/its/home/nn268/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E    0\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-586fca09c9c0>:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(x_val, dtype=torch.float32)\n",
      "  3%|â–Ž         | 1/30 [02:54<1:24:10, 174.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    1\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|â–‹         | 2/30 [05:56<1:23:26, 178.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    2\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|â–ˆ         | 3/30 [08:26<1:14:30, 165.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    3\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|â–ˆâ–Ž        | 4/30 [10:48<1:07:43, 156.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    4\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|â–ˆâ–‹        | 5/30 [13:06<1:02:23, 149.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    5\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|â–ˆâ–ˆ        | 6/30 [15:25<58:29, 146.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    6\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|â–ˆâ–ˆâ–Ž       | 7/30 [17:44<55:06, 143.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    7\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|â–ˆâ–ˆâ–‹       | 8/30 [20:04<52:18, 142.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    8\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [22:29<50:08, 143.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    9\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [24:46<47:06, 141.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    10\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [27:01<44:09, 139.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    11\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [29:17<41:30, 138.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    12\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [31:31<38:50, 137.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    13\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [33:55<37:07, 139.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    14\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [36:08<34:21, 137.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    15\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [38:20<31:39, 135.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    16\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [40:31<29:06, 134.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    17\n",
      "training...\n",
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [42:41<26:36, 133.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improvment in metrics. model saved\n",
      "E    18\n",
      "training...\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "model, loss_list, predictions, labels = pipeline(config) #7,168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = ['test','t', 'e', 's', 't', 'test']\n",
    "save_to = r'pickles/'\n",
    "with open(f'{save_to}_testingsaves.pkl', 'wb+') as f:\n",
    "                pickle.dump(test_file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "\n",
    "def plot_confusion(predictions:list, actual:list, title:str):\n",
    "    predict_list = [int(t.argmax()) for t in predictions]\n",
    "    actual = [int(l.argmax()) for l in actual]\n",
    "\n",
    "    actual = np.array(actual)\n",
    "    predict_list = np.array(predict_list)\n",
    "\n",
    "\n",
    "    #FixedLocator locations (3), usually from a call to set_ticks, does not match the number of labels (11).\n",
    "    print(f'\\n     {title}')\n",
    "    train_epoch_matrix = confusion_matrix(actual, predict_list, labels= [0,1,2,3,4,5,6,7,8,9,10])\n",
    "    disp= ConfusionMatrixDisplay(train_epoch_matrix, display_labels=[0,1,2,3,4,5,6,7,8,9,10])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "t_predict = predictions[0]\n",
    "v_predict = predictions[1]\n",
    "t_labels = labels[0]\n",
    "v_labels = labels[1]\n",
    "\n",
    "print(len(labels))\n",
    "for label in labels:\n",
    "    print(label[1])\n",
    "    #t_labels = label[0]\n",
    "    #v_labels = label[1]\n",
    "print(len(t_labels))\n",
    "print(len(v_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion(t_predict, t_labels, 'Train Confusion Matrix')\n",
    "plot_confusion(v_predict, v_labels, 'Validation Confusion Matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaughticalnonsence\u001b[0m (\u001b[33mantvis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as maths\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "#from torchsummary import summary\n",
    "#import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import wandb\n",
    "import pprint\n",
    "\n",
    "\n",
    "from functions import import_imagedata, get_data, label_oh_tf,  Unwrap, ImageProcessor\n",
    "from architectures import vgg16net, smallnet1, smallnet2, smallnet3\n",
    "from loop_fns import loop, test_loop\n",
    "from fns4wandb import build_optimizer, set_optimizer, hp_sweep, train_model, train_log, log_test_score, set_lossfn, pipeline\n",
    "\n",
    "from architectures import build_net, smallnet3\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from loop_fns import loop\\n\\ndef pipeline(config, col_dict,save_dict, title, device, seed):\\n    x_train, y_train, x_val, y_val, x_test, y_test = get_data(r\\'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/\\', seed)\\n\\n    with wandb.init(project=title, config=config):\\n        config = wandb.config\\n        model = choose_model(config).to(device) ###\\n        loss_fn = set_lossfn(config.loss_fn)\\n        \\n        #t_save_dict = train_model(model, x_train, y_train, x_val, y_val, loss_fn, config, col_dict, save_dict, device)\\n        # train model begins\\n        wandb.watch(model, loss_fn, log=\\'all\\', log_freq=10)\\n        sample_count =0\\n        batch_count = 0\\n        e_count = 0\\n        t_loss_list = []\\n        v_loss_list =[]\\n        t_predict_list = []\\n        t_label_list = []\\n        v_predict_list = []\\n        v_label_list = []\\n        t_accuracy_list= []\\n        v_accuracy_list= []\\n\\n        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config.scheduler, last_epoch=-1)\\n        \\n        for epoch in tqdm(range(config.epochs)):            \\n            t_loss_, predict_list_, t_num_correct, t_label_list_, model, optimizer = loop(model, x_train, y_train, epoch, loss_fn, device, col_dict, config.num_classes, optimizer=optimizer, scheduler=scheduler) #model, x_train, y_train, epoch, loss_fn, device, col_dict, config.num_classes, optimizer=optimizer, scheduler=scheduler\\n            sample_count += len(x_train)\\n            t_loss_list.append(t_loss_)\\n            t_predict_list.append(predict_list_)\\n            t_label_list.append(t_label_list_)\\n            t_accuracy_list.append(t_num_correct/len(x_train))\\n            v_loss_, v_predict_list_, v_num_correct, v_label_list_= loop(model, x_val,y_val, epoch, loss_fn, device,col_dict, config.num_classes, train=False) \\n            v_loss_list.append(v_loss_)\\n            v_predict_list.append(v_predict_list_)\\n            v_label_list.append(v_label_list_)\\n            v_accuracy_list.append(v_num_correct/len(x_val))\\n\\n        save_dict[\\'Current_Epoch\\'] = config[\\'epochs\\']\\n        save_dict[\\'training_samples\\'] = len(x_train)\\n        save_dict[\\'validation_samples\\'] = len(x_val)\\n        save_dict[\\'t_loss_list\\'] = t_loss_list #\\n        save_dict[\\'t_predict_list\\'] = [[c.to(\\'cpu\\') for c in k]for k in t_predict_list] #\\n        save_dict[\\'t_accuracy_list\\'] = t_accuracy_list #\\n        save_dict[\\'v_loss_list\\'] = v_loss_list #\\n        save_dict[\\'v_predict_list\\'] = [[c.to(\\'cpu\\') for c in k]for k in v_predict_list]#\\n        save_dict[\\'v_accuracy_list\\'] = v_accuracy_list #\\n        save_dict[\\'t_labels\\'] = [[c.to(\\'cpu\\') for c in k]for k in t_label_list]\\n        save_dict[\\'v_labels\\'] = [[c.to(\\'cpu\\') for c in k] for k in v_label_list]\\n\\n\\n        test_loop(model, x_test, y_test, loss_fn, device, col_dict, title, config.num_classes) \\n\\n        title = save_dict[\\'Run\\']\\n        with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", \\'wb+\\') as f:\\n            pickle.dump(save_dict, f)\\n\\n    return model\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from loop_fns import loop\n",
    "\n",
    "def pipeline(config, col_dict,save_dict, title, device, seed):\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = get_data(r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/', seed)\n",
    "\n",
    "    with wandb.init(project=title, config=config):\n",
    "        config = wandb.config\n",
    "        model = choose_model(config).to(device) ###\n",
    "        loss_fn = set_lossfn(config.loss_fn)\n",
    "        \n",
    "        #t_save_dict = train_model(model, x_train, y_train, x_val, y_val, loss_fn, config, col_dict, save_dict, device)\n",
    "        # train model begins\n",
    "        wandb.watch(model, loss_fn, log='all', log_freq=10)\n",
    "        sample_count =0\n",
    "        batch_count = 0\n",
    "        e_count = 0\n",
    "        t_loss_list = []\n",
    "        v_loss_list =[]\n",
    "        t_predict_list = []\n",
    "        t_label_list = []\n",
    "        v_predict_list = []\n",
    "        v_label_list = []\n",
    "        t_accuracy_list= []\n",
    "        v_accuracy_list= []\n",
    "\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config.scheduler, last_epoch=-1)\n",
    "        \n",
    "        for epoch in tqdm(range(config.epochs)):            \n",
    "            t_loss_, predict_list_, t_num_correct, t_label_list_, model, optimizer = loop(model, x_train, y_train, epoch, loss_fn, device, col_dict, config.num_classes, optimizer=optimizer, scheduler=scheduler) #model, x_train, y_train, epoch, loss_fn, device, col_dict, config.num_classes, optimizer=optimizer, scheduler=scheduler\n",
    "            sample_count += len(x_train)\n",
    "            t_loss_list.append(t_loss_)\n",
    "            t_predict_list.append(predict_list_)\n",
    "            t_label_list.append(t_label_list_)\n",
    "            t_accuracy_list.append(t_num_correct/len(x_train))\n",
    "            v_loss_, v_predict_list_, v_num_correct, v_label_list_= loop(model, x_val,y_val, epoch, loss_fn, device,col_dict, config.num_classes, train=False) \n",
    "            v_loss_list.append(v_loss_)\n",
    "            v_predict_list.append(v_predict_list_)\n",
    "            v_label_list.append(v_label_list_)\n",
    "            v_accuracy_list.append(v_num_correct/len(x_val))\n",
    "\n",
    "        save_dict['Current_Epoch'] = config['epochs']\n",
    "        save_dict['training_samples'] = len(x_train)\n",
    "        save_dict['validation_samples'] = len(x_val)\n",
    "        save_dict['t_loss_list'] = t_loss_list #\n",
    "        save_dict['t_predict_list'] = [[c.to('cpu') for c in k]for k in t_predict_list] #\n",
    "        save_dict['t_accuracy_list'] = t_accuracy_list #\n",
    "        save_dict['v_loss_list'] = v_loss_list #\n",
    "        save_dict['v_predict_list'] = [[c.to('cpu') for c in k]for k in v_predict_list]#\n",
    "        save_dict['v_accuracy_list'] = v_accuracy_list #\n",
    "        save_dict['t_labels'] = [[c.to('cpu') for c in k]for k in t_label_list]\n",
    "        save_dict['v_labels'] = [[c.to('cpu') for c in k] for k in v_label_list]\n",
    "\n",
    "\n",
    "        test_loop(model, x_test, y_test, loss_fn, device, col_dict, title, config.num_classes) \n",
    "\n",
    "        title = save_dict['Run']\n",
    "        with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", 'wb+') as f:\n",
    "            pickle.dump(save_dict, f)\n",
    "\n",
    "    return model\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\ngrid search\\n\\'learning_rate\\': {\\n            \\'values\\': [6.1e-5, 6.2e-5, 6.3e-5, 6.4e-5]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "grid search\n",
    "'learning_rate': {\n",
    "            'values': [6.1e-5, 6.2e-5, 6.3e-5, 6.4e-5]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"config = {\\n    'method': 'random',\\n    'metric':{\\n        'goal': 'minimize',\\n        'name': 'val_loss'},\\n    'parameters': {\\n        'dropout':{\\n            'values': [0.3]\\n        },\\n        'weight_decay':{\\n            'values': [3e-5]\\n        },\\n        'epochs':{\\n            'value': 80\\n        },\\n        'lin_layer_size': {\\n            'values': [100] #, 150, 50\\n        },\\n        'first_lin_lay':{\\n            'values':[67968]#67968 67968\\n        },\\n        'optimizer': {\\n            'values': ['adam']\\n        },\\n            'learning_rate': {\\n                'values': [6.01E-03]\\n            },\\n        'scheduler': {\\n            'values': [0.2]\\n        },\\n        'loss_fn': {\\n            'values': ['MSE', 'CrossEntropy']\\n        },\\n        'data_set':{\\n            'values':['Augmented']\\n        },\\n        'num_classes' : {\\n            'values':[11]\\n        },\\n        'ks': {\\n            'values': [(3,5)]\\n        },\\n        'model_name' : {'values': ['smallnet3']},\\n        'channels' : {'values': [3]},\\n        'image_path': {\\n            'values': [r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/']\\n        }\\n    }\\n}\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"config = {\n",
    "    'method': 'random',\n",
    "    'metric':{\n",
    "        'goal': 'minimize',\n",
    "        'name': 'val_loss'},\n",
    "    'parameters': {\n",
    "        'dropout':{\n",
    "            'values': [0.3]\n",
    "        },\n",
    "        'weight_decay':{\n",
    "            'values': [3e-5]\n",
    "        },\n",
    "        'epochs':{\n",
    "            'value': 80\n",
    "        },\n",
    "        'lin_layer_size': {\n",
    "            'values': [100] #, 150, 50\n",
    "        },\n",
    "        'first_lin_lay':{\n",
    "            'values':[67968]#67968 67968\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam']\n",
    "        },\n",
    "            'learning_rate': {\n",
    "                'values': [6.01E-03]\n",
    "            },\n",
    "        'scheduler': {\n",
    "            'values': [0.2]\n",
    "        },\n",
    "        'loss_fn': {\n",
    "            'values': ['MSE', 'CrossEntropy']\n",
    "        },\n",
    "        'data_set':{\n",
    "            'values':['Augmented']\n",
    "        },\n",
    "        'num_classes' : {\n",
    "            'values':[11]\n",
    "        },\n",
    "        'ks': {\n",
    "            'values': [(3,5)]\n",
    "        },\n",
    "        'model_name' : {'values': ['smallnet3']},\n",
    "        'channels' : {'values': [3]},\n",
    "        'image_path': {\n",
    "            'values': [r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "config = dict(\n",
    "    model_name = 'smallnet3',\n",
    "    epochs= 120, #120, \n",
    "    learning_rate =6.01E-05, #8.22E-05,# 6.40E-05  #8e-5,\n",
    "    dataset= 'IDSW_Aug',\n",
    "    architecture ='CNN',\n",
    "    optimizer= 'adam',\n",
    "    loss_fn = 'CrossEntropy',\n",
    "    weight_decay= 3.00E-05, #4e-5, #1.00E-05\n",
    "    dropout = 0.3, #0.4, #0.5 \n",
    "    first_lin_lay =1064448, #70272, #67968,#1087488,#1124352,#67968,#1087488, #70272,#1124352, #67968, #1087488, #67968, #1087488,\n",
    "    lin_layer_size= 100,\n",
    "    ks =3,#(3,5),\n",
    "    in_chan = 3,\n",
    "    num_classes =11,\n",
    "    scheduler = 0.2,\n",
    "    channels = 3\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "config = {\n",
    "     \"model_name\" : 'smallnet3',\n",
    "    \"epochs\" :120, \n",
    "    \"learning_rate\" : 6.62E-05, #5.97E-05, #6.01E-05, #6.62E-05, #0.00, 00821591686076769, #8e-5,\n",
    "    \"dataset\" : 'IDSW_Aug',\n",
    "    \"architecture\" :'CNN',\n",
    "    \"optimizer\": 'adam',\n",
    "    \"loss_fn\" : 'CrossEntropy',\n",
    "    \"weight_decay\": 4e-5, #2e-5, #3.00E-05,\n",
    "    \"dropout\" : 0.4, #0.4,\n",
    "    \"first_lin_lay\" :267264, #13888, #1055232, #67968,#1055232,\n",
    "    \"lin_layer_size\": 100,\n",
    "    \"ks\" : [3,5],\n",
    "    \"in_chan\" : 3,\n",
    "    \"num_classes\" :11,\n",
    "    \"scheduler\" : 0.2,\n",
    "    \"channels\": 3\n",
    "     \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "col_dict = {\n",
    "    'colour': 'colour',\n",
    "    'size': [226,72],\n",
    "    'padding': 5,\n",
    "    'model_size': '2c2l'\n",
    "}\n",
    "\n",
    "title = f\"IDSWAug_2c2l_e120_{col_dict['size']}_300124\"\n",
    "save_dict = {'Run' : title,\n",
    "            'Current_Epoch': 0,\n",
    "            'save_location' : r'pickles/'}\n",
    "#r'/its/home/nn268/antvis/optics/\n",
    "#pickles\n",
    "\n",
    "\n",
    "#sweep_id = wandb.sweep(config, project=title+f\"_{col_dict['colour']}_{col_dict['size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train(config=None):\\n    # lists for save dict\\n    t_loss_list = []\\n    v_loss_list =[]\\n    t_predict_list = []\\n    t_label_list = []\\n    v_predict_list = []\\n    v_label_list = []\\n    t_accuracy_list= []\\n    v_accuracy_list= []\\n    \\n    with wandb.init(config=config):\\n        config = wandb.config\\n        \\n        x_train, y_train, x_val, y_val, x_test, y_test = get_data(file_path= r\\'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/\\', seed=seed)\\n        \\n        model =smallnet3(in_chan=3, f_lin_lay=67968, l_lin_lay=11, ks=(3,5)).to(device) #10368\\n        if config.loss_fn == \\'MSE\\':\\n            loss_fn = nn.MSELoss()\\n        elif config.loss_fn == \\'CrossEntropy\\':\\n            loss_fn = nn.CrossEntropyLoss()\\n\\n        e_count = 0\\n         # *\\n\\n        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\\n\\n        for epoch in range(config.epochs):\\n            # current_loss, predict_list, num_correct, label_list, model, optimizer\\n            t_loss, t_predict_list_, t_num_correct, t_label_list_, model, optimizer = loop(model, x_train, y_train, epoch, loss_fn, device, col_dict, num_classes=11, optimizer=optimizer)\\n            t_accuracy = (t_num_correct /len(x_train))*100\\n            t_loss_list.append(t_loss)\\n            t_predict_list.append(t_predict_list_)\\n            t_label_list.append(t_label_list_)\\n            t_accuracy_list.append(t_accuracy)\\n\\n            v_loss, v_predict_list_, v_num_correct, v_label_list_= loop(model, x_val, y_val, epoch, loss_fn, device,col_dict,num_classes=11, train=False)\\n            v_accuracy= (v_num_correct / len(x_val))*100\\n            v_loss_list.append(v_loss)\\n            v_predict_list.append(v_predict_list_)\\n            v_label_list.append(v_label_list_)\\n            v_accuracy_list.append(v_accuracy)\\n\\n            t_avg_loss =t_loss/len(x_train)\\n            v_avg_loss = v_loss /len(x_val)\\n\\n            e_count +=1\\n            # logging\\n            wandb.log({\\'avg_train_loss\\': t_avg_loss, \\'epoch\\':epoch})\\n            wandb.log({\\'avg_val_loss\\': v_avg_loss, \\'epoch\\':epoch})\\n\\n            wandb.log({\\'train_loss\\': t_loss, \\'epoch\\':epoch})\\n            wandb.log({\\'val_loss\\': v_loss, \\'epoch\\':epoch})\\n\\n            wandb.log({\\'train_correct\\': t_num_correct, \\'epoch\\':epoch})\\n            wandb.log({\\'val_correct\\': v_num_correct, \\'epoch\\':epoch})\\n\\n            wandb.log({\\'train_accuracy_%\\': t_accuracy, \\'epoch\\':epoch})\\n            wandb.log({\\'val_accuracy_%\\': v_accuracy, \\'epoch\\':epoch})\\n\\n            wandb.log({\\'t_labels\\': t_label_list, \\'epoch\\':epoch})\\n            wandb.log({\\'v_labels\\': v_label_list, \\'epoch\\':epoch})\\n\\n            wandb.log({\\'t_predictions\\': t_predict_list, \\'epoch\\':epoch})\\n            wandb.log({\\'v_predictions\\': v_predict_list, \\'epoch\\':epoch})\\n\\n            # add lists to save dict after all epochs run\\n    save_dict[\\'Current_Epoch\\'] = config[\\'epochs\\']\\n    save_dict[\\'training_samples\\'] = len(x_train)# should this be the whole list for future graphs...?\\n    save_dict[\\'validation_samples\\'] = len(x_val)\\n    save_dict[\\'t_loss_list\\'] = t_loss_list #[c.to(\\'cpu\\') for c in t_loss_list]\\n    save_dict[\\'t_predict_list\\'] = [[c.to(\\'cpu\\') for c in k]for k in t_predict_list] #[[c.to(\\'cpu\\') for c in k]for k in t_predict_list]  # [c.to(\\'cpu\\') for c in t_predict_list] \\n    save_dict[\\'t_accuracy_list\\'] = t_accuracy_list #\\n    save_dict[\\'v_loss_list\\'] = v_loss_list #[c.to(\\'cpu\\') for c in v_loss_list]\\n    save_dict[\\'v_predict_list\\'] = [[c.to(\\'cpu\\') for c in k]for k in v_predict_list]#[[c.to(\\'cpu\\') for c in k]for k in v_predict_list] # [c.to(\\'cpu\\') for c in v_predict_list]\\n    save_dict[\\'v_accuracy_list\\'] = v_accuracy_list #\\n    save_dict[\\'t_labels\\'] = [[c.to(\\'cpu\\') for c in k]for k in t_label_list]\\n    save_dict[\\'v_labels\\'] = [[c.to(\\'cpu\\') for c in k] for k in v_label_list]\\n        \\n    test_predictions, test_y, test_accuracy = test_loop(model, x_test, y_test, loss_fn, device, col_dict, title, config.num_classes)\\n    save_dict[\\'test_predictions\\']= [c.to(\\'cpu\\') for c in test_predictions]\\n    save_dict[\\'test_labels\\'] = [c.to(\\'cpu\\') for c in test_y]\\n    save_dict[\\'test_acc\\'] = test_accuracy\\n\\n    title = save_dict[\\'Run\\']\\n    with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", \\'wb+\\') as f:\\n        pickle.dump(save_dict, f)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from functions import get_data\n",
    "#from loop_fns import test_loop\n",
    "\"\"\"\n",
    "def train(config=None):\n",
    "    # lists for save dict\n",
    "    t_loss_list = []\n",
    "    v_loss_list =[]\n",
    "    t_predict_list = []\n",
    "    t_label_list = []\n",
    "    v_predict_list = []\n",
    "    v_label_list = []\n",
    "    t_accuracy_list= []\n",
    "    v_accuracy_list= []\n",
    "    \n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        x_train, y_train, x_val, y_val, x_test, y_test = get_data(file_path= r'/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/', seed=seed)\n",
    "        \n",
    "        model =smallnet3(in_chan=3, f_lin_lay=67968, l_lin_lay=11, ks=(3,5)).to(device) #10368\n",
    "        if config.loss_fn == 'MSE':\n",
    "            loss_fn = nn.MSELoss()\n",
    "        elif config.loss_fn == 'CrossEntropy':\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        e_count = 0\n",
    "         # *\n",
    "\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            # current_loss, predict_list, num_correct, label_list, model, optimizer\n",
    "            t_loss, t_predict_list_, t_num_correct, t_label_list_, model, optimizer = loop(model, x_train, y_train, epoch, loss_fn, device, col_dict, num_classes=11, optimizer=optimizer)\n",
    "            t_accuracy = (t_num_correct /len(x_train))*100\n",
    "            t_loss_list.append(t_loss)\n",
    "            t_predict_list.append(t_predict_list_)\n",
    "            t_label_list.append(t_label_list_)\n",
    "            t_accuracy_list.append(t_accuracy)\n",
    "\n",
    "            v_loss, v_predict_list_, v_num_correct, v_label_list_= loop(model, x_val, y_val, epoch, loss_fn, device,col_dict,num_classes=11, train=False)\n",
    "            v_accuracy= (v_num_correct / len(x_val))*100\n",
    "            v_loss_list.append(v_loss)\n",
    "            v_predict_list.append(v_predict_list_)\n",
    "            v_label_list.append(v_label_list_)\n",
    "            v_accuracy_list.append(v_accuracy)\n",
    "\n",
    "            t_avg_loss =t_loss/len(x_train)\n",
    "            v_avg_loss = v_loss /len(x_val)\n",
    "\n",
    "            e_count +=1\n",
    "            # logging\n",
    "            wandb.log({'avg_train_loss': t_avg_loss, 'epoch':epoch})\n",
    "            wandb.log({'avg_val_loss': v_avg_loss, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'train_loss': t_loss, 'epoch':epoch})\n",
    "            wandb.log({'val_loss': v_loss, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'train_correct': t_num_correct, 'epoch':epoch})\n",
    "            wandb.log({'val_correct': v_num_correct, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'train_accuracy_%': t_accuracy, 'epoch':epoch})\n",
    "            wandb.log({'val_accuracy_%': v_accuracy, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'t_labels': t_label_list, 'epoch':epoch})\n",
    "            wandb.log({'v_labels': v_label_list, 'epoch':epoch})\n",
    "\n",
    "            wandb.log({'t_predictions': t_predict_list, 'epoch':epoch})\n",
    "            wandb.log({'v_predictions': v_predict_list, 'epoch':epoch})\n",
    "\n",
    "            # add lists to save dict after all epochs run\n",
    "    save_dict['Current_Epoch'] = config['epochs']\n",
    "    save_dict['training_samples'] = len(x_train)# should this be the whole list for future graphs...?\n",
    "    save_dict['validation_samples'] = len(x_val)\n",
    "    save_dict['t_loss_list'] = t_loss_list #[c.to('cpu') for c in t_loss_list]\n",
    "    save_dict['t_predict_list'] = [[c.to('cpu') for c in k]for k in t_predict_list] #[[c.to('cpu') for c in k]for k in t_predict_list]  # [c.to('cpu') for c in t_predict_list] \n",
    "    save_dict['t_accuracy_list'] = t_accuracy_list #\n",
    "    save_dict['v_loss_list'] = v_loss_list #[c.to('cpu') for c in v_loss_list]\n",
    "    save_dict['v_predict_list'] = [[c.to('cpu') for c in k]for k in v_predict_list]#[[c.to('cpu') for c in k]for k in v_predict_list] # [c.to('cpu') for c in v_predict_list]\n",
    "    save_dict['v_accuracy_list'] = v_accuracy_list #\n",
    "    save_dict['t_labels'] = [[c.to('cpu') for c in k]for k in t_label_list]\n",
    "    save_dict['v_labels'] = [[c.to('cpu') for c in k] for k in v_label_list]\n",
    "        \n",
    "    test_predictions, test_y, test_accuracy = test_loop(model, x_test, y_test, loss_fn, device, col_dict, title, config.num_classes)\n",
    "    save_dict['test_predictions']= [c.to('cpu') for c in test_predictions]\n",
    "    save_dict['test_labels'] = [c.to('cpu') for c in test_y]\n",
    "    save_dict['test_acc'] = test_accuracy\n",
    "\n",
    "    title = save_dict['Run']\n",
    "    with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", 'wb+') as f:\n",
    "        pickle.dump(save_dict, f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/its/home/nn268/antvis/antvis/optics/wandb/run-20240201_080627-yxgtwk0x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/yxgtwk0x' target=\"_blank\">laced-wildflower-60</a></strong> to <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics' target=\"_blank\">https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/yxgtwk0x' target=\"_blank\">https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/yxgtwk0x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                | 0/120 [00:00<?, ?it/s]/its/home/nn268/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "100%|██████████████████████████████████████| 120/120 [45:18<00:00, 22.65s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>██▇▇▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_val_loss</td><td>██▇▆▄▄▃▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▂▂▁▂▁▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy_%</td><td>▁▁▂▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█████████</td></tr><tr><td>train_correct</td><td>▁▁▂▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█████████</td></tr><tr><td>train_loss</td><td>██▇▇▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy_%</td><td>▁▁▂▄▅▆▆▇▆▇▇▆▇▇▇▇▇▇▇█▇▇██▇██▇█▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_correct</td><td>▁▁▂▄▅▆▆▇▆▇▇▆▇▇▇▇▇▇▇█▇▇██▇██▇█▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>██▇▆▄▄▃▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▂▂▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>1.8832</td></tr><tr><td>avg_val_loss</td><td>2.03842</td></tr><tr><td>epoch</td><td>119</td></tr><tr><td>train_accuracy_%</td><td>65.80287</td></tr><tr><td>train_correct</td><td>1422</td></tr><tr><td>train_loss</td><td>4069.59681</td></tr><tr><td>val_accuracy_%</td><td>50.20747</td></tr><tr><td>val_correct</td><td>121</td></tr><tr><td>val_loss</td><td>491.25812</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-wildflower-60</strong> at: <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/yxgtwk0x' target=\"_blank\">https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/runs/yxgtwk0x</a><br/> View job at <a href='https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNDg5MjM1OA==/version_details/v5' target=\"_blank\">https://wandb.ai/antvis/antvis-optics_antvis_antvis_optics/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNDg5MjM1OA==/version_details/v5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240201_080627-yxgtwk0x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ACCURACY:  49.029126213592235\n"
     ]
    }
   ],
   "source": [
    "from fns4wandb import pipeline, choose_model\n",
    "import pickle\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\"\"\"def tr(config=None, save_dict=save_dict):\n",
    "    with wandb.init(config=config):\n",
    "        seed = random.randint(0, 50)\n",
    "        config = wandb.config\n",
    "        title = save_dict['Run']\n",
    "        model = choose_model(config).to(device)\n",
    "        loss_fn = set_lossfn(config.loss_fn)\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=config.scheduler, last_epoch=-1)\n",
    "\n",
    "        model, save_dict,x_test, y_test = hp_sweep(config, col_dict, save_dict, device, seed, model, loss_fn, optimizer, scheduler)\n",
    "        test_predict_list, test_labels, test_accuracy = test_loop(model, x_test, y_test, loss_fn, device, col_dict, title, config.num_classes)\n",
    "        save_dict['test_predict_list'] = [c.to('cpu') for c in test_predict_list]\n",
    "        save_dict['test_labels'] = test_labels#[c.to('cpu') for c in test_labels]\n",
    "        save_dict['test_accuracy'] = test_accuracy\n",
    "        save_dict['seed'] = seed\n",
    "        \n",
    "        with open(f\"/its/home/nn268/antvis/antvis/optics/pickles/{title}.pkl\", 'wb+') as f:\n",
    "            pickle.dump(save_dict, f)\"\"\"\n",
    "\n",
    "#wandb.agent(sweep_id, tr, count=25)\n",
    "# config, col_dict,save_dict, title, device, seed)\n",
    "\n",
    "\n",
    "from fns4wandb import train\n",
    "#model = pipeline(config, col_dict,save_dict, title=\"2c2l_training_113x36, 3chan\", device=device, seed=seed)\n",
    "model = train(device,col_dict, save_dict, config)\n",
    "#pipeline(config, col_dict, save_dict, title, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "03\n"
     ]
    }
   ],
   "source": [
    "n = '/its/home/nn268/antvis/antvis/optics/AugmentedDS_IDSW/IDSW003_060423_1133_SW_0029.JPG_Augmented_left_1.JPG'\n",
    "print(len(n))\n",
    "print(n[59:61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.640776699029127\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def tr(config=None):\\n    with wandb.init(config=config):\\n        config = wandb.config\\n        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\\n        model = hp_sweep(config, col_dict, save_dict, device)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def tr(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = hp_sweep(config, col_dict, save_dict, device)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.agent(sweep_id, tr, count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/its/home/nn268/antvis/antvis/optics\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model = pipeline(config, col_dict, title=\"2c2l_training_113x36, 3chan\", image_file_path= \"/its/home/nn268/optics/AugmentedDS_IDSW/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
